{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should do [Working efficiently with jupyter lab](https://florianwilhelm.info/2018/11/working_efficiently_with_jupyter_lab/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_v3 import Network, Layer, IdentityLayer, AffineLayer, MapLayer\n",
    "from nnbench_v2 import NNBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.extend(AffineLayer(2,2))\n",
    "#leak = 0\n",
    "#net.extend(MapLayer(lambda x: (x*(1+leak/2)+abs(x)*(1-leak/2))/2, lambda d: [leak,1][1 if d>0 else 0]))\n",
    "#net.extend(MapLayer(lambda x: max(0, np.sign(x)) * x, lambda d: max(0, np.sign(d))))\n",
    "net.extend(MapLayer(np.tanh, lambda d: 1.0 - np.tanh(d)**2))\n",
    "net.extend(AffineLayer(2,1))\n",
    "net.extend(MapLayer(np.tanh, lambda d: 1.0 - np.tanh(d)**2))\n",
    "sigmoid = lambda x: 1/(np.exp(x)+1)\n",
    "#net.extend(MapLayer(sigmoid, lambda d: sigmoid(d)*(1-sigmoid(d))))\n",
    "#net.extend(MapLayer(lambda x: max(0, np.sign(x)) * x, lambda d: max(0, np.sign(d))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = NNBench(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = \\\n",
    "[(np.array([-1,-1]), np.array([-1])),\n",
    " (np.array([-1,1]), np.array([1])),\n",
    " (np.array([1,1]), np.array([-1])),\n",
    " (np.array([1,-1]), np.array([1]))]\n",
    "dc = 0\n",
    "amp= 1\n",
    "temp = [(d[0]*amp/2+dc,d[1]*amp/2+dc) for d in dat]\n",
    "\n",
    "bench.training_data = ((np.array([v[0] for v in temp]),\n",
    "                        np.array([v[1] for v in temp])),)\n",
    "bench.training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.training_data_gen = bench.training_data_gen_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(bench.training_data_gen(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # it would read in an old nn_v2 net\n",
    "    with open('slow_xor_1.net', 'rb') as f:\n",
    "        bench.net_checkpoint = f.read()\n",
    "    bench.rollback_net()\n",
    "else:\n",
    "    bench.randomize_net()\n",
    "    bench.checkpoint_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.plot_learning(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.rollback_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development space for plotting:\n",
    "if False:\n",
    "    bench.rollback_net()\n",
    "    losses = bench.learn(200)\n",
    "    fig, ax = plt.subplots()  # Create a figure and an axes.\n",
    "    ax.plot(losses, label=f\"$\\eta={bench.net.eta}$\")  # Plot some data on the axes.\n",
    "    ax.set_xlabel('learnings')  # Add an x-label to the axes.\n",
    "    ax.set_ylabel('loss')  # Add a y-label to the axes.\n",
    "    ax.set_title(\"Losses\")  # Add a title to the axes.\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()  # Add a legend.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.net.state_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.knobs_plot_learning(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.rollback_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isv = interesting_stubborn_sv = np.array([-4.16013824, -4.37023452, -0.83547458,  0.92877501,  1.48893334,\n",
    "        1.5066594 ,  1.10828375, -0.71174959,  0.16778073])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.net.set_state_from_vector(interesting_stubborn_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(bench.training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnings = bench.learn(100)\n",
    "net(bench.training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnings = bench.learn(100)\n",
    "net(bench.training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.net.set_state_from_vector(interesting_stubborn_sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss surface with `plotly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    rates = np.logspace(-2, 0, num=10)\n",
    "    cube = bench.learn_loss_cube(10000, rates)\n",
    "    bench.plot_loss_cube()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss surface with `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bench.mpl_plot_loss_cube()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracks\n",
    "Examine the trajectory in state space during learning, along state space, and the loss function.\n",
    "Each learning iteration changes the net state. We can examine those deltas.\n",
    "Questions:\n",
    "1. Are there regimes of direction-of-change (DoC) in state space, or does the DoC wander chaotically?\n",
    "1. What are the spectral characteristics of the DoC? Length characteristics?\n",
    "1. How do the DoC characteristics relate to the loss function, and it's first difference?\n",
    "1. How do these trajectories vary with learning rate? Are there clues in these to adapt the learning rate?\n",
    "1. How do the trajectory characteristics vary across different starting nets?\n",
    "1. How do these measures vary with the objective function of the learning process, that is, what you're trying to teach the net?\n",
    "1. How do the different layers with learning state evolve? Do they settle at different times? How does an upstream layer change, as a consequence of learning, affect downstream layers? Down affect up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bench.learn_track(n)` does n batches of learning, recording the state vector of the network after each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.randomize_net()\n",
    "bench.checkpoint_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.rollback_net()\n",
    "bench.net.eta = 0.05\n",
    "lt = bench.learn_track(2000)\n",
    "lt[0], lt[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangle the state-space trajectory and the losses into form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = np.vstack([v[0] for v in lt])\n",
    "losses = np.vstack([v[1] for v in lt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take first differences, which represent the changes at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_steps = np.diff(trajectory, axis=0)\n",
    "loss_steps = np.diff(losses, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_steps[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the L2 norm of the trajectory steps $\\lVert traj \\rVert$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_L2 = np.sqrt(np.einsum('...i,...i', traj_steps, traj_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(traj_L2), traj_L2[:5], traj_L2[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the angles between trajectory steps, from\n",
    "$$\\mathbf {a} \\cdot \\mathbf {b} = \\left\\|\\mathbf {a} \\right\\|\\left\\|\\mathbf {b} \\right\\|\\cos \\theta \\\\\n",
    "\\cos \\theta = \\frac{\\mathbf {a} \\cdot \\mathbf {b}}{\\left\\|\\mathbf {a} \\right\\|\\left\\|\\mathbf {b} \\right\\|} \\\\\n",
    "$$\n",
    "where $\\mathbf {a}$ and $\\mathbf {b}$ are a state-space trajectory step and the succeeding step respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find $\\mathbf {a} \\cdot \\mathbf {b}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajn_dot_nplus1 = np.einsum('...i,...i', traj_steps[:-1], traj_steps[1:])\n",
    "trajn_dot_nplus1[:5], np.any(trajn_dot_nplus1 < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find $\\left\\|\\mathbf {a} \\right\\|\\left\\|\\mathbf {b} \\right\\|$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cos_denom = np.multiply(traj_L2[:-1], traj_L2[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be the divisor. Some entries may be zero, so we adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(traj_L2) - np.count_nonzero(traj_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.equal(traj_L2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find $\\cos \\theta$ by dividing, excluding division by zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cos = np.divide(trajn_dot_nplus1, traj_cos_denom, where=traj_cos_denom!=0.0)\n",
    "traj_cos[:5], traj_cos[-5:], min(traj_cos), max(traj_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traj_theta = np.arccos(traj_cos)\n",
    "#traj_theta[:5], traj_theta[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development space for plotting:\n",
    "if True:\n",
    "    fig, ax = plt.subplots()  # Create a figure and an axes.\n",
    "    traj_color = 'xkcd:red'\n",
    "    loss_color = 'xkcd:blue'\n",
    "    cos_color = 'xkcd:green'\n",
    "    ax.set_xlabel('$n$')  # Add an x-label to the axes.\n",
    "    ax.set_ylabel('$|\\Delta state|$', color=traj_color)\n",
    "    ax.tick_params(axis='y', labelcolor=traj_color)\n",
    "    ax.set_title(f\"$\\eta={bench.net.eta}$\")  # Add a title to the axes.\n",
    "    ax.set_yscale('log')\n",
    "    tnl, = ax.plot(traj_L2, label=f\"traj norm\", color=traj_color)  # Plot some data on the axes.\n",
    "    ax2 = ax.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.tick_params(axis='y', labelcolor=loss_color)\n",
    "    dll, = ax2.plot(loss_steps, label=f\"$\\Delta loss$\", color=loss_color)  # Plot some data on the axes.\n",
    "    cosl, = ax2.plot(traj_cos, label=f\"$\\Delta state cosine$\", color=cos_color)\n",
    "    ax.legend([tnl, dll, cosl], [\"$\\\\|\\\\Delta state \\\\|$\", \"$\\\\Delta loss$\", \"$cos(\\\\theta)\\Delta$\"])  # Add a legend.\n",
    "    #ax2.legend()  # Add a legend.\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting structures in the loss surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.net.eta = 0.1226\n",
    "bench.plot_learning(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = np.flip(0.175/np.exp(np.arange(100)*0.0075))\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = bench.learn_loss_cube(500, rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.plot_loss_cube()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
