{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets using [JAX](https://github.com/google/jax#readme)\n",
    "**JAX is NumPy on the CPU, GPU, and TPU, with great [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) for high-performance machine learning research.**\n",
    "\n",
    "Version 0.1, in `nn-jax`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should do [Working efficiently with jupyter lab](https://florianwilhelm.info/2018/11/working_efficiently_with_jupyter_lab/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this was a notebook with integrated tests, we did: \\\n",
    "`\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "#%matplotlib inline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xpy1dSgNqCP4"
   },
   "source": [
    "We'll be generating random data in the following examples. One big difference between NumPy and JAX is how you generate random numbers. For more details, see [Common Gotchas in JAX].\n",
    "\n",
    "[Common Gotchas in JAX]: https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Random-Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u0nseKZNqOoH"
   },
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of components which:\n",
    "1. accept an ordered set of reals (we'll use `numpy.array`, and  call them vectors) at the input port and produce another at the output port - this is forward propagation. ${\\displaystyle f\\colon \\mathbf {R} ^{n}\\to \\mathbf {R} ^{m}}$\n",
    "1. accept an ordered set of reals at the output port, representing the gradient of the loss function at the output, and produce the gradient of the loss function at the input port - this is back propagation, aka backprop. ${\\displaystyle b\\colon \\mathbf {R} ^{m}\\to \\mathbf {R} ^{n}}$\n",
    "1. from the gradient of the loss function at the output, calculate the partial of the loss function w.r.t the internal parameters ${\\displaystyle \\frac{\\partial E}{\\partial w} }$\n",
    "1. accept a scalar $\\eta$ to control the adjustment of internal parameters. _Or is this effected by scaling the loss gradient before passing? YES_\n",
    "1. update internal parameters ${\\displaystyle w \\leftarrow w - \\eta \\frac{\\partial E}{\\partial w} }$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Compute response to input\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backprop(self, output_delE):\n",
    "        \"\"\"Use output error gradient to adjust internal parameters, return gradient of error at input\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def state_vector(self):\n",
    "        \"\"\"Provide the layer's learnable state as a vector\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of a cascade of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.eta = 0.1 #FIXME\n",
    "        \n",
    "    def extend(self, net):\n",
    "        self.layers.append(net)\n",
    "        return self\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        v = input\n",
    "        for net in self.layers:\n",
    "            v = net(v)\n",
    "        return v\n",
    "    \n",
    "    def learn(self, facts, eta=None):\n",
    "        self.eta = eta or self.eta\n",
    "        for x, ideal in facts:\n",
    "            y = self(x)\n",
    "            e = y - ideal\n",
    "            egrad = e * self.eta / e.shape[0]\n",
    "            for net in reversed(self.layers):\n",
    "                egrad = net.backprop(egrad)\n",
    "        #loss = float(e.dot(e.T))/2.0\n",
    "        loss = jnp.einsum('...ij,...ij', e, e) / (2.0 * e.shape[0])\n",
    "        return loss\n",
    "\n",
    "    def losses(self, facts):\n",
    "        return [jnp.einsum('...ij,...ij', e, e) / (2.0 * e.shape[0]) \\\n",
    "                for e in (self(x) - ideal for x, ideal in facts)]\n",
    "        \n",
    "    def state_vector(self):\n",
    "        \"\"\"Provide the network's learnable state as a vector\"\"\"\n",
    "        return jnp.concatenate([layer.state_vector() for layer in self.layers])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            lsvlen = len(layer.state_vector())\n",
    "            layer.set_state_from_vector(sv[i:i+lsvlen])\n",
    "            i += lsvlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(Layer):\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        return output_delE\n",
    "\n",
    "    def state_vector(self):\n",
    "        return jnp.array([])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine\n",
    "A layer that does an [affine transformation](https://mathworld.wolfram.com/AffineTransformation.html) aka affinity, which is the classic fully-connected layer with output offsets.\n",
    "\n",
    "$$ \\mathbf{M} \\mathbf{x} + \\mathbf{b} = \\mathbf{y} $$\n",
    "where\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{j=1}^{n} x_j \\mathbf{\\hat{x}}_j \\\\\n",
    "\\mathbf{b} = \\sum_{i=1}^{m} b_i \\mathbf{\\hat{y}}_i \\\\\n",
    "\\mathbf{y} = \\sum_{i=1}^{m} y_i \\mathbf{\\hat{y}}_i\n",
    "$$\n",
    "and $\\mathbf{M}$ can be written\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    m_{1,1} & \\dots & m_{1,n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    m_{m,1} & \\dots & m_{m,n}\n",
    "\\end{bmatrix} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error gradient back-propagation\n",
    "$$ \n",
    "\\begin{align}\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{x}}\n",
    "  &= \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}} \\\\\n",
    "  &= \\mathbf{M}^\\mathsf{T}\\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter adjustment\n",
    "$$\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\mathbf{x} \\\\\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapting to `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `numpy` it is more convenient to use row vectors, particularly for calculating the transform on multiple inputs in one operation. We use the identity $ \\mathbf{M} \\mathbf{x} = (\\mathbf{x} \\mathbf{M}^\\mathsf{T})^\\mathsf{T}.$ To avoid cluttering names, we will use `M` in the code below to hold $\\mathbf{M}^\\mathsf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineLayer(Layer):\n",
    "    \"\"\"An affine transformation, which is the classic fully-connected layer with offsets.\n",
    "    \n",
    "    The layer has n inputs and m outputs, which numbers must be supplied\n",
    "    upon creation. The inputs and outputs are marshalled in numpy arrays, 1-D\n",
    "    in the case of a single calculation, and 2-D when calculating the outputs\n",
    "    of multiple inputs in one call.\n",
    "    If called with 1-D array having shape == (n,), e.g numpy.arange(n), it will\n",
    "    return a 1-D numpy array of shape (m,).\n",
    "    If called with a 2-D numpy array, input shall have shape (k,n) and will return\n",
    "    a 2-D numpy array of shape (k,m), suitable as input to a subsequent layer\n",
    "    that has input width m.\n",
    "    \"\"\"\n",
    "    def __init__(self, n, m):\n",
    "        self.M = jnp.empty((n, m))\n",
    "        self.b = jnp.empty(m)\n",
    "        self.randomize()\n",
    "        \n",
    "    def randomize(self):\n",
    "        self.M = random.normal(key, self.M.shape, dtype=jnp.float32)\n",
    "        self.b = random.normal(key, self.b.shape, dtype=jnp.float32)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        self.output = x @ self.M + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = output_delE @ self.M.T\n",
    "        o_delE = jnp.atleast_2d(output_delE)\n",
    "        self.M -= jnp.einsum('...ki,...kj->...ji', o_delE, jnp.atleast_2d(self.input))\n",
    "        self.b -= jnp.sum(o_delE, 0)       \n",
    "        return input_delE\n",
    "\n",
    "    def state_vector(self):\n",
    "        return jnp.concatenate((self.M.ravel(), self.b.ravel()))\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        l_M = len(self.M.ravel())\n",
    "        l_b = len(self.b.ravel())\n",
    "        self.M = sv[:l_M].reshape(self.M.shape)\n",
    "        self.b = sv[l_M : l_M + l_b].reshape(self.b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "Maps a scalar function on the inputs, for e.g. activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapLayer(Layer):\n",
    "    \"\"\"Map a scalar function on the input taken element-wise\"\"\"\n",
    "    def __init__(self, fun, dfundx):\n",
    "        self.vfun = jnp.vectorize(fun)\n",
    "        self.vdfundx = jnp.vectorize(dfundx)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        return self.vfun(x)\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = self.vdfundx(self.input) * output_delE\n",
    "        return input_delE\n",
    "\n",
    "    def state_vector(self):\n",
    "        return jnp.array([])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "*Incomplete* \\\n",
    "Also `unittest` the `.py` version with a separate test script, see `test-nn_v3.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a few test arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_wide is:\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "two_wide is:\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "three_wide is:\n",
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    one_wide = jnp.atleast_2d(jnp.arange(1*4)).reshape(-1,1)\n",
    "    print(f\"one_wide is:\\n{one_wide}\")\n",
    "    two_wide = jnp.arange(2*4).reshape(-1,2)\n",
    "    print(f\"two_wide is:\\n{two_wide}\")\n",
    "    three_wide = jnp.arange(3*4).reshape(-1,3)\n",
    "    print(f\"three_wide is:\\n{three_wide}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tooling for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import sympy\n",
    "    \n",
    "    class VC():\n",
    "        def grad(f, x, eps=1e-3):\n",
    "            #epsihat = jnp.eye(x.size) * eps\n",
    "            epsihat = jnp.eye(x.shape[-1]) * eps\n",
    "            yp = jnp.apply_along_axis(f, 1, x + epsihat)\n",
    "            ym = jnp.apply_along_axis(f, 1, x - epsihat)\n",
    "            return (yp - ym)/(2 * eps)\n",
    "        \n",
    "        def tensor_grad(f, x, eps=1e-3):\n",
    "            return jnp.apply_along_axis(lambda v: VC.grad(f, v, eps), 1, x)\n",
    "            \n",
    "    def closenuf(a, b, tol=0.001):\n",
    "        return jnp.allclose(a, b, rtol=tol)\n",
    "    \n",
    "    def arangep(n, starting_index=0):\n",
    "        sympy.sieve.extend_to_no(starting_index + n)\n",
    "        return jnp.array(sympy.sieve._list[starting_index:starting_index + n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VC.grad(lambda x:x**2, three_wide[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VC.tensor_grad(lambda x:x**2, three_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input to a layer can be a single (row) vector, or a vertical stack of row vectors,\n",
    "a 2-d array that resembles a matrix. We need to test each layer class with both single and stacked input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    iL = IdentityLayer()\n",
    "    \n",
    "    # It's transparent from input to output\n",
    "    assert jnp.equal(iL(jnp.arange(5)), jnp.arange(5)).all()\n",
    "    assert (iL(three_wide) == three_wide).all()\n",
    "    \n",
    "    # It back-propagates the loss gradient without alteration\n",
    "    assert jnp.equal(iL.backprop(jnp.arange(7)), jnp.arange(7)).all()\n",
    "    assert (iL.backprop(three_wide) == three_wide).all()\n",
    "\n",
    "    # It works for stacked input\n",
    "    # (see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test single vector input behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1 2 2], y = [1 4 4], loss at y = 1.5\n",
      "∇𝑙𝑜𝑠𝑠(𝑦) = [-0.99998707  0.99992746 -0.99992746]\n",
      "∇𝑙𝑜𝑠𝑠(𝑥) = [-1.9999741  3.9999483 -3.9999483]\n",
      "backprop([-0.99998707  0.99992746 -0.99992746]) = [-1.9999741  3.9997098 -3.9997098]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    mL = MapLayer(lambda x:x**2, lambda d:2*d)\n",
    "    \n",
    "    # It applies the forward transformation\n",
    "    assert jnp.equal(mL(jnp.array([-2,1,3])), jnp.array([4,1,9])).all()\n",
    "    \n",
    "    # It back-propagages the loss gradient\n",
    "    x = jnp.array([1,2,2])\n",
    "    y = mL(x)\n",
    "    \n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = jnp.array([2,3,5])\n",
    "    loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x = {x}, y = {y}, loss at y = {loss_at_y}\")\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) = {grad_y}\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.grad(lambda x:loss(mL(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) = {grad_x}\")\n",
    "    \n",
    "    # The backprop method does the same\n",
    "    _ = mL(x) # Make sure the last x is in the right place\n",
    "    in_delE = mL.backprop(grad_y)\n",
    "    print(f\"backprop({grad_y}) = {in_delE}\")\n",
    "    assert closenuf(in_delE, grad_x)\n",
    "    \n",
    "    # The backprop operation did not change the behavior\n",
    "    assert jnp.equal(mL(x), y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test stacked-vectors input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "y =\n",
      "[[ 0  1]\n",
      " [ 4  9]\n",
      " [16 25]\n",
      " [36 49]], loss = 152.5\n",
      "\n",
      "∇𝑙𝑜𝑠𝑠(𝑦) =\n",
      "[[-33.99658   -36.01074  ]\n",
      " [-26.000975  -19.989012 ]\n",
      " [ -1.9989012  11.993407 ]\n",
      " [ 37.963863   59.93652  ]]\n",
      "\n",
      "∇𝑙𝑜𝑠𝑠(𝑥) =\n",
      "[[   0.        -71.99096 ]\n",
      " [-104.0039   -119.99511 ]\n",
      " [ -15.995025  119.991295]\n",
      " [ 455.99362   839.9047  ]]\n",
      "\n",
      "backprop([[-33.99658   -36.01074  ]\n",
      " [-26.000975  -19.989012 ]\n",
      " [ -1.9989012  11.993407 ]\n",
      " [ 37.963863   59.93652  ]]) =\n",
      "[[  -0.        -72.02148 ]\n",
      " [-104.0039   -119.93407 ]\n",
      " [ -15.99121   119.934074]\n",
      " [ 455.56635   839.11127 ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    mL = MapLayer(lambda x:x**2, lambda d:2*d)\n",
    "    \n",
    "    two_wide_sq = jnp.array([[ 0,  1],\n",
    "                            [ 4,  9],\n",
    "                            [16, 25],\n",
    "                            [36, 49]])\n",
    "    # It applies the forward transformation\n",
    "    assert jnp.equal(mL(two_wide), two_wide_sq).all()\n",
    "    \n",
    "    # It back-propagages the loss gradient\n",
    "    x = two_wide\n",
    "    y = mL(x)\n",
    "\n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = two_wide * 2 + 11\n",
    "    #print(y - ideal)\n",
    "    #loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss = lambda v: jnp.einsum('ij,ij', v-ideal, v-ideal) / (2 * v.shape[0])\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x =\\n{x}\\ny =\\n{y}, loss = {loss_at_y}\\n\")\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.tensor_grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) =\\n{grad_y}\\n\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.tensor_grad(lambda x:loss(mL(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) =\\n{grad_x}\\n\")\n",
    "    \n",
    "    # The backprop method does the same\n",
    "    _ = mL(x) # Make sure the last x is in the right place\n",
    "    in_delE = mL.backprop(grad_y)\n",
    "    print(f\"backprop({grad_y}) =\\n{in_delE}\")\n",
    "    assert closenuf(in_delE, grad_x)\n",
    "    \n",
    "    # The backprop operation did not change the behavior\n",
    "    assert jnp.equal(mL(x), y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test single vector input behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for single input-vector operations:\n",
    "* input and output widths\n",
    "* state vector setting and getting\n",
    "* forward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    key = random.PRNGKey(0)\n",
    "    #x = random.normal(key, (10,))\n",
    "\n",
    "    \n",
    "    # The input and output widths are correct\n",
    "    assert a(jnp.arange(2)).shape == (3,) \n",
    "\n",
    "    # Its internal state can be set\n",
    "    a.set_state_from_vector(jnp.arange(9))\n",
    "    # and read back\n",
    "    assert (a.state_vector() == jnp.arange(9)).all()\n",
    "    # NOTE: The two assertions below are commented out because they depend\n",
    "    # on white-box knowledge, and are duplicative of other tests\n",
    "    #assert jnp.equal(a.M, jnp.array([[0, 1, 2],\n",
    "    #                               [3, 4, 5]])).all()\n",
    "    #assert jnp.equal(a.b, jnp.array([6, 7, 8])).all()\n",
    "\n",
    "    # Its internal state observed using numerical gradient is correct\n",
    "    x = random.uniform(key, (2,))\n",
    "    y = a(x)\n",
    "    dydx = VC.grad(a, x)\n",
    "    b = y - x.dot(dydx)\n",
    "    #print(dydx, b)\n",
    "    #print(dydx, jnp.arange(6).reshape(2,-1))\n",
    "    assert closenuf(dydx, jnp.arange(6).reshape(2, -1))\n",
    "    #print(b, jnp.arange(6, 9))\n",
    "    assert closenuf(b, jnp.arange(6, 9))\n",
    "    \n",
    "    # It performs a single-input forward calculation correctly\n",
    "    x = jnp.array([2, 1])\n",
    "    y = a(x)\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\\n\")\n",
    "    assert (y == jnp.array([9, 13, 17])).all()\n",
    "    \n",
    "    # It performs a different single-input forward calculation correctly\n",
    "    a.set_state_from_vector(jnp.array([ 2,  3,  5,  7, 11, 13, 17, 19, 23]))\n",
    "    x = jnp.array([[29, 31]])\n",
    "    y = a(x)\n",
    "    assert (y == jnp.array([[292, 447, 571]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for single input-vector operations:\n",
    "* back-propagation of the loss gradient\n",
    "* learning (change in forward function) from the back-prop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [2 1], y = [ 9 13 17], loss = 27.0\n",
      "∇𝑙𝑜𝑠𝑠(𝑦) = [-2.0008085  1.0004042  6.9961543]\n",
      "∇𝑙𝑜𝑠𝑠(𝑥) = [14.991759 33.006664]\n",
      "backprop([-0.20008086  0.10004043  0.6996154 ]) = [1.4992713 3.2979963]\n",
      "Now a([2 1]) = [10.200485 12.399757 12.802307], loss = 4.325977325439453\n",
      "state_vector is [0.4001617  0.7999191  0.60076916 3.2000809  3.8999596  4.3003845\n",
      " 6.200081   6.8999596  7.3003845 ]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(jnp.arange(9))\n",
    "\n",
    "    # Doing a single-input-vector calculation\n",
    "    x = jnp.array([2, 1])\n",
    "    y = a(x)\n",
    "    assert jnp.equal(y, jnp.array([9, 13, 17])).all()\n",
    "\n",
    "    # It back-propagages the loss gradient\n",
    "    ideal = jnp.array([11,12,10])\n",
    "    loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x = {x}, y = {y}, loss = {loss_at_y}\")\n",
    "    grad_y = VC.grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) = {grad_y}\")\n",
    "    grad_x = VC.grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) = {grad_x}\")\n",
    "    \n",
    "    # Back-propagate the loss gradient from layer output to input\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.1 # Backprop one-tenth of the loss gradient\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    \n",
    "    # The loss gradient back-propagated to the layer input is correct\n",
    "    assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")\n",
    "    # FIXME: Check the change is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test batch operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* input and output widths\n",
    "* forward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(jnp.arange(9))\n",
    "    \n",
    "    # The input and output widths for the forward calculation are correct\n",
    "    x = two_wide\n",
    "    y = a(two_wide)\n",
    "    assert y.shape[0] == x.shape[0]\n",
    "    assert y.shape[1] == 3\n",
    "    \n",
    "    # The input and output widths for the backprop calculation are correct\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    assert bp.shape[0] == three_wide.shape[0]\n",
    "    assert bp.shape[1] == x.shape[1]\n",
    "\n",
    "    # The forward calculation is correct (in at least two instances)\n",
    "    a.set_state_from_vector(jnp.arange(9))\n",
    "    x = jnp.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    assert (a(x) == jnp.array([[ 9, 11, 13],\n",
    "                              [15, 21, 27],\n",
    "                              [21, 31, 41],\n",
    "                              [27, 41, 55]])).all()\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "    a.set_state_from_vector(jnp.array([ 2,  3,  5,  7, 11, 13, 17, 19, 23]))\n",
    "    y = a(x)\n",
    "    #print(f\"x is: {x}\\ny is: {y}\")\n",
    "    assert (y == jnp.array([[ 24,  30,  36],\n",
    "                           [ 42,  58,  72],\n",
    "                           [ 60,  86, 108],\n",
    "                           [ 78, 114, 144]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* back-propagation of the loss gradient\n",
    "* learning (change in forward function) from the back-prop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y - ideal =\n",
      "[[-15 -19 -23]\n",
      " [-27 -37 -45]\n",
      " [-39 -55 -67]\n",
      " [-51 -73 -89]]\n",
      "x =\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "ideal =\n",
      "[[ 24  30  36]\n",
      " [ 42  58  72]\n",
      " [ 60  86 108]\n",
      " [ 78 114 144]]\n",
      "y =\n",
      "[[ 9 11 13]\n",
      " [15 21 27]\n",
      " [21 31 41]\n",
      " [27 41 55]], loss = 3765.5\n",
      "\n",
      "∇𝑙𝑜𝑠𝑠(𝑦) =\n",
      "[[ -55.66406   -81.05468  -102.539055]\n",
      " [ -48.33984   -67.871086  -83.98437 ]\n",
      " [ -40.03906   -54.687496  -65.18554 ]\n",
      " [ -31.98242   -41.25976   -46.630856]]\n",
      "∇𝑙𝑜𝑠𝑠(𝑥) =\n",
      "[[ -286.13278 -1006.8359 ]\n",
      " [ -235.83983  -835.44916]\n",
      " [ -185.30272  -665.039  ]\n",
      " [ -134.52147  -494.38474]]\n",
      "backprop([[-0.55664057 -0.81054676 -1.0253905 ]\n",
      " [-0.48339838 -0.6787108  -0.83984363]\n",
      " [-0.40039057 -0.54687494 -0.65185535]\n",
      " [-0.3198242  -0.4125976  -0.46630853]]) = [[ -2.8613276 -10.039061 ]\n",
      " [ -2.358398   -8.364256 ]\n",
      " [ -1.8505857  -6.6479483]\n",
      " [ -1.3452146  -4.9414053]]\n",
      "Now a([[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]) = [[ 17.007812  21.917969  26.051758]\n",
      " [ 44.47754   60.897453  74.35839 ]\n",
      " [ 71.947266  99.876945 122.66502 ]\n",
      " [ 99.41699  138.85643  170.97166 ]], loss = 323.4553527832031\n",
      "state_vector is [ 4.487304  7.020507  9.084959  9.247559 12.469237 15.068358  7.760254\n",
      "  9.44873  10.983398]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(jnp.arange(9))\n",
    "    x = jnp.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    y = a(x)\n",
    "\n",
    "    # It back-propagages the loss gradient\n",
    "\n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = x @ arangep(2*3).reshape(2,3) + arangep(3,6) # A known, different parameter setting\n",
    "    print(f\"y - ideal =\\n{y - ideal}\")\n",
    "    #loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss = lambda v: jnp.einsum('ij,ij', v-ideal, v-ideal) / (2 * v.shape[0])\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x =\\n{x}\\nideal =\\n{ideal}\\ny =\\n{y}, loss = {loss_at_y}\\n\")\n",
    "\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.tensor_grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) =\\n{grad_y}\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.tensor_grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) =\\n{grad_x}\")\n",
    "            \n",
    "    # Back-propagate the loss gradient from layer output to input\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.01 # Backprop one percent of the loss gradient\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    \n",
    "    # The loss gradient back-propagated to the layer input is correct\n",
    "    #assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")\n",
    "    # FIXME: Check the change is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test batch operations when the affine layer has only one input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* input and output widths\n",
    "* forward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(1,3)\n",
    "    a.set_state_from_vector(jnp.arange(6))\n",
    "    \n",
    "    # The input and output widths for the forward calculation are correct\n",
    "    x = one_wide\n",
    "    y = a(one_wide)\n",
    "    assert y.shape[0] == x.shape[0]\n",
    "    assert y.shape[1] == 3\n",
    "    \n",
    "    # The input and output widths for the backprop calculation are correct\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    assert bp.shape[0] == three_wide.shape[0]\n",
    "    assert bp.shape[1] == x.shape[1]\n",
    "\n",
    "    # The forward calculation is correct (in at least two instances)\n",
    "    a.set_state_from_vector(jnp.arange(6))\n",
    "    x = jnp.array([[0],\n",
    "                  [1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "    assert (y == jnp.array([[ 3.,  4.,  5.],\n",
    "                           [ 3.,  5.,  7.],\n",
    "                           [ 3.,  6.,  9.],\n",
    "                           [ 3.,  7., 11.]])).all()\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "    a.set_state_from_vector(jnp.array([ 2,  3,  5,  7, 11, 13]))\n",
    "    y = a(x)\n",
    "    #print(f\"x is: {x}\\ny is: {y}\")\n",
    "    assert (a(x) == jnp.array([[ 7, 11, 13],\n",
    "                              [ 9, 14, 18],\n",
    "                              [11, 17, 23],\n",
    "                              [13, 20, 28]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* back-propagation of the loss gradient\n",
    "* learning (change in forward function) from the back-prop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y - ideal =\n",
      "[[-14 -15 -18]\n",
      " [-16 -17 -21]\n",
      " [-18 -19 -24]\n",
      " [-20 -21 -27]]\n",
      "x =\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "ideal =\n",
      "[[17 19 23]\n",
      " [19 22 28]\n",
      " [21 25 33]\n",
      " [23 28 38]]\n",
      "y =\n",
      "[[ 3  4  5]\n",
      " [ 3  5  7]\n",
      " [ 3  6  9]\n",
      " [ 3  7 11]], loss = 570.25\n",
      "\n",
      "∇𝑙𝑜𝑠𝑠(𝑦) =\n",
      "[[-22.613523 -25.970457 -33.935543]\n",
      " [-22.644041 -24.627684 -31.311033]\n",
      " [-22.644041 -23.254393 -28.625486]\n",
      " [-22.674559 -21.972654 -26.000975]]\n",
      "∇𝑙𝑜𝑠𝑠(𝑥) =\n",
      "[[-93.90258]\n",
      " [-87.3413 ]\n",
      " [-80.62743]\n",
      " [-73.9746 ]]\n",
      "backprop([[-0.22613522 -0.25970456 -0.3393554 ]\n",
      " [-0.2264404  -0.24627683 -0.31311032]\n",
      " [-0.2264404  -0.23254392 -0.28625485]\n",
      " [-0.22674558 -0.21972653 -0.26000974]]) = [[-0.9384154 ]\n",
      " [-0.87249744]\n",
      " [-0.8050536 ]\n",
      " [-0.739746  ]]\n",
      "Now a([[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]) = [[ 3.9057617  4.958252   6.1987305]\n",
      " [ 5.26532    7.3287964  9.86438  ]\n",
      " [ 6.6248775  9.699341  13.530029 ]\n",
      " [ 7.984435  12.069884  17.195679 ]], loss = 389.4485778808594\n",
      "state_vector is [1.3595579 2.3705442 3.6656492 3.9057617 4.958252  6.1987305]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(1,3)\n",
    "    a.set_state_from_vector(jnp.arange(6))\n",
    "    x = jnp.array([[0],\n",
    "                  [1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "    y = a(x)\n",
    "    #print(f\"x =\\n{x}\\ny =\\n{y}\")\n",
    "    \n",
    "    # It back-propagages the loss gradient\n",
    "\n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = x @ arangep(1*3).reshape(1,3) + arangep(3,6) # A known, different parameter setting\n",
    "    print(f\"y - ideal =\\n{y - ideal}\")\n",
    "    #loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss = lambda v: jnp.einsum('ij,ij', v-ideal, v-ideal) / (2 * v.shape[0])\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x =\\n{x}\\nideal =\\n{ideal}\\ny =\\n{y}, loss = {loss_at_y}\\n\")\n",
    "\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.tensor_grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) =\\n{grad_y}\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.tensor_grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) =\\n{grad_x}\")\n",
    "            \n",
    "    # Back-propagate the loss gradient from layer output to input\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.01 # Backprop one percent of the loss gradient\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    \n",
    "    # The loss gradient back-propagated to the layer input is correct\n",
    "    #assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")\n",
    "    # FIXME: Check the change is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest, the empty network, does identity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    net = Network()\n",
    "    assert all(x == net(x) for x in [0, 42, 'cows in trouble'])\n",
    "    assert all((x == net(x)).all() for x in [np.arange(7), jnp.arange(3*4*5).reshape(3,4,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stack of maps composes the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    net = Network()\n",
    "    net.extend(MapLayer(lambda x: x**3, lambda d: 3*d**2))\n",
    "    x = jnp.array([0, 2, 3, 42, -3.14])\n",
    "    assert (net(x) == x**3).all()\n",
    "    net.extend(MapLayer(lambda x: 7-x, lambda d: -1))\n",
    "    assert (net(x) == 7-x**3).all()\n",
    "    \n",
    "    # It operates on each element of an input vector separately\n",
    "    assert (net(jnp.arange(4)) == 7 - jnp.arange(4) ** 3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A composition of affine transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_[to do someday]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test simple batch learning of a single affine layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Net has state [0 1 2 3 4 5 6 7 8]\n",
      "facts are:\n",
      "[(DeviceArray([[0, 1],\n",
      "             [2, 3],\n",
      "             [4, 5],\n",
      "             [6, 7]], dtype=int32), DeviceArray([[ 24,  30,  36],\n",
      "             [ 42,  58,  72],\n",
      "             [ 60,  86, 108],\n",
      "             [ 78, 114, 144]], dtype=int32))]\n",
      "\n",
      "net(x) =\n",
      "[[ 9 11 13]\n",
      " [15 21 27]\n",
      " [21 31 41]\n",
      " [27 41 55]]\n",
      "net.learn(facts) = 3765.5\n",
      "net.learn(facts) = 1605.49853515625\n",
      "net.learn(facts) = 708.707275390625\n",
      "net.learn(facts) = 336.169189453125\n",
      "net.learn(facts) = 181.206787109375\n",
      "net.learn(facts) = 116.54439544677734\n",
      "net.learn(facts) = 89.3607177734375\n",
      "net.learn(facts) = 77.7341537475586\n",
      "net.learn(facts) = 72.56697082519531\n",
      "net.learn(facts) = 70.0833511352539\n",
      "net(x) =\n",
      "[[ 14.699151  18.972603  22.707714]\n",
      " [ 36.72516   51.692673  64.392944]\n",
      " [ 58.75116   84.41274  106.078186]\n",
      " [ 80.77717  117.13282  147.76343 ]]\n",
      "net.learn(fact_ory(facts[0],10)) = 63.1659049987793\n",
      "net(x) =\n",
      "[[ 15.146448  19.51728   23.365414]\n",
      " [ 37.197094  52.312637  65.145096]\n",
      " [ 59.24774   85.107994 106.924774]\n",
      " [ 81.29839  117.90334  148.70447 ]]\n",
      "did 10000 more learnings of fact. Now loss is 1.9352683011675254e-08\n",
      "net(x) =\n",
      "[[ 23.999866  29.99986   35.999733]\n",
      " [ 41.999924  57.99992   71.99985 ]\n",
      " [ 59.99998   85.999985 107.99996 ]\n",
      " [ 78.00004  114.00004  144.00008 ]]\n",
      "net.state_vector() = [-0.999908   0.6666431  1.6668558  9.999937  13.333387  16.333202\n",
      " 13.999928  16.666473  19.666529 ]\n",
      "\n",
      "Reset net to state [0 1 2 3 4 5 6 7 8]\n",
      "did 10000 learnings of fact. Now loss is 1.9352683011675254e-08\n",
      "net(x) =\n",
      "[[ 23.999866  29.99986   35.999733]\n",
      " [ 41.999924  57.99992   71.99985 ]\n",
      " [ 59.99998   85.999985 107.99996 ]\n",
      " [ 78.00004  114.00004  144.00008 ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    net = Network()\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(jnp.arange(9)) # A well-known initial state\n",
    "    net.extend(a)\n",
    "    print(f\"\\nNet has state {net.state_vector()}\")\n",
    "\n",
    "    x = jnp.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "\n",
    "    # The net wraps the layer\n",
    "    y = a(x)\n",
    "    assert (net(x) == y).all()\n",
    "    \n",
    "    # Make the training batch.\n",
    "    # We use a separate affine layer, initialized differently, to determine the ideal\n",
    "    t_a = AffineLayer(2,3)\n",
    "    t_a.set_state_from_vector(arangep(9)) # A known different initial state (of primes)\n",
    "    ideal = t_a(x)\n",
    "    \n",
    "    facts = [(x, ideal)]\n",
    "    print(f\"facts are:\\n{facts}\\n\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    net.eta = 0.01\n",
    "    for i in range(10):\n",
    "        print(f\"net.learn(facts) = {net.learn(facts)}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    # A simple fact yielder:\n",
    "    def fact_ory(facts, n):\n",
    "        for i in range(n):\n",
    "            yield facts\n",
    "    \n",
    "    #print(f\"list(fact_ory(facts[0], 3)) =\\n{list(fact_ory(facts[0], 3))}\\n\")\n",
    "    print(f\"net.learn(fact_ory(facts[0],10)) = {net.learn(fact_ory(facts[0],10))}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    for i in range(1000):\n",
    "        loss = net.learn(fact_ory(facts[0],10))\n",
    "        if loss < 1e-25:\n",
    "            break\n",
    "    print(f\"did {(i+1)*10} more learnings of fact. Now loss is {loss}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    print(f\"net.state_vector() = {net.state_vector()}\")\n",
    "    \n",
    "    # The network has learned the target transform\n",
    "    assert closenuf(net(x), facts[0][1])\n",
    "    \n",
    "    # Save prior results and learn again, with different batch clustering\n",
    "    prev_run_loss = loss\n",
    "    prev_y = net(x)\n",
    "    net.set_state_from_vector(jnp.arange(9)) # A well-known initial state\n",
    "    print(f\"\\nReset net to state {net.state_vector()}\")\n",
    "\n",
    "    # Try multiple batches in each call to Network.learn\n",
    "    def multibatch_fact_ory(facts, n):\n",
    "        for i in range(n//2):\n",
    "            yield facts * 2\n",
    "\n",
    "    for i in range(1000):\n",
    "        loss = net.learn(fact_ory(facts[0],10))\n",
    "        if loss < 1e-25:\n",
    "            break\n",
    "    print(f\"did {(i+1)*10} learnings of fact. Now loss is {loss}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    # The results should match exactly\n",
    "    assert loss == prev_run_loss\n",
    "    assert (net(x) == prev_y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Network.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Make a network. Leave it with the default identity behavior.\n",
    "    net = Network()\n",
    "    \n",
    "    x = jnp.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    ideal = net(x)\n",
    "    facts = [(x, ideal), (x, ideal-np.array([1,-1])), (x, 2*x)]\n",
    "    assert (net.losses(facts) == [0, 1, 17.5])\n",
    "\n",
    "    # Add some layers\n",
    "    net.extend(AffineLayer(2,3)).extend(MapLayer(jnp.sin, jnp.cos)).extend(AffineLayer(3,2))\n",
    "    # Place it in a known state for test repeatability\n",
    "    net.set_state_from_vector(jnp.arange(len(net.state_vector())))\n",
    "    ideal = net(x)\n",
    "    facts = [(x, ideal), (x, ideal-np.array([1,-1]))]\n",
    "    #print(net.losses(facts))\n",
    "    assert (net.losses(facts) == [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce an importable `nn.py`:\n",
    "1. Save this notebook\n",
    "1. Uncomment the `jupyter nbconvert` line below\n",
    "1. Execute it.\n",
    "1. Comment out the convert again\n",
    "1. Save the notebook again in that form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###!jupyter nbconvert --to script nn-jax.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
