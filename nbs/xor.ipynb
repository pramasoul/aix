{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make `xor` with a net?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "#%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = np.einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xor(x,y)` is x != y, viz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "True ^ True, False ^ False, True ^ False, False ^ True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adopt \"CMOS logic levels\", where less than 1/2 is False, greater is True. Then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flexor(a:float, b:float) -> float:\n",
    "    return 1.0 if (a < 0.5) ^ (b < 0.5) else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flexor(0, 0), flexor(0,1), flexor(1,0), flexor(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting functions\n",
    "We need some functions implementing nonlinear operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = np.vectorize(lambda x: max(0.0,x))\n",
    "t = (np.arange(10).reshape(5,2) - 4.5)/2\n",
    "t, relu(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = np.vectorize(lambda x: max(0, np.sign(x)))\n",
    "positive(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_gradient = np.vectorize(lambda x: 1-np.tanh(x)**2)\n",
    "tanh_gradient(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs to xor are an array of vectors, e.g. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([0,0, 0,1, 1,0, 1,1]).reshape(4,2); X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first parameter is an input offset vector. We'll use \"perfect\" values for the parameters for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0 = np.array([-0.5, -0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = X + v0; h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a matrix multiply, offset, and nonlinear activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = np.array([[ 1.0, -1.0],\n",
    "               [-1.0,  1.0]])\n",
    "h2 = h1 @ m1; h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([0.0, 0.0])\n",
    "h3 = h2 + v1; h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4 = relu(h3); h4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "followed by an output matrix, which in this single-output case is a dot-product and offset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = np.array([1.0, 1.0])\n",
    "h5 = h4.dot(v2); h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3 = np.array([-0.5])\n",
    "h6 = h5 + v3; h6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step we make the output discrete, effectively boolean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h7 = positive(h6); h7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put it in a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xnet_1:\n",
    "    \"Calculate exclusive-or using a network\"\n",
    "    domain = np.array([0,0, 0,1, 1,0, 1,1]).reshape(4,2)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.randomize()\n",
    "\n",
    "    \"Calculates the ideal return value directly, to provide a reference\"\n",
    "    ideal = np.vectorize(lambda x: 1 if (x[0] > 0.5) ^ (x[1] > 0.5) else 0,\n",
    "                            signature='(i)->()')\n",
    "\n",
    "    analog_ideal = np.vectorize(lambda x: 0.5 if (x[0] > 0.5) ^ (x[1] > 0.5) else -0.5,\n",
    "                            signature='(i)->()')\n",
    "\n",
    "    def randomize(self):\n",
    "        \"Set new, random parameters for the network\"\n",
    "        self.v0 = np.random.randn(2)\n",
    "        self.m1 = np.random.randn(2,2)\n",
    "        self.v1 = np.random.randn(2)\n",
    "        self.m2 = np.random.randn(2)\n",
    "        self.v2 = np.random.randn(2)\n",
    "        self.v3 = np.random.randn(1)\n",
    "        self.current = False\n",
    "        return self\n",
    "\n",
    "    def make_perfect(self):\n",
    "        \"Set the matricies to a handmade value that gives perfect behavior\"\n",
    "        self.v0 = np.array([-0.5, -0.5])\n",
    "        self.m1 = np.array([[ 1.0, -1.0],\n",
    "                            [-1.0,  1.0]])\n",
    "        self.v1 = np.array([0.0, 0.0])\n",
    "        self.m2 = np.array([1.0, 1.0])\n",
    "        self.v2 = np.array([1.0, 1.0])\n",
    "        self.v3 = np.array([-0.5])\n",
    "        self.current = False\n",
    "        return self\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        if not self.current:\n",
    "            self._calculate_forward(X)\n",
    "            self.current = True\n",
    "\n",
    "    def _calculate_forward(self, X):\n",
    "        self.X = X\n",
    "        self.h1 = X + self.v0\n",
    "        self.h2 = self.h1 @ self.m1\n",
    "        self.h3 = self.h2 + self.v1\n",
    "        self.h4 = relu(self.h3)\n",
    "        self.h5 = self.h4.dot(self.v2)\n",
    "        self.h6 = self.h5 + self.v3\n",
    "        self.h7 = positive(self.h6)\n",
    "    \n",
    "    def _propagate_backward(self):\n",
    "        self.e = self.h6 - self.analog_ideal(self.X)\n",
    "        self.loss = self.e.dot(self.e)/(2.0 * len(self.X))\n",
    "        self.dlossde = self.e\n",
    "        self.dh6ds6_v3 = np.ones(self.v3.shape)\n",
    "        self.dh6dh5 = np.ones(self.v3.shape)\n",
    "        self.dh5ds5_v2 = self.h4\n",
    "        self.dh5dh4 = self.v2\n",
    "        self.dh4ds4_relu = relu(self.h3)\n",
    "        self.dh4dh3 = positive(self.h3)\n",
    "        self.dh3ds3_v1 = np.ones(self.v1.shape)\n",
    "        self.dh3dh2 = np.ones(self.v1.shape)\n",
    "        self.dh2ds2_m1 = \n",
    "\n",
    "    \n",
    "    def analog(self, X):\n",
    "        self._forward(X)\n",
    "        return self.h6\n",
    "\n",
    "    def analog_error_and_loss(self):\n",
    "        self.current = False\n",
    "        self.e = self.analog(self.domain) - self.analog_ideal(self.domain)\n",
    "        self.loss = self.e.dot(self.e)/(2.0 * len(self.domain))\n",
    "        return self.e, self.loss\n",
    "    \"\"\"\n",
    "    def analog_loss(self):\n",
    "        assert self.current\n",
    "        return self.loss\n",
    "    \"\"\"\n",
    "    def __call__(self, X):\n",
    "        self.current = False\n",
    "        self._forward(X)\n",
    "        return self.h7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Xnet_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.analog(net.domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.make_perfect()\n",
    "net.analog(net.domain), net(net.domain), net.ideal(net.domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.analog(net.domain) - net.analog_ideal(net.domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.randomize().analog(net.domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.analog_error_and_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.make_perfect(); net.analog_error_and_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we randomize only the last vector (v3)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.v3 = np.random.randn(1); net.v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, loss = net.analog_error_and_loss(); e, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.v3 -= np.mean(e); net.v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.analog_error_and_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.make_perfect()\n",
    "net.v2 = np.random.randn(2)\n",
    "e, loss = net.analog_error_and_loss()\n",
    "print(f\"v2={net.v2}\\ne={e}\\nloss={loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.h4, net.h5, net.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.e @ net.h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.v2 -= np.mean(h4, axis=0); net.v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Netxor:\n",
    "    \"\"\"Calculate exclusive-or using a network\"\"\"\n",
    "    def __init__(self):\n",
    "        self.m0 = np.array([-0.5, -0.5])\n",
    "        self.randomize()\n",
    "        \n",
    "    def randomize(self):\n",
    "        \"Randomize the matricies\"\n",
    "        self.m1 = np.random.randn(2,2)\n",
    "        self.m2 = np.random.randn(2)\n",
    "        return self\n",
    "    \n",
    "    def make_perfect(self):\n",
    "        \"Set the matricies to a handmade value that gives perfect behavior\"\n",
    "        self.m0 = np.array([-0.5, -0.5])\n",
    "        self.m1 = np.array([[ 1.0, -1.0],\n",
    "                            [-1.0,  1.0]])\n",
    "        self.m2 = np.array([1.0, 1.0])\n",
    "        return self\n",
    "    \n",
    "    def ideal(self, a:float, b:float):\n",
    "        \"Calculates the ideal return value directly, to provide a reference\"\n",
    "        return 1 if (a > 0.5) ^ (b > 0.5) else 0\n",
    "    \n",
    "    def netwise(self, a:float, b:float):\n",
    "        \"Calculate a single result using network primitives\"\n",
    "        v = self.net_lin(a, b)\n",
    "        v = self.p5 = 0 if v < 0.5 else 1\n",
    "        return v\n",
    "\n",
    "    def net_ana(self, a:float, b:float):\n",
    "        \"Calculate a single analog result using network primitives\"\n",
    "        v = self.p4 = np.tanh(self.net_lin(a, b))\n",
    "        return v\n",
    "\n",
    "    def net_lin(self, a:float, b:float):\n",
    "        \"The network output up to the last linear stage\"\n",
    "        input = np.array([a, b])\n",
    "        v = self.p0 = self.m0 + input\n",
    "        v = self.p1 = self.m1 @ v\n",
    "        v = self.p2 = relu(v)\n",
    "        v = self.p3 = np.dot(self.m2, v)\n",
    "        return v\n",
    "\n",
    "    def __call__(self, a, b):\n",
    "        \"Vectorized calculation of result using network\"\n",
    "        return np.vectorize(self.netwise)(a, b)\n",
    "\n",
    "    def loss(self):\n",
    "        \"L2 loss function of the network implementation\"\n",
    "        return sum((self.__call__(x,y) - self.ideal(x,y))**2 for x in (-1, 1) for y in (-1, 1))\n",
    "    \n",
    "    def aloss(self):\n",
    "        \"L2 loss function of the network analog implementation\"\n",
    "        return sum((self.net_ana(x,y) - self.ideal(x,y))**2 for x in (-1, 1) for y in (-1, 1))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return F\"Exactor m0={self.m0}, m1={self.m1}, m2={self.m2})\"\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor = Exactor()\n",
    "print(exor)\n",
    "exor.net_lin(1.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor.net_ana(1.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor.netwise(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in (0, 1):\n",
    "    for y in (0, 1):\n",
    "        print(exor.ideal(x, y), exor(x,y), exor.net_ana(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor.loss(), exor.aloss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exor.make_perfect().loss())\n",
    "print(exor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in (0, 1):\n",
    "    for y in (0, 1):\n",
    "        print(exor.ideal(x, y), exor(x,y), exor.p0, exor.p1, exor.p2, exor.p3, exor.p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor.loss(), exor.aloss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor.randomize()\n",
    "exor.loss(), exor.aloss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we find working matricies by trying random matricies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1e300\n",
    "best_aloss = 1e300\n",
    "best_repr = \"\"\n",
    "for n in range(10000):\n",
    "    exor.randomize()\n",
    "    #if n == 6789:\n",
    "    #    exor.make_perfect()\n",
    "    loss = exor.loss()\n",
    "    if best_loss > loss:\n",
    "        best_loss = loss\n",
    "        best_repr = repr(exor)\n",
    "    aloss = exor.aloss()\n",
    "    if best_aloss > aloss:\n",
    "        best_aloss = aloss\n",
    "        best_arepr = repr(exor)\n",
    "    if loss == 0:\n",
    "        print(F\"Success after {n+1} tries: {exor}\")\n",
    "        break\n",
    "if exor.loss() > 0:\n",
    "    print(F\"Failure, none of {n+1} random tries worked\")\n",
    "print(f\"best net loss:{best_loss}, Best net:{best_repr}\")\n",
    "print(f\"best net aloss:{best_aloss}, Best anet:{best_arepr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density of successful random nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((1 if exor.randomize().loss() == 0.0 else 0) for i in range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [exor.randomize().loss() for i in range(100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alosses = np.array([exor.randomize().aloss() for i in range(100000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(alosses, bins=250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A less delicate network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class was_Flexor:\n",
    "    \"\"\"A more robust XOR\"\"\"\n",
    "    def __init__(self):\n",
    "        self.m0 = np.array([-0.5, -0.5])\n",
    "        self.randomize()\n",
    "        \n",
    "    def randomize(self):\n",
    "        \"Randomize the matricies\"\n",
    "        self.m1 = np.random.randn(4,2)\n",
    "        self.m2 = np.random.randn(4)\n",
    "    \n",
    "    def make_perfect(self):\n",
    "        \"Set the matricies to a handmade value that gives perfect behavior\"\n",
    "        self.m1 = np.array([[ 1.0, -1.0],\n",
    "                            [-1.0,  1.0],\n",
    "                            [ 1.0,  1.0],\n",
    "                            [-1.0, -1.0]])\n",
    "        self.m2 = np.array([1.0, 1.0, -1.0, -1.0])\n",
    "    \n",
    "    def ideal(self, a:float, b:float) -> bool:\n",
    "        \"Calculates the ideal return value directly, to provide a reference\"\n",
    "        return 1 if (a > 0.5) ^ (b > 0.5) else 0\n",
    "    \n",
    "    def netwise(self, a:float, b:float) -> bool:\n",
    "        \"Calculate a single result using network primitives\"\n",
    "        input = np.array([[a],\n",
    "                          [b]])\n",
    "        v = self.p1 = self.m1 @ input\n",
    "        v = self.p2 = relu(v)\n",
    "        v = self.p3 = np.dot(self.m2, v)\n",
    "        v = self.p4 = relu(v)\n",
    "        v = self.p5 = (-1,1)[int(np.sign(v)[0])]\n",
    "        return v\n",
    "\n",
    "    def __call__(self, a, b):\n",
    "        \"Vectorized calculation of result using network\"\n",
    "        return np.vectorize(self.netwise)(a, b)\n",
    "\n",
    "    def loss(self):\n",
    "        \"L2 loss function of the network implementation\"\n",
    "        return sum((self.__call__(x,y) - self.ideal(x,y))**2 for x in (-1, 1) for y in (-1, 1))\n",
    "    \n",
    "    def goodness(self):\n",
    "        \"analog goodness function\"\n",
    "        rv = 0.0\n",
    "        for x in (-1, 1):\n",
    "            for y in (-1, 1):\n",
    "                _ = self.netwise(x, y)\n",
    "                rv += self.p4 * self.ideal(x,y)\n",
    "        return rv[0]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return F\"Flexor(m1={self.m1}, m2={self.m2})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flexor:\n",
    "    \"\"\"A more robust XOR using a network\"\"\"\n",
    "    def __init__(self):\n",
    "        self.m0 = np.array([-0.5, -0.5])\n",
    "        self.randomize()\n",
    "        \n",
    "    def randomize(self):\n",
    "        \"Randomize the matricies\"\n",
    "        self.m1 = np.random.randn(4,2)\n",
    "        self.m2 = np.random.randn(4)\n",
    "    \n",
    "    def make_perfect(self):\n",
    "        \"Set the matricies to a handmade value that gives perfect behavior\"\n",
    "        self.m0 = np.array([-0.5, -0.5])\n",
    "        self.m1 = np.array([[ 1.0, -1.0],\n",
    "                            [-1.0,  1.0],\n",
    "                            [ 1.0,  1.0],\n",
    "                            [-1.0, -1.0]])\n",
    "        self.m2 = np.array([1.0, 1.0, -1.0, -1.0])\n",
    "    \n",
    "    def ideal(self, a:float, b:float) -> bool:\n",
    "        \"Calculates the ideal return value directly, to provide a reference\"\n",
    "        return 1 if (a > 0.5) ^ (b > 0.5) else 0\n",
    "    \n",
    "    def netwise(self, a:float, b:float):\n",
    "        \"Calculate a single result using network primitives\"\n",
    "        v = self.net_lin(a, b)\n",
    "        v = self.p5 = 0 if v < 0.5 else 1\n",
    "        return v\n",
    "\n",
    "    def net_ana(self, a:float, b:float):\n",
    "        \"Calculate a single analog result using network primitives\"\n",
    "        v = self.p4 = np.tanh(self.net_lin(a, b))\n",
    "        return v\n",
    "\n",
    "    def net_lin(self, a:float, b:float):\n",
    "        \"The network output up to the last linear stage\"\n",
    "        input = np.array([a, b])\n",
    "        v = self.p0 = self.m0 + input\n",
    "        v = self.p1 = self.m1 @ input\n",
    "        v = self.p2 = relu(v)\n",
    "        v = self.p3 = np.dot(self.m2, v)\n",
    "        return v\n",
    "\n",
    "    def __call__(self, a, b):\n",
    "        \"Vectorized calculation of result using network\"\n",
    "        return np.vectorize(self.netwise)(a, b)\n",
    "\n",
    "    def loss(self):\n",
    "        \"L2 loss function of the network implementation\"\n",
    "        return sum((self.__call__(x,y) - self.ideal(x,y))**2 for x in (-1, 1) for y in (-1, 1))\n",
    "    \n",
    "    def aloss(self):\n",
    "        \"L2 loss function of the network analog implementation\"\n",
    "        return sum((self.net_ana(x,y) - self.ideal(x,y))**2 for x in (-1, 1) for y in (-1, 1))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return F\"Exactor m0={self.m0}, m1={self.m1}, m2={self.m2})\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor = Flexor()\n",
    "flor.m1, flor.m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in (0, 1):\n",
    "    for y in (0, 1):\n",
    "        print(flor.ideal(x, y), '\\tres:', flor(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.loss(), flor.aloss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.make_perfect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in (-1, 1):\n",
    "    for y in (-1, 1):\n",
    "        print(flor.ideal(x, y), flor(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.loss(), flor.aloss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.randomize()\n",
    "flor.loss(), flor.aloss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we find working matricies by trying random matricies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10000):\n",
    "    flor.randomize()\n",
    "    if flor.loss() == 0:\n",
    "        print(F\"Success after {n+1} tries:\\n{flor}\")\n",
    "        break\n",
    "if flor.loss() > 0:\n",
    "    print(F\"Failure, none of {n+1} random tries worked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.loss(), flor.aloss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g. [Neural Network from scratch in Python](https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
