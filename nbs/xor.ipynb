{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make `xor` with a net?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xor(x,y)` is x != y, viz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False, True, True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True ^ True, False ^ False, True ^ False, False ^ True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use -1 to represent one boolean condition, and +1 the other. Then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flexor(a:float, b:float) -> float:\n",
    "    return a*b < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False, True, True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flexor(-1, -1), flexor(1,1), flexor(-1,1), flexor(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions\n",
    "We need some functions implementing nonlinear operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-2.0, array(0.)),\n",
       " (-1.5, array(0.)),\n",
       " (-1.0, array(0.)),\n",
       " (-0.5, array(0.)),\n",
       " (0.0, array(0.)),\n",
       " (0.5, array(0.5)),\n",
       " (1.0, array(1.)),\n",
       " (1.5, array(1.5))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relu(x):\n",
    "    #return (lambda v: max(0,v))(x)\n",
    "    return np.vectorize(lambda v: max(0.0,v))(x)\n",
    "\n",
    "[(v, relu(v)) for v in np.arange(-2, 2, 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-2.0, array(0)),\n",
       " (-1.5, array(0)),\n",
       " (-1.0, array(0)),\n",
       " (-0.5, array(0)),\n",
       " (0.0, array(0)),\n",
       " (0.5, array(1.)),\n",
       " (1.0, array(1.)),\n",
       " (1.5, array(1.))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positive(x):\n",
    "    return np.vectorize(lambda v: max(0, np.sign(v)))(x)\n",
    "\n",
    "[(v, positive(v)) for v in np.arange(-2, 2, 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_gradient(x):\n",
    "    return 1-np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frombits(v) -> int:\n",
    "    p = 1\n",
    "    s = 0\n",
    "    for bit in v:\n",
    "        s += p * bit\n",
    "        p <<= 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frombits([1,0,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exactor:\n",
    "    \"\"\"Calculate exclusive-or using a network\"\"\"\n",
    "    def __init__(self):\n",
    "        self.randomize()\n",
    "        \n",
    "    def randomize(self):\n",
    "        \"Randomize the matricies\"\n",
    "        self.m1 = np.random.randn(2,2)\n",
    "        self.m2 = np.random.randn(2)\n",
    "    \n",
    "    def make_perfect(self):\n",
    "        \"Set the matricies to a handmade value that gives perfect behavior\"\n",
    "        self.m1 = np.array([[ 1.0, -1.0],\n",
    "                            [-1.0,  1.0]])\n",
    "        self.m2 = np.array([1.0, 1.0])\n",
    "    \n",
    "    def ideal(self, a:float, b:float) -> bool:\n",
    "        \"Calculates the ideal return value directly, to provide a reference\"\n",
    "        return (-1.0,1.0)[a*b < 0]\n",
    "        #return np.sign(-a*b)\n",
    "    \n",
    "    def netwise(self, a:float, b:float):\n",
    "        \"Calculate a single result using network primitives\"\n",
    "        v = self.net_lin(a, b)\n",
    "        v = self.p5 = (-1,1)[int(np.sign(v-0.5)[0])]\n",
    "        self.p5 = v\n",
    "        return v\n",
    "\n",
    "    def net_ana(self, a:float, b:float):\n",
    "        \"Calculate a single analog result using network primitives\"\n",
    "        v = self.p4 = np.tanh(self.net_lin(a, b))[0]\n",
    "        return v\n",
    "\n",
    "    def net_lin(self, a:float, b:float):\n",
    "        \"The network output up to the last linear stage\"\n",
    "        input = np.array([[a],\n",
    "                          [b]])\n",
    "        v = self.p1 = self.m1 @ input\n",
    "        v = self.p2 = relu(v)\n",
    "        v = self.p3 = np.dot(self.m2, v)\n",
    "        return v\n",
    "\n",
    "    def __call__(self, a, b, analog=False):\n",
    "        \"Vectorized calculation of result using network\"\n",
    "        return np.vectorize(self.netwise)(a, b)\n",
    "\n",
    "    def loss(self, analog=False):\n",
    "        \"L2 loss function of the network implementation\"\n",
    "        return sum((self.__call__(x,y, analog) - self.ideal(x,y))**2 for x in (-1, 1) for y in (-1, 1))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return F\"Exactor m1={self.m1}, m2={self.m2})\"\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor = Exactor()\n",
    "exor\n",
    "exor.net_lin(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor.net_ana(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor.netwise(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in (-1, 1):\n",
    "    for y in (-1, 1):\n",
    "        print(exor.ideal(x, y), exor(x,y), exor.net_ana(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor.loss(), exor.loss(analog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor.make_perfect()\n",
    "print(exor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in (-1, 1):\n",
    "    for y in (-1, 1):\n",
    "        print(exor.ideal(x, y), exor(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor.loss(), exor.loss(analog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exor.randomize()\n",
    "exor.loss(), exor.loss(analog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we find working matricies by trying random matricies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1e300\n",
    "best_repr = \"\"\n",
    "for n in range(10000):\n",
    "    exor.randomize()\n",
    "    #if n == 6789:\n",
    "    #    exor.make_perfect()\n",
    "    loss = exor.loss()\n",
    "    if best_loss > loss:\n",
    "        best_loss = loss\n",
    "        best_repr = repr(exor)\n",
    "    if loss == 0:\n",
    "        print(F\"Success after {n+1} tries: {exor}\")\n",
    "        break\n",
    "if exor.loss() > 0:\n",
    "    print(F\"Failure, none of {n+1} random tries worked\")\n",
    "print(f\"best net loss:{best_loss}, Best net:{best_repr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A less delicate network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flexor:\n",
    "    \"\"\"A more robust XOR\n",
    "    Returns the sign of (|x-y| - |x+y|) where x, y are inputs\"\"\"\n",
    "    def __init__(self):\n",
    "        self.randomize()\n",
    "        \n",
    "    def randomize(self):\n",
    "        \"Randomize the matricies\"\n",
    "        self.m1 = np.random.randn(4,2)\n",
    "        self.m2 = np.random.randn(4)\n",
    "    \n",
    "    def make_perfect(self):\n",
    "        \"Set the matricies to a handmade value that gives perfect behavior\"\n",
    "        self.m1 = np.array([[ 1.0, -1.0],\n",
    "                            [-1.0,  1.0],\n",
    "                            [ 1.0,  1.0],\n",
    "                            [-1.0, -1.0]])\n",
    "        self.m2 = np.array([1.0, 1.0, -1.0, -1.0])\n",
    "    \n",
    "    def ideal(self, a:float, b:float) -> bool:\n",
    "        \"Calculates the ideal return value directly, to provide a reference\"\n",
    "        return np.sign(-a*b)\n",
    "    \n",
    "    def netwise(self, a:float, b:float) -> bool:\n",
    "        \"Calculate a single result using network primitives\"\n",
    "        input = np.array([[a],\n",
    "                          [b]])\n",
    "        v = self.p1 = self.m1 @ input\n",
    "        v = self.p2 = relu(v)\n",
    "        v = self.p3 = np.dot(self.m2, v)\n",
    "        v = self.p4 = relu(v)\n",
    "        v = self.p5 = (-1,1)[int(np.sign(v)[0])]\n",
    "        return v\n",
    "\n",
    "    def __call__(self, a, b):\n",
    "        \"Vectorized calculation of result using network\"\n",
    "        return np.vectorize(self.netwise)(a, b)\n",
    "\n",
    "    def loss(self):\n",
    "        \"L2 loss function of the network implementation\"\n",
    "        return sum((self.__call__(x,y) - self.ideal(x,y))**2 for x in (-1, 1) for y in (-1, 1))\n",
    "    \n",
    "    def goodness(self):\n",
    "        \"analog goodness function\"\n",
    "        rv = 0.0\n",
    "        for x in (-1, 1):\n",
    "            for y in (-1, 1):\n",
    "                _ = self.netwise(x, y)\n",
    "                rv += self.p4 * self.ideal(x,y)\n",
    "        return rv[0]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return F\"Flexor(m1={self.m1}, m2={self.m2})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor = Flexor()\n",
    "flor.m1, flor.m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in (-1, 1):\n",
    "    for y in (-1, 1):\n",
    "        print(flor.ideal(x, y), '\\tres:', flor(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.loss(), flor.goodness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.make_perfect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in (-1, 1):\n",
    "    for y in (-1, 1):\n",
    "        print(flor.ideal(x, y), flor(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.loss(), flor.goodness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.randomize()\n",
    "flor.loss(), flor.goodness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we find working matricies by trying random matricies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10000):\n",
    "    flor.randomize()\n",
    "    if flor.loss() == 0:\n",
    "        print(F\"Success after {n+1} tries:\\n{flor}\")\n",
    "        break\n",
    "if flor.loss() > 0:\n",
    "    print(F\"Failure, none of {n+1} random tries worked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flor.loss(), flor.goodness()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
