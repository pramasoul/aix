{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Aside: Should do [Working efficiently with jupyter lab](https://florianwilhelm.info/2018/11/working_efficiently_with_jupyter_lab/)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch our tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import Network, Layer, IdentityLayer, AffineLayer, MapLayer\n",
    "from nnbench import NNBench\n",
    "from nnvis import NNVis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a network to learn exclusive-or\n",
    "We make a two-input, one-output network, using\n",
    "a 2x2 affine followed by tanh activation\n",
    "feeding a 2x1 affine followed by tanh activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.extend(AffineLayer(2,2))\n",
    "net.extend(MapLayer(np.tanh, lambda d: 1.0 - np.tanh(d)**2))\n",
    "net.extend(AffineLayer(2,1))\n",
    "net.extend(MapLayer(np.tanh, lambda d: 1.0 - np.tanh(d)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a test bench and a visualizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = NNBench(net)\n",
    "vis = NNVis(bench)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare fixed training data for the learning process. Set it to be every training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = (np.array([[-0.5, -0.5],\n",
    "                            [-0.5,  0.5],\n",
    "                            [ 0.5,  0.5],\n",
    "                            [ 0.5, -0.5]]),\n",
    "                  np.array([[-0.5],\n",
    "                            [ 0.5],\n",
    "                            [-0.5],\n",
    "                            [ 0.5]]))\n",
    "bench.training_batch = lambda n: [training_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the learnable parameters from a net. They can be modified and injected back if we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.training_batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.state_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the state to an ordinary example starting point, for consistent notebook behavior below. We also make it the checkpoint in the bench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.set_state_from_vector(np.array([-0.88681521, -1.28596788,  0.3248974 , -2.33838503,  0.34761944,\n",
    "       -0.94541789,  1.99448043,  0.38704839, -3.8844268 ]))\n",
    "bench.checkpoint_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this untrained net work across the canonical input domain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = bench.training_batch(4)[0][0]\n",
    "domain, net(domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the ideal answer, which is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.training_batch(4)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "This net doesn't know how to do `xor` yet. Let's try teaching it. \\\n",
    "We can plot the loss as a function of learning steps, for the current learning rate $\\eta$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_learning(100, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The net has changed state from that learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.net.state_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does it work now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We can learn some more, carrying on from where we left off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_learning(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The net is again changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.net.state_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check its behavior now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get to floating-point-$\\epsilon$ perfect with more training. Let's add 800 more, for a total of a thousand learning cycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = bench.learn(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's well-trained, at least for the canonical inputs. What was the resultant net?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.net.state_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the loss as a function of learning steps and learning rate $\\eta$. Get back to our starting point network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.rollback_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the sliders to adjust the number of learning steps, and the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.knobs_plot_learning(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does it work after this training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the loss surface\n",
    "We can plot the loss surface with `plotly`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.rollback_net() if True else bench.randomize_net()\n",
    "rates = np.logspace(-8, 0, base=2, num=32)\n",
    "cube = bench.learn_loss_cube(1000, rates)\n",
    "vis.plot_loss_cube()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The track that learning takes\n",
    "Let us examine the trajectory in state space during learning, and the loss function.\n",
    "Each learning iteration changes the net state. We can examine those deltas.\n",
    "Questions:\n",
    "1. Are there regimes of direction-of-change (DoC) in state space, or does the DoC wander chaotically?\n",
    "1. What are the spectral characteristics of the DoC? Length characteristics?\n",
    "1. How do the DoC characteristics relate to the loss function, and it's first difference?\n",
    "1. How do these trajectories vary with learning rate? Are there clues in these to adapt the learning rate?\n",
    "1. How do the trajectory characteristics vary across different starting nets?\n",
    "1. How do these measures vary with the objective function of the learning process, that is, what you're trying to teach the net?\n",
    "1. How do the different layers with learning state evolve? Do they settle at different times? How does an upstream layer change, as a consequence of learning, affect downstream layers? Down affect up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.rollback_net()\n",
    "bench.net.eta = 0.3\n",
    "learned_track = bench.learn_track(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traja = bench.analyze_learning_track(learned_track)\n",
    "vis.plot_trajectory(traja)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Stop notebook execution here if entering from above\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.randomize_net()\n",
    "t = bench.learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.net.state_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = bench.net\n",
    "net([[-.5, -.5], [-.5, .5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnbench import Thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Thing(color='brown', weight=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cow = 'moo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boneyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Stop notebook execution, the rest is scrap\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangle the state-space trajectory and the losses into form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = np.vstack([v[0] for v in lt])\n",
    "losses = np.vstack([v[1] for v in lt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take first differences, which represent the changes at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_steps = np.diff(trajectory, axis=0)\n",
    "loss_steps = np.diff(losses, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_steps[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the L2 norm of the trajectory steps $\\lVert traj \\rVert$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_L2 = np.sqrt(np.einsum('...i,...i', traj_steps, traj_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(traj_L2), traj_L2[:5], traj_L2[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the angles between trajectory steps, from\n",
    "$$\\mathbf {a} \\cdot \\mathbf {b} = \\left\\|\\mathbf {a} \\right\\|\\left\\|\\mathbf {b} \\right\\|\\cos \\theta \\\\\n",
    "\\cos \\theta = \\frac{\\mathbf {a} \\cdot \\mathbf {b}}{\\left\\|\\mathbf {a} \\right\\|\\left\\|\\mathbf {b} \\right\\|} \\\\\n",
    "$$\n",
    "where $\\mathbf {a}$ and $\\mathbf {b}$ are a state-space trajectory step and the succeeding step respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find $\\mathbf {a} \\cdot \\mathbf {b}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajn_dot_nplus1 = np.einsum('...i,...i', traj_steps[:-1], traj_steps[1:])\n",
    "trajn_dot_nplus1[:5], np.any(trajn_dot_nplus1 < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find $\\left\\|\\mathbf {a} \\right\\|\\left\\|\\mathbf {b} \\right\\|$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cos_denom = np.multiply(traj_L2[:-1], traj_L2[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be the divisor. Some entries may be zero, so we adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(traj_L2) - np.count_nonzero(traj_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.equal(traj_L2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find $\\cos \\theta$ by dividing, excluding division by zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_cos = np.divide(trajn_dot_nplus1, traj_cos_denom, where=traj_cos_denom!=0.0)\n",
    "traj_cos[:5], traj_cos[-5:], min(traj_cos), max(traj_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traj_theta = np.arccos(traj_cos)\n",
    "#traj_theta[:5], traj_theta[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.extend(AffineLayer(2,2))\n",
    "#leak = 0\n",
    "#net.extend(MapLayer(lambda x: (x*(1+leak/2)+abs(x)*(1-leak/2))/2, lambda d: [leak,1][1 if d>0 else 0]))\n",
    "#net.extend(MapLayer(lambda x: max(0, np.sign(x)) * x, lambda d: max(0, np.sign(d))))\n",
    "net.extend(MapLayer(np.tanh, lambda d: 1.0 - np.tanh(d)**2))\n",
    "net.extend(AffineLayer(2,1))\n",
    "net.extend(MapLayer(np.tanh, lambda d: 1.0 - np.tanh(d)**2))\n",
    "#sigmoid = lambda x: 1/(np.exp(x)+1)\n",
    "#net.extend(MapLayer(sigmoid, lambda d: sigmoid(d)*(1-sigmoid(d))))\n",
    "#net.extend(MapLayer(lambda x: max(0, np.sign(x)) * x, lambda d: max(0, np.sign(d))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = \\\n",
    "[(np.array([-1,-1]), np.array([-1])),\n",
    " (np.array([-1,1]), np.array([1])),\n",
    " (np.array([1,1]), np.array([-1])),\n",
    " (np.array([1,-1]), np.array([1]))]\n",
    "dc = 0\n",
    "amp= 1\n",
    "temp = [(d[0]*amp/2+dc,d[1]*amp/2+dc) for d in dat]\n",
    "\n",
    "bench.training_data = ((np.array([v[0] for v in temp]),\n",
    "                        np.array([v[1] for v in temp])),)\n",
    "bench.training_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
