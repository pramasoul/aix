{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Learning\n",
    "In which we adapt the learning rate $\\eta$ during the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch our tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import Network, Layer, IdentityLayer, AffineLayer, MapLayer\n",
    "from nnbench import NNBench\n",
    "from nnvis import NNVis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our `xor` net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.extend(AffineLayer(2,2))\n",
    "net.extend(MapLayer(np.tanh, lambda d: 1.0 - np.tanh(d)**2))\n",
    "net.extend(AffineLayer(2,1))\n",
    "net.extend(MapLayer(np.tanh, lambda d: 1.0 - np.tanh(d)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a test bench and a visualizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = NNBench(net)\n",
    "vis = NNVis(bench)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare fixed training data for the learning process _[improve]_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = \\\n",
    "[(np.array([-1,-1]), np.array([-1])),\n",
    " (np.array([-1,1]), np.array([1])),\n",
    " (np.array([1,1]), np.array([-1])),\n",
    " (np.array([1,-1]), np.array([1]))]\n",
    "dc = 0\n",
    "amp= 1\n",
    "temp = [(d[0]*amp/2+dc,d[1]*amp/2+dc) for d in dat]\n",
    "\n",
    "bench.training_data = ((np.array([v[0] for v in temp]),\n",
    "                        np.array([v[1] for v in temp])),)\n",
    "bench.training_data_gen = bench.training_data_gen_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the state to an ordinary example starting point, for consistent notebook behavior below. We also make it the checkpoint in the bench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.set_state_from_vector(np.array([-0.88681521, -1.28596788,  0.3248974 , -2.33838503,  0.34761944,\n",
    "       -0.94541789,  1.99448043,  0.38704839, -3.8844268 ]))\n",
    "bench.checkpoint_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.rollback_net()\n",
    "n = 10\n",
    "t = bench.analyze_learning_track(bench.learn_track(n))\n",
    "#[(a, eval('t.' + a)) for a in filter(lambda v: not v.startswith('__'), dir(t))]\n",
    "list(filter(lambda v: not v.startswith('__'), dir(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.rollback_net()\n",
    "bench.net.eta = 0.3\n",
    "traja = bench.analyze_learning_track(bench.learn_track(500))\n",
    "vis.plot_trajectory(traja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adaptive_learning(losses):\n",
    "    fig, ax = plt.subplots()  # Create a figure and an axes.\n",
    "    ax.plot(losses, label=f\"$\\eta=FIXME$\")  # Plot some data on the axes.\n",
    "    ax.set_xlabel('learnings')  # Add an x-label to the axes.\n",
    "    ax.set_ylabel('loss')  # Add a y-label to the axes.\n",
    "    ax.set_title(\"Losses\")  # Add a title to the axes.\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()  # Add a legend.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a function of (`analyze_learning_track`'s output object, $\\eta_t$, N) to give\n",
    "($\\eta_t+1$, n, stop_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_learning_t1(traja, eta, n_cum):\n",
    "    return eta, 10, n_cum>3000 or np.mean(traja.losses)<1e-25\n",
    "\n",
    "def adapt_learning_t2(traja, eta, n_cum):\n",
    "    eta += 0.04 * np.mean(traja.traj_cos) * eta\n",
    "    return eta, 3, n_cum>3000 or np.mean(traja.losses)<1e-25\n",
    "\n",
    "def adapt_learning_t3(traja, eta, n_cum):\n",
    "    eta += 0.1 * (np.mean(traja.traj_cos) - 0.7) * eta\n",
    "    return eta, 3, n_cum>3000 or np.mean(traja.losses)<1e-25\n",
    "\n",
    "def adapt_learning_t4(traja, eta, n_cum):\n",
    "    eta += 0.1 * (np.mean(traja.traj_cos) - 0.9) * eta\n",
    "    return eta, 3, n_cum>3000 or np.mean(traja.losses)<1e-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_learning = adapt_learning_t3\n",
    "\n",
    "bench.rollback_net()\n",
    "bench.net.eta = 0.1\n",
    "n = 3\n",
    "n_cum = 0\n",
    "losses = []\n",
    "for i in range(1000):\n",
    "    traja = bench.analyze_learning_track(bench.learn_track(n))\n",
    "    #vis.plot_trajectory(traja)\n",
    "    losses.append(traja.losses)\n",
    "    n_cum += n\n",
    "    eta, n, stop = adapt_learning(traja, bench.net.eta, n_cum)\n",
    "    if stop:\n",
    "        break\n",
    "    bench.net.eta = eta\n",
    "    print(f\"{eta:.3f}\", end=\" \")\n",
    "print(f\"\\n{n_cum} total lessons\")\n",
    "losses = np.concatenate(losses)\n",
    "plot_adaptive_learning(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
