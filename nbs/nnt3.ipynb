{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "#%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Importing_Notebooks\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of components which:\n",
    "1. accept an ordered set of reals (we'll use `numpy.array`, and  call them vectors) at the input port and produce another at the output port - this is forward propagation. ${\\displaystyle f\\colon \\mathbf {R} ^{n}\\to \\mathbf {R} ^{m}}$\n",
    "1. accept an ordered set of reals at the output port, representing the gradient of the loss function at the output, and produce the gradient of the loss function at the input port - this is back propagation, aka backprop. ${\\displaystyle b\\colon \\mathbf {R} ^{m}\\to \\mathbf {R} ^{n}}$\n",
    "1. from the gradient of the loss function at the output, calculate the partial of the loss function w.r.t the internal parameters ${\\displaystyle \\frac{\\partial E}{\\partial w} }$\n",
    "1. accept a scalar $\\eta$ to control the adjustment of internal parameters. _Or is this effected by scaling the loss gradient before passing??_\n",
    "1. update internal parameters ${\\displaystyle w \\leftarrow w - \\eta \\frac{\\partial E}{\\partial w} }$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Computes response to input\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backprop(self, output_delE):\n",
    "        \"\"\"Uses output error gradient to adjust internal parameters, and returns gradient of error at input\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of a cascade of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.eta = 0.1 #FIXME\n",
    "        \n",
    "    def extend(self, net):\n",
    "        self.layers.append(net)\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        v = input\n",
    "        for net in self.layers:\n",
    "            v = net(v)\n",
    "        return v\n",
    "    \n",
    "    def learn(self, facts):\n",
    "        for (x, expected) in facts:\n",
    "            y = self(x)\n",
    "            e = y - expected\n",
    "            loss = e.dot(e)/2.0\n",
    "            egrad = e * self.eta\n",
    "            for net in reversed(self.layers):\n",
    "                egrad = net.backprop(egrad)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Neural Net lab bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nnbench import NNBench\n",
    "from matplotlib.widgets import Slider, Button, RadioButtons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBench:\n",
    "    def __init__(self, net, ideal=lambda x:x):\n",
    "        self.net = net\n",
    "        self.ideal = ideal\n",
    "        self.gc_protect = []\n",
    "        self.seed = 3\n",
    "    \n",
    "    def checkpoint_net(self):\n",
    "        self.net_checkpoint = dill.dumps(self.net)\n",
    "        \n",
    "    def rollback_net(self):\n",
    "        self.net = dill.loads(self.net_checkpoint)\n",
    "        \n",
    "    def training_data_gen(self, n):\n",
    "        \"\"\"Generate n instances of labelled training data\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        for i in range(n):\n",
    "            v = np.random.randn(2)\n",
    "            yield (v, self.ideal(v))\n",
    "            \n",
    "    def learn(self, n=100):\n",
    "        return [self.net.learn([fact]) for fact in self.training_data_gen(n)]\n",
    "            \n",
    "    def learning_potential(self, n=100, eta=None):\n",
    "        stash = dill.dumps(self.net)\n",
    "        if eta is not None: # only change the net's eta if a value was passed to us\n",
    "            self.net.eta = eta\n",
    "        loss = self.net.learn(fact for fact in self.training_data_gen(n))\n",
    "        self.net = dill.loads(stash)\n",
    "        return -np.log(loss)\n",
    "        \n",
    "    def plot_learning(self, n):\n",
    "        from matplotlib import pyplot as plt\n",
    "        # self.losses = losses = [self.net.learn(fact for fact in self.training_data_gen(n))]\n",
    "        losses = self.learn(n)\n",
    "        plt.yscale('log')\n",
    "        plt.plot(range(len(losses)),losses)\n",
    "        plt.show(block=0)\n",
    "        \n",
    "    def knobs_plot_learning(self, n):\n",
    "        pickled_net = dill.dumps(self.net)\n",
    "        # from matplotlib import pyplot as plt\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.subplots_adjust(left=0.25, bottom=0.25)\n",
    "        a0 = 5\n",
    "        f0 = 3\n",
    "        \n",
    "        ###\n",
    "        losses = [self.net.learn([fact]) for fact in self.training_data_gen(n)]\n",
    "        l, = plt.plot(range(len(losses)), losses, lw=2)\n",
    "        ax.margins(x=0)\n",
    "        plt.yscale('log')\n",
    "\n",
    "        axcolor = 'lightgoldenrodyellow'\n",
    "        axfreq = plt.axes([0.25, 0.1, 0.65, 0.03], facecolor=axcolor)\n",
    "        axamp = plt.axes([0.25, 0.15, 0.65, 0.03], facecolor=axcolor)\n",
    "\n",
    "        sfreq = Slider(axfreq, '⍺', 0, 1, valinit=self.net.eta)\n",
    "        samp = Slider(axamp, 'Num', 1, 1000, valinit=100, valstep=1)\n",
    "        \n",
    "        filtfunc = [lambda x:x]\n",
    "        \n",
    "        \n",
    "        big = max(losses)\n",
    "        ax.set_title(f\"maxloss:{big}\")\n",
    "    \n",
    "        iax = plt.axes([0.025, 0.7, 0.15, 0.15])\n",
    "        def make_iax_image():\n",
    "            return np.concatenate([np.concatenate((l.M,np.array([l.b])),axis=0)\n",
    "                                   for l in self.net.layers\n",
    "                                  if hasattr(l, 'M')],axis=1)\n",
    "        def update_iax(img=[iax.imshow(make_iax_image())]):\n",
    "            img[0].remove()\n",
    "            img[0] = iax.imshow(make_iax_image())\n",
    "\n",
    "        def update(val,ax=ax,loc=[l]):\n",
    "            n = int(samp.val)\n",
    "            self.net = dill.loads(pickled_net)\n",
    "            sfunc = lambda x: 2**(-1.005/(x+.005))\n",
    "            self.net.eta = sfunc(sfreq.val)\n",
    "            #sfreq.set_label(\"2.4e\"%(self.net.eta,))\n",
    "            losses = filtfunc[0]([self.net.learn([fact]) for fact in self.training_data_gen(n)])\n",
    "            big = max(losses)\n",
    "            ax.set_title(f\"⍺={self.net.eta:1.3e},max loss:{big}\")\n",
    "            loc[0].remove()\n",
    "            loc[0], = ax.plot(range(len(losses)), losses, lw=2,color='xkcd:blue')\n",
    "            ax.set_xlim((0,len(losses)))\n",
    "            ax.set_ylim((min(losses),big))\n",
    "            update_iax()\n",
    "            fig.canvas.draw_idle()\n",
    "\n",
    "        sfreq.on_changed(update)\n",
    "        samp.on_changed(update)\n",
    "\n",
    "        resetax = plt.axes([0.8, 0.025, 0.1, 0.04])\n",
    "        button = Button(resetax, 'Reset', color=axcolor, hovercolor='0.975')\n",
    "\n",
    "    \n",
    "        def reset(event):\n",
    "            self.seed += 1\n",
    "            update()\n",
    "        button.on_clicked(reset)\n",
    "\n",
    "        rax = plt.axes([0.025, 0.5, 0.15, 0.15], facecolor=axcolor)\n",
    "        radio = RadioButtons(rax, ('raw', 'low pass', 'green'), active=0)\n",
    "\n",
    "        \n",
    "        def colorfunc(label):\n",
    "            if label == \"raw\":\n",
    "                filtfunc[0] = lambda x:x\n",
    "            elif label == \"low pass\":\n",
    "                filtfunc[0] = lambda x:ndimage.gaussian_filter(np.array(x),3)\n",
    "            #l.set_color(label)\n",
    "            #fig.canvas.draw_idle()\n",
    "            update()\n",
    "        radio.on_clicked(colorfunc)\n",
    "\n",
    "        plt.show()\n",
    "        #return 'gc protect:', update, reset, colorfunc,sfreq,samp, radio, button\n",
    "        self.gc_protect.append((update, reset, colorfunc,sfreq,samp, radio, button))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(Layer):\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        return output_delE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine\n",
    "A layer that does an [affine transformation](https://mathworld.wolfram.com/AffineTransformation.html) aka affinity, which is the classic fully-connected layer with output offsets.\n",
    "\n",
    "$$ \\mathbf{M} \\mathbf{x} + \\mathbf{b} = \\mathbf{y} $$\n",
    "where\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{j=1}^{n} x_j \\mathbf{\\hat{x}}_j \\\\\n",
    "\\mathbf{b} = \\sum_{i=1}^{m} b_i \\mathbf{\\hat{y}}_i \\\\\n",
    "\\mathbf{y} = \\sum_{i=1}^{m} y_i \\mathbf{\\hat{y}}_i\n",
    "$$\n",
    "and $\\mathbf{M}$ can be written\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    m_{1,1} & \\dots & m_{1,n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    m_{m,1} & \\dots & m_{m,n}\n",
    "\\end{bmatrix} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error gradient back-propagation\n",
    "$$ \n",
    "\\begin{align}\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{x}}\n",
    "  = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}\n",
    "  = \\mathbf{M}\\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "\\end{align}\n",
    "$$\n",
    "_SOLVE: Left-multiply or right-multiply?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter adjustment\n",
    "$$\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\mathbf{x} \\\\\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffinityLayer(Layer):\n",
    "    \"\"\"An affine transformation, which is the classic fully-connected layer with offsets\"\"\"\n",
    "    def __init__(self, n, m):\n",
    "        self.M = np.empty((m, n))\n",
    "        self.b = np.empty(m)\n",
    "        self.randomize()\n",
    "        \n",
    "    def randomize(self):\n",
    "        self.M[:] = np.random.randn(*self.M.shape)\n",
    "        self.b[:] = np.random.randn(*self.b.shape)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        self.output = self.M @ x + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = self.M @ output_delE\n",
    "        self.M -= np.einsum('i,j', output_delE, self.input) # use np.outer?\n",
    "        self.b -= output_delE\n",
    "        return input_delE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "Maps a scalar function on the inputs, for e.g. activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapLayer(Layer):\n",
    "    \"\"\"Map a scalar function on the input taken element-wise\"\"\"\n",
    "    def __init__(self, fun, dfundx):\n",
    "        self.vfun = np.vectorize(fun)\n",
    "        self.vdfundx = np.vectorize(dfundx)\n",
    "\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        return self.vfun(x)\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = self.vdfundx(self.input) * output_delE\n",
    "        return input_delE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One identity layer\n",
    "See if the wheels turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network()\n",
    "net.extend(IdentityLayer())\n",
    "all(net(np.arange(3)) == np.arange(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not learn, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facts = [(np.arange(2*n, 2*n+2), np.arange(2*n+1, 2*n-1, -1)) for n in range(3)]\n",
    "net.learn(facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(np.arange(2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One map layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network()\n",
    "net.extend(MapLayer(lambda x: x+1, lambda d: 1))\n",
    "all(net(np.arange(3)) == np.arange(3)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not learn, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, True, array([3, 4]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.learn(facts), all(net(np.arange(5)) == np.arange(5)+1), net(np.arange(2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One affine layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.extend(AffinityLayer(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.07555974,  0.50894321],\n",
       "        [-0.79693707,  0.52115632]]),\n",
       " array([-0.30574839,  0.99669659]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = net.layers[0]\n",
    "t.M, t.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can it learn the identity transformation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.852776766237184"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench = NNBench(net)\n",
    "bench.checkpoint_net()\n",
    "bench.learning_potential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fecdb1d84ea4df89436baeddf766b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bench.plot_learning(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79edbc96989a4159b1d66e3d1e9220aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bench.ideal = lambda v: np.array([v[1], v[0]])\n",
    "bench.knobs_plot_learning(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn thru a map layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer squares its input and divides by two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.extend(AffinityLayer(2,2))\n",
    "\n",
    "def dtanh(x):\n",
    "    v = np.tanh(x)\n",
    "    return (1+v)*(1-v)\n",
    "\n",
    "net.extend(MapLayer(lambda x:x*x/2.0, lambda d:d))\n",
    "#net.extend(MapLayer(np.tanh, dtanh))\n",
    "bench = NNBench(net)\n",
    "bench.checkpoint_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.32158469,  0.15113037],\n",
       "        [-0.01862772,  0.48352879]]),\n",
       " array([0.76896516, 1.36624284]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[0].M, net.layers[0].b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can it learn difference squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in multiply\n",
      "  del sys.path[0]\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in subtract\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench.ideal = lambda v: [(v[0]-v[1])**2,0]\n",
    "#bench.ideal = lambda v: [(v[0]>0)*2-1,(v[0]>v[1])*2-1]\n",
    "bench.learning_potential()\n",
    "#bench.knobs_plot_learning(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e62ff0649a4a52a7419b780426e1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: overflow encountered in multiply\n",
      "  del sys.path[0]\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in subtract\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    }
   ],
   "source": [
    "bench.knobs_plot_learning(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add a RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.AffinityLayer at 0x7fbec396afd0>,\n",
       " <__main__.MapLayer at 0x7fbec3971110>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench.net.layers = []\n",
    "bench.net.extend(AffinityLayer(2,2))\n",
    "leak = 0\n",
    "bench.net.extend(MapLayer(lambda x: (x*(1+leak/2)+abs(x)*(1-leak/2))/2, lambda d: [leak,1][1 if d>0 else 0]))\n",
    "bench.net.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.extend(AffinityLayer(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.14726479, -0.11022916],\n",
       "        [ 0.38825041, -0.38712718]]),\n",
       " array([-0.58722031,  1.91082685]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = net.layers[0]\n",
    "t.M, t.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
