{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets t1\n",
    "In which I attempt some structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "#%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Importing_Notebooks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of components which:\n",
    "1. accept an ordered set of reals (we'll use `numpy.array`, and  call them vectors) at the input port and produce another at the output port - this is forward propagation. ${\\displaystyle f\\colon \\mathbf {R} ^{n}\\to \\mathbf {R} ^{m}}$\n",
    "1. accept an ordered set of reals at the output port, representing the gradient of the loss function at the output, and produce the gradient of the loss function at the input port - this is back propagation, aka backprop. ${\\displaystyle b\\colon \\mathbf {R} ^{m}\\to \\mathbf {R} ^{n}}$\n",
    "1. from the gradient of the loss function at the output, calculate the partial of the loss function w.r.t the internal parameters ${\\displaystyle \\frac{\\partial E}{\\partial w} }$\n",
    "1. accept a scalar $\\alpha$ to control the adjustment of internal parameters. _Or is this effected by scaling the loss gradient before passing??_\n",
    "1. update internal parameters ${\\displaystyle w \\leftarrow w - \\alpha \\frac{\\partial E}{\\partial w} }$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Computes response to input\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backprop(self, output_delE):\n",
    "        \"\"\"Uses output error gradient to adjust internal parameters, and returns gradient of error at input\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of a cascade of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.alpha = 0.1 #FIXME\n",
    "        \n",
    "    def extend(self, net):\n",
    "        self.layers.append(net)\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        v = input\n",
    "        for net in self.layers:\n",
    "            v = net(v)\n",
    "        return v\n",
    "    \n",
    "    def learn(self, facts):\n",
    "        for (x, expected) in facts:\n",
    "            y = self(x)\n",
    "            e = y - expected\n",
    "            loss = e.dot(e)/2.0\n",
    "            agrad = e * self.alpha\n",
    "            for net in reversed(self.layers):\n",
    "                agrad = net.backprop(agrad)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(Layer):\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        return output_delE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine\n",
    "A layer that does an [affine transformation](https://mathworld.wolfram.com/AffineTransformation.html) aka affinity, which is the classic fully-connected layer with output offsets.\n",
    "\n",
    "$$ \\mathbf{M} \\mathbf{x} + \\mathbf{b} = \\mathbf{y} $$\n",
    "where\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{j=1}^{n} x_j \\mathbf{\\hat{x}}_j \\\\\n",
    "\\mathbf{b} = \\sum_{i=1}^{m} b_i \\mathbf{\\hat{y}}_i \\\\\n",
    "\\mathbf{y} = \\sum_{i=1}^{m} y_i \\mathbf{\\hat{y}}_i\n",
    "$$\n",
    "and $\\mathbf{M}$ can be written\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    m_{1,1} & \\dots & m_{1,n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    m_{m,1} & \\dots & m_{m,n}\n",
    "\\end{bmatrix} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error gradient back-propagation\n",
    "$$ \n",
    "\\begin{align}\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{x}}\n",
    "  = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}\n",
    "  = \\mathbf{M}\\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "\\end{align}\n",
    "$$\n",
    "_SOLVE: Left-multiply or right-multiply?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter adjustment\n",
    "$$\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\mathbf{x} \\\\\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffinityLayer(Layer):\n",
    "    \"\"\"An affine transformation, which is the classic fully-connected layer with offsets\"\"\"\n",
    "    def __init__(self, n, m):\n",
    "        self.M = np.empty((m, n))\n",
    "        self.b = np.empty(m)\n",
    "        self.randomize()\n",
    "        \n",
    "    def randomize(self):\n",
    "        self.M[:] = np.random.randn(*self.M.shape)\n",
    "        self.b[:] = np.random.randn(*self.b.shape)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        self.output = self.M @ x + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = self.M @ output_delE\n",
    "        self.M -= np.einsum('i,j', output_delE, self.input) # use np.outer?\n",
    "        self.b -= output_delE\n",
    "        return input_delE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "Maps a scalar function on the inputs, for e.g. activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapLayer(Layer):\n",
    "    \"\"\"Map a scalar function on the input taken element-wise\"\"\"\n",
    "    def __init__(self, fun, dfundx):\n",
    "        self.vfun = np.vectorize(fun)\n",
    "        self.vdfundx = np.vectorize(dfundx)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.vfun(x)\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = self.vdfundx(output_delE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One identity layer\n",
    "See if the wheels turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.extend(IdentityLayer())\n",
    "all(net(np.arange(3)) == np.arange(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not learn, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = [(np.arange(2*n, 2*n+2), np.arange(2*n+1, 2*n-1, -1)) for n in range(3)]\n",
    "net.learn(facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(np.arange(2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One map layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.extend(MapLayer(lambda x: x+1, lambda d: 1))\n",
    "all(net(np.arange(3)) == np.arange(3)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not learn, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.learn(facts), all(net(np.arange(5)) == np.arange(5)+1), net(np.arange(2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One affine layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.extend(AffinityLayer(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = net.layers[0]\n",
    "t.M, t.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can it learn the identity transformation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nnbench import NNBench\n",
    "from matplotlib.widgets import Slider, Button, RadioButtons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBench:\n",
    "    def __init__(self, net, ideal=lambda x:x):\n",
    "        self.net = net\n",
    "        self.ideal = ideal\n",
    "    \n",
    "    def checkpoint_net(self):\n",
    "        self.net_checkpoint = pickle.dumps(self.net)\n",
    "        \n",
    "    def rollback_net(self):\n",
    "        self.net = pickle.loads(self.net_checkpoint)\n",
    "        \n",
    "    def training_data_gen(self, n):\n",
    "        \"\"\"Generate n instances of labelled training data\"\"\"\n",
    "        for i in range(n):\n",
    "            v = np.random.randn(2)\n",
    "            yield (v,self.ideal(v))\n",
    "        \n",
    "    def plot_learning(self, n):\n",
    "        from matplotlib import pyplot as plt\n",
    "        # self.losses = losses = [self.net.learn(fact for fact in self.training_data_gen(n))]\n",
    "        self.losses = losses = [self.net.learn([fact]) for fact in self.training_data_gen(n)]\n",
    "        plt.yscale('log')\n",
    "        plt.plot(range(len(losses)),losses)\n",
    "        plt.show(block=0)\n",
    "        \n",
    "    def knobs_plot_learning(self, n):\n",
    "        # from matplotlib import pyplot as plt\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.subplots_adjust(left=0.25, bottom=0.25)\n",
    "        #t = np.arange(0.0, 1.0, 0.001)\n",
    "        a0 = 5\n",
    "        f0 = 3\n",
    "        #delta_f = 5.0\n",
    "        #s = a0 * np.sin(2 * np.pi * f0 * t)\n",
    "        \n",
    "        losses = [self.net.learn([fact]) for fact in self.training_data_gen(n)]\n",
    "        l, = plt.plot(range(len(losses)), losses, lw=2)\n",
    "        ax.margins(x=0)\n",
    "        plt.yscale('log')\n",
    "\n",
    "        axcolor = 'lightgoldenrodyellow'\n",
    "        axfreq = plt.axes([0.25, 0.1, 0.65, 0.03], facecolor=axcolor)\n",
    "        axamp = plt.axes([0.25, 0.15, 0.65, 0.03], facecolor=axcolor)\n",
    "\n",
    "        sfreq = Slider(axfreq, '⍺', 0, 1, valinit=self.net.alpha)\n",
    "        samp = Slider(axamp, 'Num', 1, 1000, valinit=100, valstep=1)\n",
    "        \n",
    "        big = max(losses)\n",
    "        ax.set_title(f\"maxloss:{big}\")\n",
    "    \n",
    "        iax = plt.axes([0.025, 0.7, 0.15, 0.15])\n",
    "        def make_iax_image():\n",
    "            return np.concatenate([np.concatenate((l.M,np.array([l.b])),axis=0) for l in self.net.layers],axis=1)\n",
    "        def update_iax(img=[iax.imshow(make_iax_image())]):\n",
    "            img[0].remove()\n",
    "            img[0] = iax.imshow(make_iax_image())\n",
    "\n",
    "        \n",
    "        def update(val,ax=ax,loc=[l]):\n",
    "            n = int(samp.val)\n",
    "            self.rollback_net()\n",
    "            self.net.alpha = sfreq.val\n",
    "            losses = [self.net.learn([fact]) for fact in self.training_data_gen(n)]\n",
    "            big = max(losses)\n",
    "            ax.set_title(f\"max loss:{big}\")\n",
    "            loc[0].remove()\n",
    "            loc[0], = ax.plot(range(len(losses)), losses, lw=2,color='xkcd:blue')\n",
    "            ax.set_xlim((0,len(losses)))\n",
    "            ax.set_ylim((min(losses),big))\n",
    "            update_iax()\n",
    "            fig.canvas.draw_idle()\n",
    "\n",
    "        sfreq.on_changed(update)\n",
    "        samp.on_changed(update)\n",
    "\n",
    "        resetax = plt.axes([0.8, 0.025, 0.1, 0.04])\n",
    "        button = Button(resetax, 'Reset', color=axcolor, hovercolor='0.975')\n",
    "\n",
    "\n",
    "        def reset(event):\n",
    "            sfreq.reset()\n",
    "            samp.reset()\n",
    "        button.on_clicked(reset)\n",
    "\n",
    "        rax = plt.axes([0.025, 0.5, 0.15, 0.15], facecolor=axcolor)\n",
    "        radio = RadioButtons(rax, ('red', 'blue', 'green'), active=0)\n",
    "\n",
    "\n",
    "        def colorfunc(label):\n",
    "            #l.set_color(label)\n",
    "            #fig.canvas.draw_idle()\n",
    "            pass\n",
    "        radio.on_clicked(colorfunc)\n",
    "\n",
    "        plt.show()\n",
    "        return 'gc protect:', update, reset, colorfunc,sfreq,samp, radio, button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = NNBench(net)\n",
    "bench.checkpoint_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.plot_learning(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.ideal = lambda v: np.array([v[1], v[0]])\n",
    "bench.knobs_plot_learning(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
