{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets t1\n",
    "In which I attempt some structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "#%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of components which:\n",
    "1. accept an ordered set of reals (we'll use `numpy.array`, and  call them vectors) at the input port and produce another at the output port - this is forward propagation. ${\\displaystyle f\\colon \\mathbf {R} ^{n}\\to \\mathbf {R} ^{m}}$\n",
    "1. accept an ordered set of reals at the output port, representing the gradient of the loss function at the output, and produce the gradient of the loss function at the input port - this is back propagation, aka backprop. ${\\displaystyle b\\colon \\mathbf {R} ^{m}\\to \\mathbf {R} ^{n}}$\n",
    "1. from the gradient of the loss function at the output, calculate the partial of the loss function w.r.t the internal parameters ${\\displaystyle \\frac{\\partial E}{\\partial w} }$\n",
    "1. accept a scalar $\\alpha$ to control the adjustment of internal parameters. _Or is this effected by scaling the loss gradient before passing??_\n",
    "1. update internal parameters ${\\displaystyle w \\leftarrow w - \\alpha \\frac{\\partial E}{\\partial w} }$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Computes response to input\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backprop(self, output_delE):\n",
    "        \"\"\"Uses output error gradient to adjust internal parameters, and returns gradient of error at input\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of a cascade of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.alpha = 0.1 #FIXME\n",
    "        \n",
    "    def extend(self, net):\n",
    "        self.layers.append(net)\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        v = input\n",
    "        for net in self.layers:\n",
    "            v = net(v)\n",
    "        return v\n",
    "    \n",
    "    def learn(self, facts):\n",
    "        for x, expected in facts:\n",
    "            y = self(x)\n",
    "            e = y - expected\n",
    "            loss = e.dot(e)/2.0\n",
    "            agrad = e * self.alpha\n",
    "            for net in reversed(self.layers):\n",
    "                agrad = net.backprop(agrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Subnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(Layer):\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        return output_delE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer that does an [affine transformation](https://mathworld.wolfram.com/AffineTransformation.html) aka affinity, which is the classic fully-connected layer with output offsets.\n",
    "\n",
    "$$ \\mathbf{M} \\mathbf{x} + \\mathbf{b} = \\mathbf{y} $$\n",
    "where\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{j=1}^{n} x_j \\mathbf{\\hat{x}}_j \\\\\n",
    "\\mathbf{b} = \\sum_{i=1}^{m} b_i \\mathbf{\\hat{y}}_i \\\\\n",
    "\\mathbf{y} = \\sum_{i=1}^{m} y_i \\mathbf{\\hat{y}}_i\n",
    "$$\n",
    "and $\\mathbf{M}$ can be written\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    m_{1,1} & \\dots & m_{1,n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    m_{m,1} & \\dots & m_{m,n}\n",
    "\\end{bmatrix} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error gradient back-propagation\n",
    "$$ \n",
    "\\begin{align}\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{x}}\n",
    "  = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}\n",
    "  = \\mathbf{M}\\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "\\end{align}\n",
    "$$\n",
    "_SOLVE: Left-multiply or right-multiply?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter adjustment\n",
    "$$\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\mathbf{x} \\\\\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affinity(Layer):\n",
    "    \"\"\"An affine transformation, which is the classic fully-connected layer with offsets\"\"\"\n",
    "    def __init__(self, n, m):\n",
    "        self.M = np.random.randn(m, n)\n",
    "        self.b = np.random.randn(m)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        self.output = self.M @ x + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = self.M @ output_delE\n",
    "        \n",
    "        return input_delE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
