{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intimate-appeal",
   "metadata": {},
   "source": [
    "# Neural Net observatory\n",
    "We run the net training in a child process, so that it can proceed while we observe and analyze partial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "applicable-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib inline\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-stereo",
   "metadata": {},
   "source": [
    "FIXME: clean up imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hybrid-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.max_open_warning'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scenic-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import threading\n",
    "import time\n",
    "import trio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preceding-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "import rx\n",
    "from rx import Observable\n",
    "from rx.subject import Subject\n",
    "from rx import operators as op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-meeting",
   "metadata": {},
   "source": [
    "Fetch our tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acting-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import Network, Layer, IdentityLayer, AffineLayer, MapLayer\n",
    "#from nnbench import NNBench\n",
    "#from nnvis import NNVis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-albuquerque",
   "metadata": {},
   "source": [
    "Use [`ipywidgets`](https://ipywidgets.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-august",
   "metadata": {},
   "source": [
    "# A global (within the parent) state object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "refined-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thing:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "g = Thing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-fields",
   "metadata": {},
   "source": [
    "# Tooling\n",
    " * `JSONConn` over the Process Pipe\n",
    " -- Not seeing the exception on `recv()` of a closed connection, so we accomplish a close by a non-JSON message of four bytes of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worldwide-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONConn():\n",
    "    def __init__(self, conn):\n",
    "        self.conn = conn\n",
    "        \n",
    "    def send(self, v):\n",
    "        self.conn.send_bytes(json.dumps(v).encode('utf8'))\n",
    "        \n",
    "    def poll(self):\n",
    "        return self.conn.poll()\n",
    "    \n",
    "    def recv(self):\n",
    "        r = self.conn.recv_bytes()\n",
    "        if r == bytes(4):\n",
    "            self.close()\n",
    "            raise EOFError\n",
    "        return json.loads(r)\n",
    "        \n",
    "    def close(self):\n",
    "        self.conn.send_bytes(bytes(4))\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "minute-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileSystemConn():\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "        self.outf = open(fname, 'wb')\n",
    "        \n",
    "    def send(self, v):\n",
    "        self.outf.write(json.dumps(v).encode('utf8'))\n",
    "        self.outf.write('\\n'.encode('utf8'))\n",
    "        \n",
    "    def poll(self):\n",
    "        raise UnimplementedError\n",
    "    \n",
    "    def recv(self):\n",
    "        raise UnimplementedError\n",
    "        \n",
    "    def close(self):\n",
    "        self.outf.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-guyana",
   "metadata": {},
   "source": [
    "# The Machine Learning compute process\n",
    "* Contain the ML model\n",
    "* Run in a separate O/S process for isolation\n",
    "* Communicate over `multiprocessing.Pipe` via messages each of which is a JSON-encoded dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "private-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(conn, fname):\n",
    "    jc = JSONConn(conn)\n",
    "    fc = FileSystemConn(fname)\n",
    " \n",
    "    net = Network()\n",
    "    net.extend(AffineLayer(2,2))\n",
    "    net.extend(MapLayer(np.tanh, lambda d: 1.0 - np.tanh(d)**2))\n",
    "    net.extend(AffineLayer(2,1))\n",
    "    net.extend(MapLayer(np.tanh, lambda d: 1.0 - np.tanh(d)**2))\n",
    "\n",
    "    training_batch = (np.array([[-0.5, -0.5],\n",
    "                                [-0.5,  0.5],\n",
    "                                [ 0.5,  0.5],\n",
    "                                [ 0.5, -0.5]]),\n",
    "                      np.array([[-0.5],\n",
    "                                [ 0.5],\n",
    "                                [-0.5],\n",
    "                                [ 0.5]]))\n",
    "\n",
    "    # Initialize a lot of variables that control the state machine\n",
    "    batch_ctr = 0\n",
    "    batch_to = 0\n",
    "    report_state = True\n",
    "    report_state_interval = 0\n",
    "    last_state_report_was_at_batch = 0\n",
    "    last_reported_loss = None\n",
    "    loss = -1\n",
    "    last_loss_report_time = 0\n",
    "    loss_report_min_interval = 0.05\n",
    "    loss_report_max_interval = 1\n",
    "    done = False\n",
    "\n",
    "    #for i in range(100):\n",
    "    #    if done:\n",
    "    #        break\n",
    "    while not done:\n",
    "        txm = dict()\n",
    "        \n",
    "        # Check for new instructions\n",
    "        while jc.poll():\n",
    "            rxm = jc.recv()\n",
    "            print(f\"compute process got {rxm}\")\n",
    "            for k,v in rxm.items():\n",
    "                if k == 'eta':\n",
    "                    net.eta = v\n",
    "                elif k == 'batch to':\n",
    "                    batch_to = v\n",
    "                elif k == 'tell state':\n",
    "                    report_state = True\n",
    "                elif k == 'report state every':\n",
    "                    report_state_interval = v\n",
    "                elif k == 'shutdown':\n",
    "                    print(f\"compute process got shutdown at batch {batch_ctr}\")\n",
    "                    done = True\n",
    "        \n",
    "        # Report states if it's the right batch phase, or if asked to\n",
    "        report_state = report_state or \\\n",
    "                        report_state_interval > 0 \\\n",
    "                        and batch_ctr % report_state_interval == 0 \\\n",
    "                        and last_state_report_was_at_batch < batch_ctr\n",
    "\n",
    "        if report_state:\n",
    "            txm['eta'] = [batch_ctr, net.eta]\n",
    "            txm['sv'] = [batch_ctr, list(float(v) for v in net.state_vector())]\n",
    "            last_state_report_was_at_batch = batch_ctr\n",
    "            report_state = False\n",
    "            \n",
    "        # Run a learning step if we aren't at the target number of steps\n",
    "        if batch_to > batch_ctr:\n",
    "            loss = net.learn([training_batch])\n",
    "            batch_ctr += 1\n",
    "            # Report the loss when we reach the target number of batches\n",
    "            if batch_ctr == batch_to:\n",
    "                txm['loss'] = [batch_ctr, loss]\n",
    "            #time.sleep(0.2) # Pretend this was a time-consuming calculation\n",
    "            #time.sleep(0.01) #DEBUG: rate limit\n",
    "            \n",
    "        # Report the loss, with rate limiting, if it's changed since last report\n",
    "        if loss != last_reported_loss \\\n",
    "                and time.time() - last_loss_report_time > loss_report_min_interval:\n",
    "            txm['loss'] = [batch_ctr, loss]\n",
    "            last_reported_loss = loss\n",
    "            last_loss_report_time = time.time()\n",
    "            \n",
    "        # Report the loss, when it's been too long, and we're still working on it\n",
    "        # even if the loss is unchanged, as a sort of heartbeat\n",
    "        if time.time() - last_loss_report_time > loss_report_max_interval:\n",
    "            txm['loss'] = [batch_ctr, loss]\n",
    "            last_reported_loss = loss\n",
    "            last_loss_report_time = time.time()\n",
    "            \n",
    "        if txm:\n",
    "            jc.send(txm)\n",
    "            fc.send(txm)\n",
    "        elif batch_ctr >= batch_to:\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    jc.close()\n",
    "    fc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-speed",
   "metadata": {},
   "source": [
    "# The parent\n",
    "* Do work in a background thread, as coroutines in `trio`\n",
    "    * Burst and route received messages from ML compute process using ReactiveX\n",
    "    * Interact with the UI widgets\n",
    "* Leave the foreground free for the notebook and its widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-minimum",
   "metadata": {},
   "source": [
    "## Set up the compute process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "simplified-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below did not work with either of 'spawn' or 'forkserver'\n",
    "# It resulted in 'AttributeError: Can't get attribute 'f' on <module '__main__' (built-in)>'\n",
    "\"\"\"# Spawn the worker process (fork is risky)\n",
    "ctx = mp.get_context('forkserver') \n",
    "ipc_pipe = ctx.Pipe()\n",
    "parent_conn, child_conn = ipc_pipe\n",
    "jc = JSONConn(parent_conn)\n",
    "p = ctx.Process(target=f, args=(child_conn, 't1.jsons'))\n",
    "\"\"\"\n",
    "\n",
    "# Fork the compute process, but don't start it yet\n",
    "# Could be trouble for trio, viz. https://github.com/python-trio/trio/issues/1614\n",
    "# so we do this first, outside of any `trio.run`\n",
    "ipc_pipe = mp.Pipe()\n",
    "parent_conn, child_conn = ipc_pipe\n",
    "jc = JSONConn(parent_conn)\n",
    "p = mp.Process(target=f, args=(child_conn, 't1.jsons'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-colon",
   "metadata": {},
   "source": [
    "## Build UI widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "returning-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some visibility widgets\n",
    "batch_w = widgets.FloatText(value=-1.0, description='Batch:', max_width=6, disabled=True)\n",
    "loss_w = widgets.FloatText(value=-1.0, description='Loss:', max_width=6, disabled=True)\n",
    "\n",
    "# Set up control widgets\n",
    "# Button to stop the worker\n",
    "shutdown_b_w = widgets.Button(description=\"Shutdown worker\")\n",
    "\n",
    "# Input field to modify batch_to, and button to submit it\n",
    "batch_to_w = widgets.IntText(value=50, description='target batches:')\n",
    "batch_to_b_w = widgets.Button(description=\"submit target batches\")\n",
    "\n",
    "ui_widgets = (batch_w, loss_w, batch_to_w, batch_to_b_w, shutdown_b_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-brooks",
   "metadata": {},
   "source": [
    "## ... and widget behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "intelligent-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_w_value(w, val):\n",
    "    ov = w.value\n",
    "    w.value = val\n",
    "    return ov\n",
    "\n",
    "g.shut_down_child = False\n",
    "def shutdown_child(w):\n",
    "    #print('sending shutdown from shutdown_child')\n",
    "    jc.send({'shutdown': 'now'})\n",
    "    g.shut_down_child = True\n",
    "shutdown_b_w.on_click(shutdown_child)\n",
    "\n",
    "def submit_target_batches(w):\n",
    "    target_batches = batch_to_w.value\n",
    "    jc.send({'batch to': target_batches})\n",
    "batch_to_b_w.on_click(submit_target_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-johnson",
   "metadata": {},
   "source": [
    "## Build the ReactiveX pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dependent-adventure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rx.disposable.disposable.Disposable at 0x7f5d5979d520>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up pipeline to process worker messages into topic observables\n",
    "compute_worker_messages_s = rx.subject.Subject()\n",
    "burst_messages_s = compute_worker_messages_s.pipe(\n",
    "    op.flat_map(lambda m: m.items()))\n",
    "loss_s = burst_messages_s.pipe(\n",
    "    op.filter(lambda t: t[0] == 'loss'),\n",
    "    op.map(lambda t: t[1]))\n",
    "sv_s = burst_messages_s.pipe(\n",
    "    op.filter(lambda t: t[0] == 'sv'),\n",
    "    op.map(lambda t: t[1]))\n",
    "eta_s = burst_messages_s.pipe(\n",
    "    op.filter(lambda t: t[0] == 'eta'),\n",
    "    op.map(lambda t: t[1]))\n",
    "\n",
    "loss_s.subscribe(lambda t: set_w_value(batch_w, t[0]) + set_w_value(loss_w, t[1]))\n",
    "loss_s.pipe(op.take_last(1)).subscribe(print) # show the last loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-remove",
   "metadata": {},
   "source": [
    "# Background thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-engineering",
   "metadata": {},
   "source": [
    "## The background thread function is a `trio` task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "magnetic-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_work(g, jc, s):\n",
    "\n",
    "    async def receive_from_compute_process(g, jc, s):\n",
    "        while not g.stop_requested:\n",
    "            try:\n",
    "                if jc.poll():\n",
    "                    try:\n",
    "                        m = jc.recv()\n",
    "                        s.on_next(m)\n",
    "                    except EOFError:\n",
    "                        s.on_completed()\n",
    "                        print(\"sender closed\")\n",
    "                        g.done = True\n",
    "                    except BrokenPipeError:\n",
    "                        s.on_completed()\n",
    "                        print(\"broken pipe\")\n",
    "                        g.done = True\n",
    "                else:\n",
    "                    await trio.sleep(0.01)\n",
    "            except OSError as e:\n",
    "                print(e)\n",
    "                g.done = True\n",
    "                break\n",
    "\n",
    "    async def worker(g):\n",
    "        while not g.stop_requested:\n",
    "            await trio.sleep(await g.threadwork())\n",
    "\n",
    "    \n",
    "    async def threadloop(g, jc, s):\n",
    "        async with trio.open_nursery() as nursery:\n",
    "            nursery.start_soon(receive_from_compute_process, g, jc, s)\n",
    "            nursery.start_soon(worker, g)\n",
    "        \n",
    "    trio.run(threadloop, g, jc, s)\n",
    "    g.done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-suicide",
   "metadata": {},
   "source": [
    "## Set up an idle job and the thread\n",
    "But don't start the thread just yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "completed-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def just_one():\n",
    "    g.work_ctr += 1\n",
    "    return 1\n",
    "\n",
    "g.work_ctr = 0\n",
    "g.threadwork = just_one\n",
    "g.stop_requested = False\n",
    "g.thread = threading.Thread(target=thread_work, args=(g, jc, compute_worker_messages_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-intensity",
   "metadata": {},
   "source": [
    "# Start backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "biblical-parade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c41c374f4af441ca0fcd41705b2f6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=-1.0, description='Batch:', disabled=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa95b9a452304dc18514d61783664888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=-1.0, description='Loss:', disabled=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fda19868ae4befbece70c6a86641d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=50, description='target batches:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628880333a044e83bb3e4abcfddec740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='submit target batches', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c984a0bf734235a59c493e265477d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Shutdown worker', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute process got {'batch to': 50}\n",
      "compute process got {'batch to': 5000}\n",
      "compute process got {'batch to': 50000}\n"
     ]
    }
   ],
   "source": [
    "display(*ui_widgets)\n",
    "p.start()\n",
    "g.thread.start()\n",
    "#jc.send({'batch to': 50})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "surgical-recorder",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "You can use the UI now",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-de942743bade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You can use the UI now\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: You can use the UI now"
     ]
    }
   ],
   "source": [
    "assert False, \"You can use the UI now\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.stop_requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.thread.is_alive() or g.thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-ivory",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-joining",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"stop here if entering from above\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoawait trio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
