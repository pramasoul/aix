{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partials etc involving matricies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few ways to get test numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([4, 5, 6, 7]), array([5, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(3), np.arange(4,8), np.arange(5,1,-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experiments with multiplication, arrays of primes may be helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arangep(n, starting_index=0):\n",
    "    sympy.sieve.extend_to_no(starting_index + n)\n",
    "    return np.array(sympy.sieve._list[starting_index:starting_index + n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2,  3,  5,  7, 11]), array([ 5,  7, 11, 13]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arangep(5), arangep(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 3],\n",
       "        [5, 7]]),\n",
       " array([11, 13]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = arangep(4).reshape(2,2)\n",
    "x = arangep(2,4)\n",
    "# x = np.arange(2)+1\n",
    "M,x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einstein summation notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy provides [Einstein summation](https://mathworld.wolfram.com/EinsteinSummation.html) operations with [einsum](https://numpy.org/devdocs/reference/generated/numpy.einsum.html)\n",
    "1. Repeated indices are implicitly summed over.\n",
    "1. Each index can appear at most twice in any term.\n",
    "1. Each term must contain identical non-repeated indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = np.einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$a_{ik}a_{ij} \\equiv \\sum_{i} a_{ik}a_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 61, 146]), array([ 87, 124]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es('ij,j', M, x), es('ij,i', M, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix __M__ multiplies a (column) vector __x__ to its right to produce a (column) vector __y__:\n",
    "$$ \\normalsize \\mathbf{M} \\mathbf{x} = \\mathbf{y} $$\n",
    "where\n",
    "$$ \\normalsize\n",
    "\\mathbf{x} = \\sum_{j=1}^{n} x_j \\mathbf{\\hat{x}}_j \\\\\n",
    "\\mathbf{y} = \\sum_{i=1}^{m} y_i \\mathbf{\\hat{y}}_i\n",
    "$$\n",
    "and $\\mathbf{M}$ can be written\n",
    "$$ \\normalsize\n",
    "\\begin{bmatrix}\n",
    "    m_{1,1} & \\dots & m_{1,n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    m_{m,1} & \\dots & m_{m,n}\n",
    "\\end{bmatrix} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `python` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 61, 146])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = M @ x\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Einstein summation notation, $y_i = m_{ij}x_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 61, 146])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('ij,j', M, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial derivative of a matrix multiply of a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia [defines](https://en.wikipedia.org/wiki/Partial_derivative#Formal_definition) the partial derivative thus: \\\n",
    "Let _U_ be an open subset of $\\mathbb{R}^n$ and ${\\displaystyle f:U\\to \\mathbb {R} }$ a function. The partial derivative of _f_ at the point ${\\displaystyle \\mathbf {a} =(a_{1},\\ldots ,a_{n})\\in U}$ with respect to the _i_-th variable $x_i$ is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial x_i }f(\\mathbf{a}) & = \\lim_{h \\to 0} \\frac{f(a_1, \\ldots , a_{i-1}, a_i+h, a_{i+1}, \\ldots ,a_n) -\n",
    "f(a_1, \\ldots, a_i, \\dots ,a_n)}{h} \\\\ \n",
    "& = \\lim_{h \\to 0} \\frac{f(\\mathbf{a}+he_i) -\n",
    "f(\\mathbf{a})}{h} \\tag{2.1}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $f(\\mathbf{a})$ is linear, $f(\\mathbf{a}+he_i) = f(\\mathbf{a}) + f(he_i) = f(\\mathbf{a}) + h f(e_i)$, and we have\n",
    "$$ \\begin{align} \\\\\n",
    "\\frac{\\partial }{\\partial x_i }f(\\mathbf{a}) &= \\lim_{h \\to 0} \\frac{f(\\mathbf{a}+he_i) - f(\\mathbf{a})}{h} \\\\\n",
    " & = \\lim_{h \\to 0} \\frac{f(\\mathbf{a}) + h f(e_i) - f(\\mathbf{a})}{h} \\\\\n",
    " & = \\lim_{h \\to 0} \\frac{h f(e_i)}{h} \\\\\n",
    " & = \\lim_{h \\to 0} {f(e_i)} \\\\\n",
    " &= f(e_i) \\tag{2.2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\partial\\mathbf{y} / \\partial\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does vector $\\mathbf{y}$ vary with vector $\\mathbf{x}$, with $M$ held constant? I.e. what is $\\partial\\mathbf{y}/\\partial\\mathbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With\n",
    "$$ %\\normalsize\n",
    "\\mathbf{x} = \\sum_{j=1}^{n} x_j \\mathbf{\\hat{x}}_j, \\;\\;\n",
    "\\mathbf{y} = \\sum_{i=1}^{m} y_i \\mathbf{\\hat{y}}_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix equation $\\mathbf{y} = \\mathbf{M} \\mathbf{x}$ can be written as\n",
    "$$ \\normalsize\n",
    "\\begin{align}\n",
    "\\mathbf{y} &= \\sum_i y_i \\mathbf{\\hat{y}}_i \n",
    "  = \\mathbf{M}\\mathbf{x}  \\tag{2.3} \\label{mmul}\n",
    "\\end{align}\n",
    "$$\n",
    "where\n",
    "$$ \\normalsize\n",
    "\\begin{align}\n",
    "y_i &= f_i(x_1, x_2, \\dots x_n) \\\\[6pt]\n",
    "  &= \\sum_j m_{ij}x_j \\tag{2.4}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have\n",
    "$$ \\normalsize\n",
    "\\begin{align}\n",
    " \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}}\n",
    " &= \\frac{\\partial\\sum_{i=1}^{m} y_i \\mathbf{\\hat{y}}_i}{\\partial\\mathbf{x}} \\\\[10pt]\n",
    " &= \\frac{\\partial\\sum_{i=1}^{m} f_i(x_1, x_2, \\dots x_n) \\mathbf{\\hat{y}}_i}{\\partial\\mathbf{x}} \\\\[10pt]\n",
    " &= \\sum_{i=1}^{m} \\frac{\\sum_{j=1}^{n} \\partial(m_{ij}x_j) \\mathbf{\\hat{y}}_i}{{\\partial x_j} \\mathbf{\\hat{x}_j}} \\\\[10pt]\n",
    " &= \\sum_{i=1}^{m}\n",
    "     \\sum_{j=1}^{n} \n",
    "      \\frac{\\partial(m_{ij}x_j)}\n",
    "           {\\partial x_j} \n",
    "        \\frac{\\mathbf{\\hat{y}}_i}{\\mathbf{\\hat{x}_j}}  \\\\[10pt]\n",
    " &= \\sum_{i=1}^{m}\n",
    "     \\sum_{j=1}^{n} m_{ij}\n",
    "      \\frac{\\partial x_j}\n",
    "           {\\partial x_j} \n",
    "        \\frac{\\mathbf{\\hat{y}}_i}{\\mathbf{\\hat{x}_j}}  \\\\[10pt]\n",
    " &= \\sum_{i=1}^{m}\n",
    "     \\sum_{j=1}^{n} m_{ij}\n",
    "      \\frac{\\mathbf{\\hat{y}}_i}{\\mathbf{\\hat{x}_j}}  \\\\[10pt]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis vectors for $\\partial\\mathbf{y} / \\partial\\mathbf{x}$ are $\\mathbf{\\hat{y}}_i / \\mathbf{\\hat{x}_j}$. We can array the components in a matrix to say \\\n",
    "\\\n",
    "$$ \\normalsize\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} =\n",
    "%\\large\n",
    "\\begin{bmatrix}\n",
    "m_{1,1}\\frac{\\mathbf{\\hat{y}}_1}{\\mathbf{\\hat{x}_1}} & \\cdots &\n",
    "m_{1,n}\\frac{\\mathbf{\\hat{y}}_1}{\\mathbf{\\hat{x}_n}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "m_{m,1}\\frac{\\mathbf{\\hat{y}}_n}{\\mathbf{\\hat{x}_1}} & \\cdots &\n",
    "m_{m,n}\\frac{\\mathbf{\\hat{y}}_m}{\\mathbf{\\hat{x}_n}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then\n",
    "\\\n",
    "$$ \\normalsize\n",
    "\\partial \\mathbf{y} =\n",
    "%\\large\n",
    "\\begin{bmatrix}\n",
    "m_{1,1}\\frac{\\mathbf{\\hat{y}}_1}{\\mathbf{\\hat{x}_1}} & \\cdots &\n",
    "m_{1,n}\\frac{\\mathbf{\\hat{y}}_1}{\\mathbf{\\hat{x}_n}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "m_{m,1}\\frac{\\mathbf{\\hat{y}}_n}{\\mathbf{\\hat{x}_1}} & \\cdots &\n",
    "m_{m,n}\\frac{\\mathbf{\\hat{y}}_m}{\\mathbf{\\hat{x}_n}}\n",
    "\\end{bmatrix}\n",
    "\\partial \\mathbf{x}\n",
    "$$\n",
    "and\n",
    "$$ \\normalsize\n",
    "\\begin{align}\n",
    "\\partial \\mathbf{x} &=\n",
    "%\\large\n",
    "\\begin{bmatrix}\n",
    "m_{1,1}\\frac{\\mathbf{\\hat{y}}_1}{\\mathbf{\\hat{x}_1}} & \\cdots &\n",
    "m_{1,n}\\frac{\\mathbf{\\hat{y}}_1}{\\mathbf{\\hat{x}_n}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "m_{m,1}\\frac{\\mathbf{\\hat{y}}_n}{\\mathbf{\\hat{x}_1}} & \\cdots &\n",
    "m_{m,n}\\frac{\\mathbf{\\hat{y}}_m}{\\mathbf{\\hat{x}_n}}\n",
    "\\end{bmatrix}^\\mathsf{T}\n",
    "\\partial\\mathbf{y} \\\\[10pt]\n",
    "&=\n",
    "%\\large\n",
    "\\begin{bmatrix}\n",
    "m_{1,1}\\frac{\\mathbf{\\hat{x}}_1}{\\mathbf{\\hat{y}_1}} & \\cdots &\n",
    "m_{m,1}\\frac{\\mathbf{\\hat{x}}_1}{\\mathbf{\\hat{y}_m}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "m_{1,n}\\frac{\\mathbf{\\hat{x}}_n}{\\mathbf{\\hat{y}_1}} & \\cdots &\n",
    "m_{m,n}\\frac{\\mathbf{\\hat{x}}_n}{\\mathbf{\\hat{y}_m}}\n",
    "\\end{bmatrix}\n",
    "\\partial\\mathbf{y}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating ([2.1](#mjx-eqn-partial)) numerically with our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 3],\n",
       "        [5, 7]]),\n",
       " array([2., 5.]),\n",
       " array([3., 7.]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, (M@(x + np.array([0.001, 0])) - M@x) / 0.001, (M@(x + np.array([0, 0.001])) - M@x) / 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test (2.5) numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4982115870801231e-30"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(err.dot(err)\n",
    "    for err in (((M@(x + veps) - M@x) - M@veps)\n",
    "              for M,x,veps in ((np.random.randn(2,2), np.random.randn(2), np.random.randn(2) * 0.001)\n",
    "                          for i in range(1000))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\partial\\mathbf{y} / \\partial\\mathbf{M}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does vector $\\mathbf{y}$ vary with matrix $M$, with vector $\\mathbf{x}$ held constant? I.e. what is $\\partial\\mathbf{y}/\\partial\\mathbf{M}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From (2.3):\n",
    "$$\\begin{align}\n",
    " y_i &= \\sum_j m_{ij}x_j \\\\\n",
    " \\partial y_i &= \\sum_j \\partial m_{ij}x_j \\\\\n",
    "% \\frac{\\partial y_i}{\\partial M_{ij}} &= 2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then _[explain]_\n",
    "$$\n",
    " \\partial\\mathbf{y} = \\partial\\mathbf{M}\\mathbf{x} \\\\\n",
    " \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{M}} = \\mathbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 3],\n",
       "        [5, 7]]),\n",
       " array([11, 13]),\n",
       " array([ 61, 146]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, x, M@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0],\n",
       "        [0, 0]]),\n",
       " array([[0, 1],\n",
       "        [0, 0]]),\n",
       " array([[0, 0],\n",
       "        [1, 0]]),\n",
       " array([[0, 0],\n",
       "        [0, 1]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k11 = np.array([[1, 0], [0, 0]])\n",
    "k12 = np.fliplr(k11)\n",
    "k21 = np.flipud(k11)\n",
    "k22 = np.fliplr(k21)\n",
    "singles = (k11, k12, k21, k22)\n",
    "singles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([11.,  0.]), array([13.,  0.]), array([ 0., 11.]), array([ 0., 13.])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[((M+(e*0.001))@x - M@x) / 0.001 for e in singles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([11,  0]), array([13,  0]), array([ 0, 11]), array([ 0, 13])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e@x for e in singles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test numerically: Create random vector x and random M and dM matricies. Use an approximation of (2.1) to estimate\n",
    "$\\partial\\mathbf{y}/\\partial\\mathbf{M}$ numerically, and compare to $\\partial\\mathbf{M}\\mathbf{x}$. Find the maximum squared error in a number of random trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4641463207114085e-24"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(v.dot(v)\n",
    "    for v in (dM@x - (((M+(dM*0.001))@x - M@x) / 0.001)\n",
    "              for M,dM,x in ((np.random.randn(2,2), np.random.randn(2,2), np.random.randn(2))\n",
    "                          for i in range(1000))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Wikipedia](https://en.wikipedia.org/wiki/Gradient):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vector calculus, the **gradient** of a scalar-valued differentiable function $f$ of several variables is the vector field (or vector-valued function) $\\nabla f$ whose value at a point $p$ is the vector whose components are the partial derivatives of $f$ at $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, for $f \\colon \\mathbf{R}^n \\to \\mathbf{R}$, its gradient $\\nabla f \\colon \\mathbf{R}^n \\to \\mathbf{R}^n$ is defined at the point $p = (x_1,\\ldots,x_n)$ in *n-*dimensional space as the vector:\n",
    "\n",
    "$$\\nabla f(p) = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1}(p) \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n}(p) \\end{bmatrix}.$$\n",
    "\n",
    "Strictly speaking, the gradient is a vector field $f \\colon \\mathbf{R}^n \\to T\\mathbf{R}^n$, and the value of the gradient at a point is a tangent vector in the tangent space at that point, $T_p \\mathbf{R}^n$, not a vector in the original space $\\mathbf{R}^n$. However, all the tangent spaces can be naturally identified with the original space $\\mathbf{R}^n$, so these do not need to be distinguished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\displaystyle \\nabla f(p)\\cdot \\mathrm {v} = {\\tfrac {\\partial f}{\\partial \\mathbf {v} }}(p)=df_{\\mathrm {v} }(p)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computationally, given a tangent vector, the vector can be _multiplied_ by the derivative (as matrices), which is equal to taking the dot product with the gradient: \\\n",
    "${\\displaystyle (df_{p})(v)={\\begin{bmatrix}{\\frac {\\partial f}{\\partial x_{1}}}(p)\\cdots {\\frac {\\partial f}{\\partial x_{n}}}(p)\\end{bmatrix}}{\\begin{bmatrix}v_{1}\\\\\\vdots \\\\v_{n}\\end{bmatrix}}=\\sum _{i=1}^{n}{\\frac {\\partial f}{\\partial x_{i}}}(p)v_{i}={\\begin{bmatrix}{\\frac {\\partial f}{\\partial x_{1}}}(p)\\\\\\vdots \\\\{\\frac {\\partial f}{\\partial x_{n}}}(p)\\end{bmatrix}}\\cdot {\\begin{bmatrix}v_{1}\\\\\\vdots \\\\v_{n}\\end{bmatrix}}=\\nabla f(p)\\cdot v}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Euclidian 3-space,\n",
    "$$ \\nabla\\phi(x, y, z) =\n",
    "\\frac{\\partial\\phi}{\\partial x}\\mathbf{\\hat{x}} +\n",
    "\\frac{\\partial\\phi}{\\partial y}\\mathbf{\\hat{y}} +\n",
    "\\frac{\\partial\\phi}{\\partial z}\\mathbf{\\hat{z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    class VC():\n",
    "        def grad(f, x, eps=1e-6):\n",
    "            epsihat = np.eye(x.size) * eps\n",
    "            yp = np.apply_along_axis(f, 1, x + epsihat)\n",
    "            ym = np.apply_along_axis(f, 1, x - epsihat)\n",
    "            return (yp - ym)/(2 * eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of a constant scalar $f(x) = c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VC.grad(lambda x: 42, np.array([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of a scalar polynomial $x(1-x) = -x^2 + x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VC.grad(lambda x: x * (1-x), np.array([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of an element-wise multiply by a constant vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 2., 0., 0.],\n",
       "       [0., 0., 3., 0.],\n",
       "       [0., 0., 0., 4.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda v: np.multiply(v, np.arange(v.size) + 1)\n",
    "VC.grad(f, np.arange(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of a matrix multiply. Here's a non-square matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.random.rand(3)\n",
    "np.arange(v.size * (v.size+1)).reshape((v.size, v.size+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the matrix multiplication at a given point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]),\n",
       " array([20, 23, 26, 29]),\n",
       " array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda v: v @ np.arange(v.size * (v.size+1)).reshape((v.size, v.size+1))\n",
    "x = np.arange(3)\n",
    "y = f(x)\n",
    "g = VC.grad(f, x)\n",
    "x, y, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of an affine transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]),\n",
       " array([20, 24, 28, 32]),\n",
       " array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda v: v @ np.arange(v.size * (v.size+1)).reshape((v.size, v.size+1)) + np.arange(v.size+1)\n",
    "x = np.arange(3)\n",
    "y = f(x)\n",
    "g = VC.grad(f, x)\n",
    "x, y, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([20., 23., 26., 29.]),\n",
       " array([20., 23., 26., 29.]),\n",
       " array([162.00000006, 554.00000003, 946.00000009]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ g, x.dot(g), g @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a loss function $\\displaystyle loss(\\mathbf{x}) = \\frac{\\| \\mathbf{x} - \\mathbf{x}_{ideal}\\|}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal = np.array([2,3,5])\n",
    "loss = lambda v: (v - ideal).dot(v - ideal) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of this loss function at $\\mathbf{x} = (-2, 0, 1) \\circ \\mathbf{\\hat x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2,  0,  1]), 20.5, array([-4., -3., -4.]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([-2,0,1])\n",
    "loss_at_y = loss(y)\n",
    "g = VC.grad(loss, y)\n",
    "y, loss_at_y, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [-1  1], y = [3 4 5], loss at y = 1.0\n",
      "‚àáùëôùëúùë†ùë†(ùë¶) = [1. 1. 0.]\n",
      "‚àáùëôùëúùë†ùë†(ùë•) = [1. 7.]\n"
     ]
    }
   ],
   "source": [
    "f = lambda v: v @ np.arange(v.size * (v.size+1)).reshape((v.size, v.size+1)) + np.arange(v.size+1)\n",
    "x = np.array([-1,1])\n",
    "y = f(x)\n",
    "loss_at_y = loss(y)\n",
    "print(f\"x = {x}, y = {y}, loss at y = {loss_at_y}\")\n",
    "print(f\"‚àáùëôùëúùë†ùë†(ùë¶) = {VC.grad(loss, y)}\")\n",
    "print(f\"‚àáùëôùëúùë†ùë†(ùë•) = {VC.grad(lambda x:loss(f(x)), x)}\")\n",
    "g_at_x = VC.grad(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla loss(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
