{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "Version 0.3, in `nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should do [Working efficiently with jupyter lab](https://florianwilhelm.info/2018/11/working_efficiently_with_jupyter_lab/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this was a notebook with integrated tests, we did: \\\n",
    "`\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "#%matplotlib inline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of components which:\n",
    "1. accept an ordered set of reals (we'll use `numpy.array`, and  call them vectors) at the input port and produce another at the output port - this is forward propagation. ${\\displaystyle f\\colon \\mathbf {R} ^{n}\\to \\mathbf {R} ^{m}}$\n",
    "1. accept an ordered set of reals at the output port, representing the gradient of the loss function at the output, and produce the gradient of the loss function at the input port - this is back propagation, aka backprop. ${\\displaystyle b\\colon \\mathbf {R} ^{m}\\to \\mathbf {R} ^{n}}$\n",
    "1. from the gradient of the loss function at the output, calculate the partial of the loss function w.r.t the internal parameters ${\\displaystyle \\frac{\\partial E}{\\partial w} }$\n",
    "1. accept a scalar $\\eta$ to control the adjustment of internal parameters. _Or is this effected by scaling the loss gradient before passing? YES_\n",
    "1. update internal parameters ${\\displaystyle w \\leftarrow w - \\eta \\frac{\\partial E}{\\partial w} }$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Compute response to input\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backprop(self, output_delE):\n",
    "        \"\"\"Use output error gradient to adjust internal parameters, return gradient of error at input\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def state_vector(self):\n",
    "        \"\"\"Provide the layer's learnable state as a vector\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of a cascade of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.eta = 0.1 #FIXME\n",
    "        \n",
    "    def extend(self, net):\n",
    "        self.layers.append(net)\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        v = input\n",
    "        for net in self.layers:\n",
    "            v = net(v)\n",
    "        return v\n",
    "    \n",
    "    def learn(self, facts, eta=None):\n",
    "        self.eta = eta or self.eta\n",
    "        for (x, expected) in facts:\n",
    "            y = self(x)\n",
    "            e = y - expected\n",
    "            #loss = float(e.dot(e.T))/2.0\n",
    "            loss = np.einsum('ij,ij', e, e)/2.0\n",
    "            egrad = e * self.eta\n",
    "            for net in reversed(self.layers):\n",
    "                egrad = net.backprop(egrad)\n",
    "        return loss\n",
    "\n",
    "    def state_vector(self):\n",
    "        \"\"\"Provide the network's learnable state as a vector\"\"\"\n",
    "        return np.concatenate([layer.state_vector() for layer in self.layers])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            lsvlen = len(layer.state_vector())\n",
    "            layer.set_state_from_vector(sv[i:i+lsvlen])\n",
    "            i += lsvlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(Layer):\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        return output_delE\n",
    "\n",
    "    def state_vector(self):\n",
    "        return np.array([])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine\n",
    "A layer that does an [affine transformation](https://mathworld.wolfram.com/AffineTransformation.html) aka affinity, which is the classic fully-connected layer with output offsets.\n",
    "\n",
    "$$ \\mathbf{M} \\mathbf{x} + \\mathbf{b} = \\mathbf{y} $$\n",
    "where\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{j=1}^{n} x_j \\mathbf{\\hat{x}}_j \\\\\n",
    "\\mathbf{b} = \\sum_{i=1}^{m} b_i \\mathbf{\\hat{y}}_i \\\\\n",
    "\\mathbf{y} = \\sum_{i=1}^{m} y_i \\mathbf{\\hat{y}}_i\n",
    "$$\n",
    "and $\\mathbf{M}$ can be written\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    m_{1,1} & \\dots & m_{1,n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    m_{m,1} & \\dots & m_{m,n}\n",
    "\\end{bmatrix} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error gradient back-propagation\n",
    "$$ \n",
    "\\begin{align}\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{x}}\n",
    "  &= \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}} \\\\\n",
    "  &= \\mathbf{M}^\\mathsf{T}\\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter adjustment\n",
    "$$\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\mathbf{x} \\\\\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapting to `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `numpy` it is more convenient to use row vectors, particularly for calculating the transform on multiple inputs in one operation. We use the identity $ \\mathbf{M} \\mathbf{x} = (\\mathbf{x} \\mathbf{M}^\\mathsf{T})^\\mathsf{T}.$ To avoid cluttering names, we will use `M` in the code below to hold $\\mathbf{M}^\\mathsf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineLayer(Layer):\n",
    "    \"\"\"An affine transformation, which is the classic fully-connected layer with offsets.\n",
    "    \n",
    "    The layer has n inputs and m outputs, which numbers must be supplied\n",
    "    upon creation. The inputs and outputs are marshalled in numpy arrays, 1-D\n",
    "    in the case of a single calculation, and 2-D when calculating the outputs\n",
    "    of multiple inputs in one call.\n",
    "    If called with 1-D array having shape == (n,), e.g numpy.arange(n), it will\n",
    "    return a 1-D numpy array of shape (m,).\n",
    "    If called with a 2-D numpy array, input shall have shape (k,n) and will return\n",
    "    a 2-D numpy array of shape (k,m), suitable as input to a subsequent layer\n",
    "    that has input width m.\n",
    "    \"\"\"\n",
    "    def __init__(self, n, m):\n",
    "        self.M = np.empty((n, m))\n",
    "        self.b = np.empty(m)\n",
    "        self.randomize()\n",
    "        \n",
    "    def randomize(self):\n",
    "        self.M[:] = np.random.randn(*self.M.shape)\n",
    "        self.b[:] = np.random.randn(*self.b.shape)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        self.output = x @ self.M + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = output_delE @ self.M.T\n",
    "        o_delE = np.atleast_2d(output_delE)\n",
    "        self.M -= np.einsum('ki,kj->ji', o_delE, np.atleast_2d(self.input))\n",
    "        self.b -= np.sum(o_delE, 0)       \n",
    "        return input_delE\n",
    "\n",
    "    def state_vector(self):\n",
    "        return np.concatenate((self.M.ravel(), self.b.ravel()))\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        l_M = len(self.M.ravel())\n",
    "        l_b = len(self.b.ravel())\n",
    "        self.M[:] = sv[:l_M].reshape(self.M.shape)\n",
    "        self.b[:] = sv[l_M : l_M + l_b].reshape(self.b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "Maps a scalar function on the inputs, for e.g. activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapLayer(Layer):\n",
    "    \"\"\"Map a scalar function on the input taken element-wise\"\"\"\n",
    "    def __init__(self, fun, dfundx):\n",
    "        self.vfun = np.vectorize(fun)\n",
    "        self.vdfundx = np.vectorize(dfundx)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        return self.vfun(x)\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = self.vdfundx(self.input) * output_delE\n",
    "        return input_delE\n",
    "\n",
    "    def state_vector(self):\n",
    "        return np.array([])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "*Dangerously incomplete* \\\n",
    "Mostly `unittest` the `.py` version with a separate test script, see `test-nn_v3.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a few test arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_wide is:\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "three_wide is:\n",
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    two_wide = np.arange(2*4).reshape(-1,2)\n",
    "    print(f\"two_wide is:\\n{two_wide}\")\n",
    "    three_wide = np.arange(3*4).reshape(-1,3)\n",
    "    print(f\"three_wide is:\\n{three_wide}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    class VC():\n",
    "        def grad(f, x, eps=1e-6):\n",
    "            epsihat = np.eye(x.size) * eps\n",
    "            yp = np.apply_along_axis(f, 1, x + epsihat)\n",
    "            ym = np.apply_along_axis(f, 1, x - epsihat)\n",
    "            return (yp - ym)/(2 * eps)\n",
    "        \n",
    "    def closenuf(a, b, places=4):\n",
    "        return (np.around(a, places) == np.around(b, places)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    iL = IdentityLayer()\n",
    "    \n",
    "    # It's transparent from input to output\n",
    "    assert np.equal(iL(np.arange(5)), np.arange(5)).all()\n",
    "    \n",
    "    # It back-propagates the loss gradient without alteration\n",
    "    assert np.equal(iL.backprop(np.arange(7)), np.arange(7)).all()\n",
    "    assert np.equal(iL(two_wide), two_wide).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1 2 2], y = [1 4 4], loss at y = 1.5\n",
      "âˆ‡ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¦) = [-1.  1. -1.]\n",
      "âˆ‡ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¥) = [-2.  4. -4.]\n",
      "backprop([-1.  1. -1.]) = [-2.  4. -4.]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    mL = MapLayer(lambda x:x**2, lambda d:2*d)\n",
    "    x = np.array([1,2,2])\n",
    "    y = mL(x)\n",
    "    \n",
    "    # It applies the forward transformation\n",
    "    #assert np.equal(y, np.array([49,9,121])).all()\n",
    "    \n",
    "    # It back-propagages the loss gradient\n",
    "    ideal = np.array([2,3,5])\n",
    "    loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x = {x}, y = {y}, loss at y = {loss_at_y}\")\n",
    "    grad_y = VC.grad(loss, y)\n",
    "    print(f\"âˆ‡ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¦) = {grad_y}\")\n",
    "    grad_x = VC.grad(lambda x:loss(mL(x)), x)\n",
    "    print(f\"âˆ‡ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¥) = {grad_x}\")\n",
    "    \n",
    "    # See if the backprop does the same\n",
    "    _ = mL(x) # Make sure the last x is in the right place\n",
    "    in_delE = mL.backprop(grad_y)\n",
    "    print(f\"backprop({grad_y}) = {in_delE}\")\n",
    "    assert closenuf(in_delE, grad_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    \n",
    "    # Input and output widths\n",
    "    assert a(np.arange(2)).shape == (3,) \n",
    "\n",
    "    # Can set internal state\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    assert np.equal(a.M, np.array([[0, 1, 2],\n",
    "                                   [3, 4, 5]])).all()\n",
    "    assert np.equal(a.b, np.array([6, 7, 8])).all()\n",
    "\n",
    "    # Let's find the internal state using numerical gradient\n",
    "    x = np.random.rand(2)\n",
    "    y = a(x)\n",
    "    dydx = VC.grad(a, x)\n",
    "    b = y - x.dot(dydx)\n",
    "    #print(dydx, b)\n",
    "    #print(dydx, np.arange(6).reshape(2,-1))\n",
    "    assert closenuf(dydx, np.arange(6).reshape(2, -1))\n",
    "    #print(b, np.arange(6, 9))\n",
    "    assert closenuf(b, np.arange(6, 9))\n",
    "    \n",
    "    # Single-input calculation\n",
    "    x = np.array([2, 1])\n",
    "    y = a(x)\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\\n\")\n",
    "    assert np.equal(y, np.array([9, 13, 17])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [2 1], y = [ 9. 13. 17.], loss at y = 27.0\n",
      "âˆ‡ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¦) = [-2.          1.          7.00000001]\n",
      "âˆ‡ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¥) = [15.00000002 33.00000001]\n",
      "backprop([-0.2  0.1  0.7]) = [1.5 3.3]\n",
      "Now a([2 1]) = [10.2 12.4 12.8], loss = 4.319999988809314\n",
      "state_vector is [0.4 0.8 0.6 3.2 3.9 4.3 6.2 6.9 7.3]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "\n",
    "    # Single-input calculation\n",
    "    x = np.array([2, 1])\n",
    "    y = a(x)\n",
    "    assert np.equal(y, np.array([9, 13, 17])).all()\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\\n\")\n",
    "\n",
    "    # It back-propagages the loss gradient\n",
    "    ideal = np.array([11,12,10])\n",
    "    loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x = {x}, y = {y}, loss at y = {loss_at_y}\")\n",
    "    grad_y = VC.grad(loss, y)\n",
    "    print(f\"âˆ‡ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¦) = {grad_y}\")\n",
    "    grad_x = VC.grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"âˆ‡ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¥) = {grad_x}\")\n",
    "    \n",
    "    # See if the backprop does the same\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.1\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a(two_wide) is:\n",
      "[[ 9. 11. 13.]\n",
      " [15. 21. 27.]\n",
      " [21. 31. 41.]\n",
      " [27. 41. 55.]]\n",
      "bp is:\n",
      "[[0.005 0.014]\n",
      " [0.014 0.05 ]\n",
      " [0.023 0.086]\n",
      " [0.032 0.122]]\n",
      "a.M is:\n",
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]]\n",
      "a.b is [6. 7. 8.]\n",
      "x is: [[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "y is: [ 9. 13. 17.]\n",
      "x is: [[29 31]]\n",
      "y is: [[292. 447. 571.]]\n",
      "in_grad is:\n",
      "[0.049 0.141]\n",
      "a(two_wide) is:\n",
      "[[ 23.872  29.936  35.776]\n",
      " [ 41.392  57.696  70.936]\n",
      " [ 58.912  85.456 106.096]\n",
      " [ 76.432 113.216 141.256]]\n",
      "bp is:\n",
      "[[0.012536 0.036504]\n",
      " [0.041405 0.128295]\n",
      " [0.070274 0.220086]\n",
      " [0.099143 0.311877]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    \n",
    "    print(f\"a(two_wide) is:\\n{a(two_wide)}\")\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    print(f\"bp is:\\n{bp}\")\n",
    "\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    assert np.equal(a(x), np.array([[ 9, 11, 13],\n",
    "                                    [15, 21, 27],\n",
    "                                    [21, 31, 41],\n",
    "                                    [27, 41, 55]])).all()\n",
    "    \n",
    "    \n",
    "    print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "    \n",
    "    \n",
    "    a.set_state_from_vector(np.array([ 2,  3,  5,  7, 11, 13, 17, 19, 23]))\n",
    "    x = np.array([[29, 31]])\n",
    "    y = a(x)\n",
    "    print(f\"x is: {x}\\ny is: {y}\")\n",
    "    \n",
    "    # AffineLayer has parameters that learn\n",
    "    out_grad = np.array([4, 2, 7]) * 0.001\n",
    "    in_grad = a.backprop(out_grad)\n",
    "    print(f\"in_grad is:\\n{in_grad}\")\n",
    "    print(f\"a(two_wide) is:\\n{a(two_wide)}\")\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    print(f\"bp is:\\n{bp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False and __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    \n",
    "    # Input and output widths\n",
    "    assert a(np.arange(2)).shape == (3,) \n",
    "\n",
    "    # Can set internal state\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    assert np.equal(a.M, np.array([[0, 1, 2],\n",
    "                                   [3, 4, 5]])).all()\n",
    "    assert np.equal(a.b, np.array([6, 7, 8])).all()\n",
    "\n",
    "    # Single-input calculation\n",
    "    x = np.array([2, 1])\n",
    "    y = a(x)\n",
    "    assert np.equal(y, np.array([9, 13, 17])).all()\n",
    "    print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "\n",
    "    # AffineLayer has parameters that learn\n",
    "    out_grad = np.array([5, 3, 2]) * 0.001\n",
    "    in_grad = a.backprop(out_grad)\n",
    "    print(f\"out_grad is: {out_grad}, in_grad is:{in_grad}\")\n",
    "    print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "    \n",
    "    \n",
    "    print(f\"a(two_wide) is:\\n{a(two_wide)}\")\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    print(f\"bp is:\\n{bp}\")\n",
    "\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    assert np.equal(a(x), np.array([[ 9, 11, 13],\n",
    "                                    [15, 21, 27],\n",
    "                                    [21, 31, 41],\n",
    "                                    [27, 41, 55]])).all()\n",
    "    \n",
    "    \n",
    "    print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "    \n",
    "    \n",
    "    a.set_state_from_vector(np.array([ 2,  3,  5,  7, 11, 13, 17, 19, 23]))\n",
    "    x = np.array([[29, 31]])\n",
    "    y = a(x)\n",
    "    print(f\"x is: {x}\\ny is: {y}\")\n",
    "    \n",
    "    # AffineLayer has parameters that learn\n",
    "    out_grad = np.array([4, 2, 7]) * 0.001\n",
    "    in_grad = a.backprop(out_grad)\n",
    "    print(f\"in_grad is:\\n{in_grad}\")\n",
    "    print(f\"a(two_wide) is:\\n{a(two_wide)}\")\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    print(f\"bp is:\\n{bp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce an importable `nn.py`:\n",
    "1. Save this notebook\n",
    "1. Uncomment the `jupyter nbconvert` line below\n",
    "1. Execute it.\n",
    "1. Comment out the convert again\n",
    "1. Save the notebook again in that form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook nn.ipynb to script\n",
      "[NbConvertApp] Writing 9337 bytes to nn.py\n"
     ]
    }
   ],
   "source": [
    "###!jupyter nbconvert --to script nn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
