{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "choice-genius",
   "metadata": {},
   "source": [
    "# Graph Experiment 2: Experiment Preparation\n",
    "Set up experiments for the bots to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-shareware",
   "metadata": {},
   "source": [
    "## Capture code as strings\n",
    "We collect code from chosen cells as strings, and place them in nodes in the graph. The code is intended to be sufficient to reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.nnbench import NetMaker, NNMEG\n",
    "import secrets\n",
    "import time\n",
    "import lib.neotools as nj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strings.append(In[-2]) # Grab the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import neo4j\n",
    "import lib.grexutils as gu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-corruption",
   "metadata": {},
   "source": [
    "## Connecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = neo4j.GraphDatabase.driver(\"neo4j://neo4j:7687\", auth=(\"neo4j\", \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-regulation",
   "metadata": {},
   "source": [
    "# The graph database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "display(SVG('ml graphdb structure r2.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-hopkins",
   "metadata": {},
   "source": [
    "# Prepare code for later `eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    np.random.seed(random_seed)\n",
    "except NameError:\n",
    "    np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strings.append(In[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-factory",
   "metadata": {},
   "source": [
    "## Create the net we will train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_shorthand = '1x8tx8tx3tx3t'\n",
    "# NOTE this is not captured for running by workers!\n",
    "# It is here as a stand-in to prevent NameError.\n",
    "# The Parameters node's prepend code string provides it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnm = NetMaker(NNMEG)\n",
    "#net = mnm('1x8tx8tx3tx3t')\n",
    "net = mnm(net_shorthand)\n",
    "\n",
    "def adc(input):\n",
    "    m = max(0, min(7, int(8*input)))\n",
    "    return np.array([(m>>2)&1, (m>>1)&1, m&1]) * 2 - 1\n",
    "\n",
    "vadc = lambda v: np.array([adc(p) for p in v])\n",
    "\n",
    "x = np.arange(0, 1, 1.0/(8*8)).reshape(-1,1) # 8 points in each output region\n",
    "training_batch = (x, vadc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strings.append(In[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-pound",
   "metadata": {},
   "source": [
    "## First net node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_net_start_to_graph(driver, facts):\n",
    "    q = \"\"\"\n",
    "MATCH (par:Parameters {unikey: $parameters_unikey})\n",
    "CREATE (par)-[:CONFIGURES]->(:Net \n",
    "            {shorthand: $shorthand,\n",
    "                unikey: $unikey,\n",
    "                   ksv: $ksv,\n",
    "                  loss: $loss,\n",
    "                    ts: timestamp(),\n",
    "                  head: $head,\n",
    "    batches_from_start: 0})\n",
    "\"\"\"\n",
    "    d = {'unikey': secrets.token_urlsafe(16)}\n",
    "    d.update(**facts)\n",
    "    nj.query_write(driver, q, **d)\n",
    "    return d['unikey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strings.append(In[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-principal",
   "metadata": {},
   "source": [
    "## Trained net nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_net_subsequent_to_graph(driver, facts):\n",
    "    q = \"\"\"\n",
    "MATCH (a:Net {unikey: $prior_unikey})\n",
    "CREATE (a)-[:LEARNED\n",
    "         {batch_points: $batch_points,\n",
    "                  etas: $etas,\n",
    "    eta_change_batches: $eta_change_batches,\n",
    "  batches_this_segment: $batches_this_segment,\n",
    "                losses: $loss,\n",
    "            loss_steps: $loss_step,\n",
    "           traj_L2_sqs: $traj_L2_sq,\n",
    "   traj_cos_sq_signeds: $traj_cos_sq_signed,\n",
    "                    ts: timestamp()}]->\n",
    "(b:Net\n",
    "            {shorthand: $shorthand,\n",
    "                unikey: $unikey,\n",
    "                   ksv: $ksv,\n",
    "                  loss: $end_loss,\n",
    "                    ts: timestamp(),\n",
    "    batches_from_start: $batches_from_start})\n",
    "\"\"\"\n",
    "    d = {'unikey': secrets.token_urlsafe(16)}\n",
    "    d.update(**facts)\n",
    "    nj.query_write(driver, q, **d)\n",
    "    return d['unikey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strings.append(In[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-associate",
   "metadata": {},
   "source": [
    "## Train, recording trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net_an_increment(net, nps, properties):\n",
    "    loss = net.losses([training_batch])[0]\n",
    "    batch_ctr = 0\n",
    "    while loss > 1e-3:\n",
    "        batch_ctr_at_seg_start = batch_ctr\n",
    "        losses = []\n",
    "        etas = []\n",
    "        deltas = []\n",
    "        prior_loss = loss\n",
    "        while loss / prior_loss > 0.7071 and len(deltas) < 100:\n",
    "            if not etas or net.eta != etas[-1][1]:\n",
    "                etas.append([batch_ctr, net.eta])\n",
    "            loss = net.learn([training_batch])\n",
    "            if batch_ctr < 100 or batch_ctr % 100 == 0:\n",
    "                losses.append([batch_ctr, loss])\n",
    "                deltas.append([batch_ctr, net.deltas()])\n",
    "            batch_ctr += 1\n",
    "        #if losses[-1][0] < (batch_ctr-1):\n",
    "        #    losses.append([batch_ctr, loss])\n",
    "        if not deltas or deltas[-1][0] < (batch_ctr-1):\n",
    "            deltas.append((batch_ctr, net.deltas()))\n",
    "        properties = dict(zip(deltas[0][1]._fields, map(list, (zip(*(v[1] for v in deltas))))))\n",
    "        #properties = {}\n",
    "        properties['batch_points'] = [v[0] for v in deltas]\n",
    "        #properties['etas'] = etas\n",
    "        properties['etas'], properties['eta_change_batches'] = (list(v) for v in zip(*etas))\n",
    "        properties['batches_this_segment'] = batch_ctr - batch_ctr_at_seg_start\n",
    "        properties['ts'] = time.time()\n",
    "        properties['shorthand'] = net.shorthand\n",
    "        properties['ksv'] = nps.store(net.state_vector())\n",
    "        properties['end_loss'] = net.losses([training_batch])[0]\n",
    "        properties['experiment'] = 'ADC'\n",
    "        yield properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(net, nps):\n",
    "    loss = net.losses([training_batch])[0]\n",
    "    batch_ctr = 0\n",
    "    for i in range(3):\n",
    "        batch_ctr_at_seg_start = batch_ctr\n",
    "        losses = []\n",
    "        etas = []\n",
    "        deltas = []\n",
    "        prior_loss = loss\n",
    "        while loss / prior_loss > 0.7071 and len(deltas) < 100:\n",
    "            if not etas or net.eta != etas[-1][1]:\n",
    "                etas.append([batch_ctr, net.eta])\n",
    "            loss = net.learn([training_batch])\n",
    "            if batch_ctr < 100 or batch_ctr % 100 == 0:\n",
    "                losses.append([batch_ctr, loss])\n",
    "                deltas.append([batch_ctr, net.deltas()])\n",
    "            batch_ctr += 1\n",
    "        #if losses[-1][0] < (batch_ctr-1):\n",
    "        #    losses.append([batch_ctr, loss])\n",
    "        if not deltas or deltas[-1][0] < (batch_ctr-1):\n",
    "            deltas.append((batch_ctr, net.deltas()))\n",
    "        properties = dict(zip(deltas[0][1]._fields, map(list, (zip(*(v[1] for v in deltas))))))\n",
    "        #properties = {}\n",
    "        properties['batch_points'] = [v[0] for v in deltas]\n",
    "        #properties['etas'] = etas\n",
    "        properties['etas'], properties['eta_change_batches'] = (list(v) for v in zip(*etas))\n",
    "        properties['batches_this_segment'] = batch_ctr - batch_ctr_at_seg_start\n",
    "        properties['ts'] = time.time()\n",
    "        properties['shorthand'] = net.shorthand\n",
    "        properties['ksv'] = nps.store(net.state_vector())\n",
    "        properties['end_loss'] = net.losses([training_batch])[0]\n",
    "        properties['batches_from_start'] = batch_ctr\n",
    "        yield properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(net, nps):\n",
    "    loss = net.losses([training_batch])[0]\n",
    "    batch_ctr = 0\n",
    "    while loss > 1e-3 and batch_ctr < 100_000:\n",
    "        batch_ctr_at_seg_start = batch_ctr\n",
    "        losses = []\n",
    "        etas = []\n",
    "        deltas = []\n",
    "        prior_loss = loss\n",
    "        while loss / prior_loss > 0.7071 and len(deltas) < 100:\n",
    "            if not etas or net.eta != etas[-1][1]:\n",
    "                etas.append([batch_ctr, net.eta])\n",
    "            loss = net.learn([training_batch])\n",
    "            if batch_ctr < 100 or batch_ctr % 100 == 0:\n",
    "                losses.append([batch_ctr, loss])\n",
    "                deltas.append([batch_ctr, net.deltas()])\n",
    "            batch_ctr += 1\n",
    "        #if losses[-1][0] < (batch_ctr-1):\n",
    "        #    losses.append([batch_ctr, loss])\n",
    "        if not deltas or deltas[-1][0] < (batch_ctr-1):\n",
    "            deltas.append((batch_ctr, net.deltas()))\n",
    "        properties = dict(zip(deltas[0][1]._fields, map(list, (zip(*(v[1] for v in deltas))))))\n",
    "        #properties = {}\n",
    "        properties['batch_points'] = [v[0] for v in deltas]\n",
    "        #properties['etas'] = etas\n",
    "        properties['etas'], properties['eta_change_batches'] = (list(v) for v in zip(*etas))\n",
    "        properties['batches_this_segment'] = batch_ctr - batch_ctr_at_seg_start\n",
    "        properties['ts'] = time.time()\n",
    "        properties['shorthand'] = net.shorthand\n",
    "        properties['ksv'] = nps.store(net.state_vector())\n",
    "        properties['end_loss'] = net.losses([training_batch])[0]\n",
    "        properties['batches_from_start'] = batch_ctr\n",
    "        yield properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(net, nps):\n",
    "    loss = net.losses([training_batch])[0]\n",
    "    batch_ctr = 0\n",
    "    batch_delta = 1.0\n",
    "    while batch_ctr < batch_limit:\n",
    "        batch_delta *= batch_delta_factor\n",
    "        batch_ctr_at_seg_start = batch_ctr\n",
    "        losses = []\n",
    "        etas = []\n",
    "        deltas = []\n",
    "        prior_loss = loss\n",
    "        for i in range(int(batch_delta)):\n",
    "            if not etas or net.eta != etas[-1][1]:\n",
    "                etas.append([batch_ctr, net.eta])\n",
    "            loss = net.learn([training_batch])\n",
    "            if batch_ctr < 100 or batch_ctr % 100 == 0:\n",
    "                losses.append([batch_ctr, loss])\n",
    "                deltas.append([batch_ctr, net.deltas()])\n",
    "            batch_ctr += 1\n",
    "        if not deltas or deltas[-1][0] < (batch_ctr-1):\n",
    "            deltas.append((batch_ctr, net.deltas()))\n",
    "        properties = dict(zip(deltas[0][1]._fields, map(list, (zip(*(v[1] for v in deltas))))))\n",
    "        properties['batch_points'] = [v[0] for v in deltas]\n",
    "        properties['eta_change_batches'], properties['etas'] = (list(v) for v in zip(*etas))\n",
    "        properties['batches_this_segment'] = batch_ctr - batch_ctr_at_seg_start\n",
    "        properties['ts'] = time.time()\n",
    "        properties['shorthand'] = net.shorthand\n",
    "        properties['ksv'] = nps.store(net.state_vector())\n",
    "        properties['end_loss'] = net.losses([training_batch])[0]\n",
    "        properties['batches_from_start'] = batch_ctr\n",
    "        yield properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strings.append(In[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starting_facts(net, nps):\n",
    "    rv = {'shorthand': net.shorthand,\n",
    "          'ksv': nps.store(net.state_vector()),\n",
    "          'loss': net.losses([training_batch])[0],\n",
    "          'head': True,\n",
    "         }\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strings.append(In[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-center",
   "metadata": {},
   "source": [
    "## Build the runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_starting_entry(driver, net, nps, get_starting_facts, q_add_start):\n",
    "    starting_facts = get_starting_facts(net, nps)\n",
    "    tj.query_write(driver, q_add_start, **starting_facts)\n",
    "    return starting_facts['ksv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code_strings.append(In[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_by_one(driver, net, ksv, nps, observations, add_subsequent):\n",
    "    with driver.session() as session:\n",
    "        session.write_transaction(add_subsequent, observations, net)\n",
    "        print(f\"loss {observations['end_loss']}\")\n",
    "        return observations['ksv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code_strings.append(In[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_it(cx, driver, nps):\n",
    "    net = cx['net']\n",
    "    add_start = cx['add_net_start_to_graph']\n",
    "    add_subsequent = cx['add_net_subsequent_to_graph']\n",
    "    get_starting_facts = cx['get_starting_facts']\n",
    "    trainer = cx['trainer']\n",
    "\n",
    "    with driver.session() as session:\n",
    "        starting_facts = get_starting_facts(net, nps)\n",
    "        starting_facts['parameters_unikey'] = cx['parameters_unikey']\n",
    "        print(starting_facts)\n",
    "        prior_unikey = add_start(driver, starting_facts)\n",
    "        #prior_ksv = starting_facts['ksv']\n",
    "        for observations in trainer(net, nps):\n",
    "            #observations['prior_ksv'] = prior_ksv\n",
    "            #prior_ksv = observations['ksv']\n",
    "            observations['prior_unikey'] = prior_unikey\n",
    "            prior_unikey = add_subsequent(driver, observations)\n",
    "            #print(f\"loss {observations['end_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strings.append(In[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-chemical",
   "metadata": {},
   "source": [
    "* At this point, all the code we need to have in the procedure has been prepared and placed in `code_strings`. It is also in the current notebook context and we can test it here too.\n",
    "* Some run-specific parameters will be placed in the Parameters nodes created by the methods below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-final",
   "metadata": {},
   "source": [
    "# Create the Experiment -> Procedure -> Parameters\n",
    "Create an experiment, add a procedure, add parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-found",
   "metadata": {},
   "source": [
    "# Experiment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-pollution",
   "metadata": {},
   "source": [
    "## Switches on what to set up now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_experiment = False\n",
    "setup_procedure = False\n",
    "setup_parameters = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-saturn",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 't4'\n",
    "procedure_name = 'Train Pauls ADCs'\n",
    "parameters = defaultdict(dict)\n",
    "parameters['eps 0.01']['prepend_code_strings'] = [\"net_shorthand = '1x3sx3t'\",\n",
    "                                                 'batch_limit = 10_000',\n",
    "                                                 'batch_delta_factor = 2**(1/5)']\n",
    "parameters['eps 0.01']['append_code_strings'] = ['net.eps = 0.01']\n",
    "parameters['eps 0.01']['trials'] = 1024\n",
    "parameters['eps 0.03']['prepend_code_strings'] = [\"net_shorthand = '1x3sx3t'\",\n",
    "                                                 'batch_limit = 10_000',\n",
    "                                                 'batch_delta_factor = 2**(1/5)']\n",
    "parameters['eps 0.03']['append_code_strings'] = ['net.eps = 0.03']\n",
    "parameters['eps 0.03']['trials'] = 1024\n",
    "parameters['eps 0.1']['prepend_code_strings'] = [\"net_shorthand = '1x3sx3t'\",\n",
    "                                                 'batch_limit = 10_000',\n",
    "                                                 'batch_delta_factor = 2**(1/5)']\n",
    "parameters['eps 0.1']['append_code_strings'] = ['net.eps = 0.1']\n",
    "parameters['eps 0.1']['trials'] = 1024\n",
    "parameters['eps 0.3']['prepend_code_strings'] = [\"net_shorthand = '1x3sx3t'\",\n",
    "                                                 'batch_limit = 10_000',\n",
    "                                                 'batch_delta_factor = 2**(1/5)']\n",
    "parameters['eps 0.3']['append_code_strings'] = ['net.eps = 0.3']\n",
    "parameters['eps 0.3']['trials'] = 1024\n",
    "parameters['eps 1.0']['prepend_code_strings'] = [\"net_shorthand = '1x3sx3t'\",\n",
    "                                                 'batch_limit = 10_000',\n",
    "                                                 'batch_delta_factor = 2**(1/5)']\n",
    "parameters['eps 1.0']['append_code_strings'] = ['net.eps = 1.0']\n",
    "parameters['eps 1.0']['trials'] = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-metadata",
   "metadata": {},
   "source": [
    "## Do setups according to the switches above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup_experiment:\n",
    "    experiment_unikey = secrets.token_urlsafe(16)\n",
    "    gu.create_an_experiment(driver,\n",
    "        experiment_name=experiment_name,\n",
    "        experiment_unikey=experiment_unikey,\n",
    "    )\n",
    "else:\n",
    "    experiment_unikey = gu.get_experiment_key_from_name(driver, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup_procedure:\n",
    "    procedure_unikey = secrets.token_urlsafe(16)\n",
    "    gu.create_a_procedure(driver,\n",
    "        experiment_unikey=experiment_unikey,\n",
    "        procedure_name=procedure_name,\n",
    "        procedure_unikey=procedure_unikey,\n",
    "        code_strings=code_strings,\n",
    "    )\n",
    "else:\n",
    "    procedure_unikey = dict(gu.get_procedure_names_keys_from_experiment_key(driver, experiment_unikey))[procedure_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup_parameters:\n",
    "    for name, params in parameters.items():\n",
    "        trials = params['trials']\n",
    "        if isinstance(trials, int):\n",
    "            trials = (trials,)\n",
    "        for i in range(*trials):\n",
    "            parameters_unikey = secrets.token_urlsafe(16)\n",
    "            random_seed = abs(np.frombuffer(secrets.token_bytes(4), dtype='int32')[0])\n",
    "            prepend_code_strings = [s for s in params['prepend_code_strings']]\n",
    "            append_code_strings = [s for s in params['append_code_strings']]\n",
    "            prepend_code_strings.append(f\"random_seed = {random_seed}\")\n",
    "            prepend_code_strings.append(f\"trial_number = {i}\")\n",
    "            prepend_code_strings.append(f\"parameters_unikey = '{parameters_unikey}'\")\n",
    "            #print(i, code_strings)\n",
    "            gu.create_parameters_to_experiment_procedure(driver,\n",
    "                procedure_unikey=procedure_unikey,\n",
    "                parameters_name=f\"{name} {i}\",\n",
    "                parameters_unikey=parameters_unikey,\n",
    "                prepend_code_strings=prepend_code_strings,\n",
    "                append_code_strings=append_code_strings,\n",
    "                trial=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-capitol",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-champagne",
   "metadata": {},
   "source": [
    "# Here the experiment is set up in the graph database\n",
    "\n",
    "We could:\n",
    "* Run it locally here\n",
    "* Run it from the code strings we have stored here\n",
    "* Run it like the bot would, by getting the code from the database\n",
    "* Launch the bot against it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run == 'local':\n",
    "    nps = nj.NumpyStore(driver)\n",
    "    run_it(globals(), driver, nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run == 'code_strings':\n",
    "    nps = nj.NumpyStore(driver)\n",
    "    cx = {}\n",
    "    for s in code_strings:\n",
    "        exec(s, cx)\n",
    "    cx['run_it'](cx, driver, nps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-diploma",
   "metadata": {},
   "source": [
    "## Find work and do it\n",
    "As the bot would"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_strings_from_db(driver, experiment_name, procedure_name):\n",
    "    experiment_unikey = get_experiment_key_from_name(driver, name=experiment_name)\n",
    "    #print(experiment_unikey)\n",
    "    procedures = dict(get_procedure_names_keys_from_experiment_key(driver, key=experiment_unikey))\n",
    "    procedure_unikey = procedures[procedure_name]\n",
    "    #print(procedure_unikey)\n",
    "    unstarted_parameters = get_unstarted_parameters_of_procedure(driver, procedure_unikey=procedure_unikey)\n",
    "    parameters_unikey = unstarted_parameters[0]\n",
    "    #print(parameters_unikey)\n",
    "    code_strings_from_db = get_code_strings_of_experiment_procedure_parameters(driver,\n",
    "        experiment_unikey=experiment_unikey,\n",
    "        procedure_unikey=procedure_unikey,\n",
    "        parameters_unikey=parameters_unikey)\n",
    "    return code_strings_from_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_run_it(driver, code_strings):\n",
    "    cx = {}\n",
    "    for s in code_strings:\n",
    "        exec(s, cx)\n",
    "    nps = nj.NumpyStore(driver)\n",
    "    cx['run_it'](cx, driver, nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run == 'like bot':\n",
    "    code_strings = get_code_strings_from_db(driver, 't2', 'Train ADCs')\n",
    "    now_run_it(driver, code_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run == 'bot':\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    experiment_unikey = gu.get_experiment_key_from_name(driver, name=experiment_name)\n",
    "    print(experiment_unikey)\n",
    "    procedures = dict(gu.get_procedure_names_keys_from_experiment_key(driver, key=experiment_unikey))\n",
    "    procedure_unikey = procedures[procedure_name]\n",
    "    unstarted_parameters = gu.get_unstarted_parameters_of_procedure(driver, procedure_unikey=procedure_unikey)\n",
    "    parameters_unikey = unstarted_parameters[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strings_from_db = gu.get_code_strings_from_db(driver, experiment_name, procedure_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(('\\n\\n/' + '*'*80 + '/\\n').join(code_strings_from_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-nirvana",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-column",
   "metadata": {},
   "source": [
    "# Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"stop here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-million",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
