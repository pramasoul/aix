{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "Version 0.5, in `nn`\n",
    "\n",
    "# Note:\n",
    "This is the source for `nn.py`, the foundation library of these neural net experiments. It contains a simple library, and a quantity, never sufficient, of test code, guarded by `if __name__ = '__main__'`. It makes various noises as it runs, but should not fail any assertsSee the bottom of the file for the procedure to produce the importable library file `nn.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should do [Working efficiently with jupyter lab](https://florianwilhelm.info/2018/11/working_efficiently_with_jupyter_lab/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of components which:\n",
    "1. accept an ordered set of reals (we'll use `numpy.array`, and  call them vectors) at the input port and produce another at the output port - this is forward propagation. ${\\displaystyle f\\colon \\mathbf {R} ^{n}\\to \\mathbf {R} ^{m}}$\n",
    "1. accept an ordered set of reals at the output port, representing the gradient of the loss function at the output, and produce the gradient of the loss function at the input port - this is back propagation, aka backprop. ${\\displaystyle b\\colon \\mathbf {R} ^{m}\\to \\mathbf {R} ^{n}}$\n",
    "1. from the gradient of the loss function at the output, calculate the partial of the loss function w.r.t the internal parameters ${\\displaystyle \\frac{\\partial E}{\\partial w} }$\n",
    "1. accept a scalar $\\eta$ to control the adjustment of internal parameters. _Or is this effected by scaling the loss gradient before passing? YES_\n",
    "1. update internal parameters ${\\displaystyle w \\leftarrow w - \\eta \\frac{\\partial E}{\\partial w} }$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Compute response to input\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backprop(self, output_delE):\n",
    "        \"\"\"Use output error gradient to adjust internal parameters, return gradient of error at input\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def state_vector(self):\n",
    "        \"\"\"Provide the layer's learnable state as a vector\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of a cascade of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    version_string = '0.5'\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.eta = 0.1 #FIXME\n",
    "        \n",
    "    def extend(self, net):\n",
    "        self.layers.append(net)\n",
    "        return self\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        v = input\n",
    "        for net in self.layers:\n",
    "            v = net(v)\n",
    "        return v\n",
    "    \n",
    "    def learn(self, facts, eta=None):\n",
    "        eta = eta or self.eta\n",
    "        for x, ideal in facts:\n",
    "            y = self(x)\n",
    "            e = y - ideal\n",
    "            egrad = e * eta / e.shape[0]\n",
    "            for net in reversed(self.layers):\n",
    "                egrad = net.backprop(egrad)\n",
    "        #loss = float(e.dot(e.T))/2.0\n",
    "        loss = np.einsum('...ij,...ij', e, e) / (2.0 * e.shape[0])\n",
    "        self.eta = eta\n",
    "        return loss\n",
    "\n",
    "    def losses(self, facts):\n",
    "        return [np.einsum('...ij,...ij', e, e) / (2.0 * e.shape[0]) \\\n",
    "                for e in (self(x) - ideal for x, ideal in facts)]\n",
    "        \n",
    "    def state_vector(self):\n",
    "        \"\"\"Provide the network's learnable state as a vector\"\"\"\n",
    "        return np.concatenate([layer.state_vector() for layer in self.layers])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            lsvlen = len(layer.state_vector())\n",
    "            layer.set_state_from_vector(sv[i:i+lsvlen])\n",
    "            i += lsvlen\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(Layer):\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        return output_delE\n",
    "\n",
    "    def state_vector(self):\n",
    "        return np.array([])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine\n",
    "A layer that does an [affine transformation](https://mathworld.wolfram.com/AffineTransformation.html) aka affinity, which is the classic fully-connected layer with output offsets.\n",
    "\n",
    "$$ \\mathbf{M} \\mathbf{x} + \\mathbf{b} = \\mathbf{y} $$\n",
    "where\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{j=1}^{n} x_j \\mathbf{\\hat{x}}_j \\\\\n",
    "\\mathbf{b} = \\sum_{i=1}^{m} b_i \\mathbf{\\hat{y}}_i \\\\\n",
    "\\mathbf{y} = \\sum_{i=1}^{m} y_i \\mathbf{\\hat{y}}_i\n",
    "$$\n",
    "and $\\mathbf{M}$ can be written\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    m_{1,1} & \\dots & m_{1,n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    m_{m,1} & \\dots & m_{m,n}\n",
    "\\end{bmatrix} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error gradient back-propagation\n",
    "$$ \n",
    "\\begin{align}\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{x}}\n",
    "  &= \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}} \\\\\n",
    "  &= \\mathbf{M}^\\mathsf{T}\\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter adjustment\n",
    "$$\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\mathbf{x} \\\\\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapting to `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `numpy` it is more convenient to use row vectors, particularly for calculating the transform on multiple inputs in one operation. We use the identity $ \\mathbf{M} \\mathbf{x} = (\\mathbf{x} \\mathbf{M}^\\mathsf{T})^\\mathsf{T}.$ To avoid cluttering names, we will use `M` in the code below to hold $\\mathbf{M}^\\mathsf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineLayer(Layer):\n",
    "    \"\"\"An affine transformation, which is the classic fully-connected layer with offsets.\n",
    "    \n",
    "    The layer has n inputs and m outputs, which numbers must be supplied\n",
    "    upon creation. The inputs and outputs are marshalled in numpy arrays, 1-D\n",
    "    in the case of a single calculation, and 2-D when calculating the outputs\n",
    "    of multiple inputs in one call.\n",
    "    If called with 1-D array having shape == (n,), e.g numpy.arange(n), it will\n",
    "    return a 1-D numpy array of shape (m,).\n",
    "    If called with a 2-D numpy array, input shall have shape (k,n) and will return\n",
    "    a 2-D numpy array of shape (k,m), suitable as input to a subsequent layer\n",
    "    that has input width m.\n",
    "    \"\"\"\n",
    "    def __init__(self, n, m):\n",
    "        self.M = np.empty((n, m))\n",
    "        self.b = np.empty(m)\n",
    "        self.randomize()\n",
    "        \n",
    "    def randomize(self):\n",
    "        self.M[:] = np.random.randn(*self.M.shape)\n",
    "        self.b[:] = np.random.randn(*self.b.shape)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        self.output = x @ self.M + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = output_delE @ self.M.T\n",
    "        o_delE = np.atleast_2d(output_delE)\n",
    "        self.M -= np.einsum('...ki,...kj->...ji', o_delE, np.atleast_2d(self.input))\n",
    "        self.b -= np.sum(o_delE, 0)       \n",
    "        return input_delE\n",
    "\n",
    "    def state_vector(self):\n",
    "        return np.concatenate((self.M.ravel(), self.b.ravel()))\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        l_M = len(self.M.ravel())\n",
    "        l_b = len(self.b.ravel())\n",
    "        self.M[:] = sv[:l_M].reshape(self.M.shape)\n",
    "        self.b[:] = sv[l_M : l_M + l_b].reshape(self.b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "Maps a scalar function on the inputs, for e.g. activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapLayer(Layer):\n",
    "    \"\"\"Map a scalar function on the input taken element-wise\"\"\"\n",
    "    def __init__(self, fun, dfundx):\n",
    "        self.vfun = np.vectorize(fun)\n",
    "        self.vdfundx = np.vectorize(dfundx)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        return self.vfun(x)\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = self.vdfundx(self.input) * output_delE\n",
    "        return input_delE\n",
    "\n",
    "    def state_vector(self):\n",
    "        return np.array([])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "*Incomplete* \\\n",
    "Also `unittest` the `.py` version with a separate test script, see `test-nn_v3.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a few test arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    one_wide = np.atleast_2d(np.arange(1*4)).reshape(-1,1)\n",
    "    print(f\"one_wide is:\\n{one_wide}\")\n",
    "    two_wide = np.arange(2*4).reshape(-1,2)\n",
    "    print(f\"two_wide is:\\n{two_wide}\")\n",
    "    three_wide = np.arange(3*4).reshape(-1,3)\n",
    "    print(f\"three_wide is:\\n{three_wide}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tooling for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import sympy\n",
    "    \n",
    "    class VC():\n",
    "        def grad(f, x, eps=1e-6):\n",
    "            #epsihat = np.eye(x.size) * eps\n",
    "            epsihat = np.eye(x.shape[-1]) * eps\n",
    "            yp = np.apply_along_axis(f, 1, x + epsihat)\n",
    "            ym = np.apply_along_axis(f, 1, x - epsihat)\n",
    "            return (yp - ym)/(2 * eps)\n",
    "        \n",
    "        def tensor_grad(f, x, eps=1e-6):\n",
    "            return np.apply_along_axis(lambda v: VC.grad(f, v, eps), 1, x)\n",
    "            \n",
    "    def closenuf(a, b, tol=0.001):\n",
    "        return np.allclose(a, b, rtol=tol)\n",
    "    \n",
    "    def arangep(n, starting_index=0):\n",
    "        sympy.sieve.extend_to_no(starting_index + n)\n",
    "        return np.array(sympy.sieve._list[starting_index:starting_index + n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VC.grad(lambda x:x**2, three_wide[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VC.tensor_grad(lambda x:x**2, three_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input to a layer can be a single (row) vector, or a vertical stack of row vectors,\n",
    "a 2-d array that resembles a matrix. We need to test each layer class with both single and stacked input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    iL = IdentityLayer()\n",
    "    \n",
    "    # It's transparent from input to output\n",
    "    assert np.equal(iL(np.arange(5)), np.arange(5)).all()\n",
    "    assert (iL(three_wide) == three_wide).all()\n",
    "    \n",
    "    # It back-propagates the loss gradient without alteration\n",
    "    assert np.equal(iL.backprop(np.arange(7)), np.arange(7)).all()\n",
    "    assert (iL.backprop(three_wide) == three_wide).all()\n",
    "\n",
    "    # It works for stacked input\n",
    "    # (see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test single vector input behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    mL = MapLayer(lambda x:x**2, lambda d:2*d)\n",
    "    \n",
    "    # It applies the forward transformation\n",
    "    assert np.equal(mL(np.array([-2,1,3])), np.array([4,1,9])).all()\n",
    "    \n",
    "    # It back-propagages the loss gradient\n",
    "    x = np.array([1,2,2])\n",
    "    y = mL(x)\n",
    "    \n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = np.array([2,3,5])\n",
    "    loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x = {x}, y = {y}, loss at y = {loss_at_y}\")\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) = {grad_y}\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.grad(lambda x:loss(mL(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) = {grad_x}\")\n",
    "    \n",
    "    # The backprop method does the same\n",
    "    _ = mL(x) # Make sure the last x is in the right place\n",
    "    in_delE = mL.backprop(grad_y)\n",
    "    print(f\"backprop({grad_y}) = {in_delE}\")\n",
    "    assert closenuf(in_delE, grad_x)\n",
    "    \n",
    "    # The backprop operation did not change the behavior\n",
    "    assert np.equal(mL(x), y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test stacked-vectors input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    mL = MapLayer(lambda x:x**2, lambda d:2*d)\n",
    "    \n",
    "    two_wide_sq = np.array([[ 0,  1],\n",
    "                            [ 4,  9],\n",
    "                            [16, 25],\n",
    "                            [36, 49]])\n",
    "    # It applies the forward transformation\n",
    "    assert np.equal(mL(two_wide), two_wide_sq).all()\n",
    "    \n",
    "    # It back-propagages the loss gradient\n",
    "    x = two_wide\n",
    "    y = mL(x)\n",
    "\n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = two_wide * 2 + 11\n",
    "    #print(y - ideal)\n",
    "    #loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss = lambda v: np.einsum('ij,ij', v-ideal, v-ideal) / (2 * v.shape[0])\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x =\\n{x}\\ny =\\n{y}, loss = {loss_at_y}\\n\")\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.tensor_grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) = {grad_y}\\n\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.tensor_grad(lambda x:loss(mL(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) = {grad_x}\\n\")\n",
    "    \n",
    "    # The backprop method does the same\n",
    "    _ = mL(x) # Make sure the last x is in the right place\n",
    "    in_delE = mL.backprop(grad_y)\n",
    "    print(f\"backprop({grad_y}) =\\n{in_delE}\")\n",
    "    assert closenuf(in_delE, grad_x)\n",
    "    \n",
    "    # The backprop operation did not change the behavior\n",
    "    assert np.equal(mL(x), y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test single vector input behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for single input-vector operations:\n",
    "* input and output widths\n",
    "* state vector setting and getting\n",
    "* forward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    \n",
    "    # The input and output widths are correct\n",
    "    assert a(np.arange(2)).shape == (3,) \n",
    "\n",
    "    # Its internal state can be set\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    # and read back\n",
    "    assert (a.state_vector() == np.arange(9)).all()\n",
    "    # NOTE: The two assertions below are commented out because they depend\n",
    "    # on white-box knowledge, and are duplicative of other tests\n",
    "    #assert np.equal(a.M, np.array([[0, 1, 2],\n",
    "    #                               [3, 4, 5]])).all()\n",
    "    #assert np.equal(a.b, np.array([6, 7, 8])).all()\n",
    "\n",
    "    # Its internal state observed using numerical gradient is correct\n",
    "    x = np.random.rand(2)\n",
    "    y = a(x)\n",
    "    dydx = VC.grad(a, x)\n",
    "    b = y - x.dot(dydx)\n",
    "    #print(dydx, b)\n",
    "    #print(dydx, np.arange(6).reshape(2,-1))\n",
    "    assert closenuf(dydx, np.arange(6).reshape(2, -1))\n",
    "    #print(b, np.arange(6, 9))\n",
    "    assert closenuf(b, np.arange(6, 9))\n",
    "    \n",
    "    # It performs a single-input forward calculation correctly\n",
    "    x = np.array([2, 1])\n",
    "    y = a(x)\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\\n\")\n",
    "    assert (y == np.array([9, 13, 17])).all()\n",
    "    \n",
    "    # It performs a different single-input forward calculation correctly\n",
    "    a.set_state_from_vector(np.array([ 2,  3,  5,  7, 11, 13, 17, 19, 23]))\n",
    "    x = np.array([[29, 31]])\n",
    "    y = a(x)\n",
    "    assert (y == np.array([[292, 447, 571]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for single input-vector operations:\n",
    "* back-propagation of the loss gradient\n",
    "* learning (change in forward function) from the back-prop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "\n",
    "    # Doing a single-input-vector calculation\n",
    "    x = np.array([2, 1])\n",
    "    y = a(x)\n",
    "    assert np.equal(y, np.array([9, 13, 17])).all()\n",
    "\n",
    "    # It back-propagages the loss gradient\n",
    "    ideal = np.array([11,12,10])\n",
    "    loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x = {x}, y = {y}, loss = {loss_at_y}\")\n",
    "    grad_y = VC.grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) = {grad_y}\")\n",
    "    grad_x = VC.grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) = {grad_x}\")\n",
    "    \n",
    "    # Back-propagate the loss gradient from layer output to input\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.1 # Backprop one-tenth of the loss gradient\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    \n",
    "    # The loss gradient back-propagated to the layer input is correct\n",
    "    assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")\n",
    "    # FIXME: Check the change is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test batch operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* input and output widths\n",
    "* forward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    \n",
    "    # The input and output widths for the forward calculation are correct\n",
    "    x = two_wide\n",
    "    y = a(two_wide)\n",
    "    assert y.shape[0] == x.shape[0]\n",
    "    assert y.shape[1] == 3\n",
    "    \n",
    "    # The input and output widths for the backprop calculation are correct\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    assert bp.shape[0] == three_wide.shape[0]\n",
    "    assert bp.shape[1] == x.shape[1]\n",
    "\n",
    "    # The forward calculation is correct (in at least two instances)\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    assert (a(x) == np.array([[ 9, 11, 13],\n",
    "                              [15, 21, 27],\n",
    "                              [21, 31, 41],\n",
    "                              [27, 41, 55]])).all()\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "    a.set_state_from_vector(np.array([ 2,  3,  5,  7, 11, 13, 17, 19, 23]))\n",
    "    y = a(x)\n",
    "    #print(f\"x is: {x}\\ny is: {y}\")\n",
    "    assert (y == np.array([[ 24,  30,  36],\n",
    "                           [ 42,  58,  72],\n",
    "                           [ 60,  86, 108],\n",
    "                           [ 78, 114, 144]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* back-propagation of the loss gradient\n",
    "* learning (change in forward function) from the back-prop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    y = a(x)\n",
    "\n",
    "    # It back-propagages the loss gradient\n",
    "\n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = x @ arangep(2*3).reshape(2,3) + arangep(3,6) # A known, different parameter setting\n",
    "    print(f\"y - ideal =\\n{y - ideal}\")\n",
    "    #loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss = lambda v: np.einsum('ij,ij', v-ideal, v-ideal) / (2 * v.shape[0])\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x =\\n{x}\\nideal =\\n{ideal}\\ny =\\n{y}, loss = {loss_at_y}\\n\")\n",
    "\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.tensor_grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) =\\n{grad_y}\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.tensor_grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) =\\n{grad_x}\")\n",
    "            \n",
    "    # Back-propagate the loss gradient from layer output to input\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.01 # Backprop one percent of the loss gradient\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    \n",
    "    # The loss gradient back-propagated to the layer input is correct\n",
    "    #assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")\n",
    "    # FIXME: Check the change is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test batch operations when the affine layer has only one input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* input and output widths\n",
    "* forward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(1,3)\n",
    "    a.set_state_from_vector(np.arange(6))\n",
    "    \n",
    "    # The input and output widths for the forward calculation are correct\n",
    "    x = one_wide\n",
    "    y = a(one_wide)\n",
    "    assert y.shape[0] == x.shape[0]\n",
    "    assert y.shape[1] == 3\n",
    "    \n",
    "    # The input and output widths for the backprop calculation are correct\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    assert bp.shape[0] == three_wide.shape[0]\n",
    "    assert bp.shape[1] == x.shape[1]\n",
    "\n",
    "    # The forward calculation is correct (in at least two instances)\n",
    "    a.set_state_from_vector(np.arange(6))\n",
    "    x = np.array([[0],\n",
    "                  [1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "    assert (y == np.array([[ 3.,  4.,  5.],\n",
    "                           [ 3.,  5.,  7.],\n",
    "                           [ 3.,  6.,  9.],\n",
    "                           [ 3.,  7., 11.]])).all()\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "    a.set_state_from_vector(np.array([ 2,  3,  5,  7, 11, 13]))\n",
    "    y = a(x)\n",
    "    #print(f\"x is: {x}\\ny is: {y}\")\n",
    "    assert (a(x) == np.array([[ 7, 11, 13],\n",
    "                              [ 9, 14, 18],\n",
    "                              [11, 17, 23],\n",
    "                              [13, 20, 28]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* back-propagation of the loss gradient\n",
    "* learning (change in forward function) from the back-prop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(1,3)\n",
    "    a.set_state_from_vector(np.arange(6))\n",
    "    x = np.array([[0],\n",
    "                  [1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "    y = a(x)\n",
    "    #print(f\"x =\\n{x}\\ny =\\n{y}\")\n",
    "    \n",
    "    # It back-propagages the loss gradient\n",
    "\n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = x @ arangep(1*3).reshape(1,3) + arangep(3,6) # A known, different parameter setting\n",
    "    print(f\"y - ideal =\\n{y - ideal}\")\n",
    "    #loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss = lambda v: np.einsum('ij,ij', v-ideal, v-ideal) / (2 * v.shape[0])\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x =\\n{x}\\nideal =\\n{ideal}\\ny =\\n{y}, loss = {loss_at_y}\\n\")\n",
    "\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.tensor_grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) =\\n{grad_y}\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.tensor_grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) =\\n{grad_x}\")\n",
    "            \n",
    "    # Back-propagate the loss gradient from layer output to input\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.01 # Backprop one percent of the loss gradient\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    \n",
    "    # The loss gradient back-propagated to the layer input is correct\n",
    "    #assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")\n",
    "    # FIXME: Check the change is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest, the empty network, does identity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    net = Network()\n",
    "    assert all(x == net(x) for x in [0, 42, 'cows in trouble'])\n",
    "    assert all((x == net(x)).all() for x in [np.arange(7), np.arange(3*4*5).reshape(3,4,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stack of maps composes the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    net = Network()\n",
    "    net.extend(MapLayer(lambda x: x**3, lambda d: 3*d**2))\n",
    "    assert all(net(x) == x**3 for x in [0, 42, -3.14])\n",
    "    net.extend(MapLayer(lambda x: 7-x, lambda d: -1))\n",
    "    assert all(net(x) == 7-x**3 for x in [0, 42, -3.14])\n",
    "    \n",
    "    # It operates on each element of an input vector separately\n",
    "    assert (net(np.arange(4)) == 7 - np.arange(4) ** 3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A composition of affine transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_[to do someday]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test simple batch learning of a single affine layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from pprint import pprint\n",
    "    \n",
    "    net = Network()\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(np.arange(9)) # A well-known initial state\n",
    "    net.extend(a)\n",
    "    print(f\"\\nNet has state {net.state_vector()}\")\n",
    "\n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "\n",
    "    # The net wraps the layer\n",
    "    y = a(x)\n",
    "    assert (net(x) == y).all()\n",
    "    \n",
    "    # Make the training batch.\n",
    "    # We use a separate affine layer, initialized differently, to determine the ideal\n",
    "    t_a = AffineLayer(2,3)\n",
    "    t_a.set_state_from_vector(arangep(9)) # A known different initial state (of primes)\n",
    "    ideal = t_a(x)\n",
    "    \n",
    "    fact = (x, ideal)\n",
    "    print(f\"fact is:\")\n",
    "    pprint(fact, indent=1)\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    net.eta = 0.01\n",
    "    for i in range(10):\n",
    "        print(f\"net.learn([fact]) = {net.learn([fact])}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    # A simple fact yielder. Since it delivers multiple facts in succession,\n",
    "    # it is a \"facts\", aka \"batch cluster\"\n",
    "    def fact_ory(fact, n):\n",
    "        for i in range(n):\n",
    "            yield fact\n",
    "    \n",
    "    def facts_printer(facts):\n",
    "        # after Network.learn\n",
    "        for fact in facts:\n",
    "            print(f\"fact: \")\n",
    "            pprint(fact)\n",
    "            x, ideal = fact\n",
    "            print(f\"from which we get:\\n\\tx={x}\\n\\tideal={ideal}\\n\")\n",
    "    \n",
    "    #facts_printer(fact_ory(fact, 5))\n",
    "    \n",
    "    #print(f\"list(fact_ory(facts[0], 3)) =\\n{list(fact_ory(facts[0], 3))}\\n\")\n",
    "    print(f\"net.learn(fact_ory(fact,10)) = {net.learn(fact_ory(fact,10))}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    for i in range(1000):\n",
    "        loss = net.learn(fact_ory(fact,10))\n",
    "        if loss < 1e-7:\n",
    "            break\n",
    "    print(f\"did {(i+1)*10} more learnings of fact. Now loss is {loss}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    print(f\"net.state_vector() = {net.state_vector()}\")\n",
    "    \n",
    "    # The network has learned the target transform\n",
    "    assert closenuf(net(x), fact[1])\n",
    "    \n",
    "if False: # This section of the test is misconceived. Skip it\n",
    "    # Save prior results and learn again, with different batch clustering\n",
    "    prev_run_loss = loss\n",
    "    prev_y = net(x)\n",
    "    net.set_state_from_vector(np.arange(9)) # A well-known initial state\n",
    "    print(f\"\\nReset net to state {net.state_vector()}\")\n",
    "\n",
    "    # Try multiple batches in each call to Network.learn\n",
    "    def multibatch_fact_ory(fact, n):\n",
    "        for i in range(n//2):\n",
    "            yield fact, fact\n",
    "    facts_printer(multibatch_fact_ory(fact, 5))\n",
    "\n",
    "    for i in range(1000):\n",
    "        loss = net.learn(multibatch_fact_ory(fact,10))\n",
    "        if loss < 1e-7:\n",
    "            break\n",
    "    print(f\"did {(i+1)*10} learnings of fact. Now loss is {loss}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    # The results should match exactly\n",
    "    assert loss == prev_run_loss\n",
    "    assert (net(x) == prev_y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Network.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Make a network. Leave it with the default identity behavior.\n",
    "    net = Network()\n",
    "    \n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    ideal = net(x)\n",
    "    facts = [(x, ideal), (x, ideal-np.array([1,-1])), (x, 2*x)]\n",
    "    assert (net.losses(facts) == [0, 1, 17.5])\n",
    "\n",
    "    # Add some layers\n",
    "    net.extend(AffineLayer(2,3)).extend(MapLayer(np.sin, np.cos)).extend(AffineLayer(3,2))\n",
    "    # Place it in a known state for test repeatability\n",
    "    net.set_state_from_vector(np.arange(len(net.state_vector())))\n",
    "    ideal = net(x)\n",
    "    facts = [(x, ideal), (x, ideal-np.array([1,-1]))]\n",
    "    #print(net.losses(facts))\n",
    "    assert (net.losses(facts) == [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce an importable `nn.py`:\n",
    "1. Save this notebook\n",
    "1. Uncomment the `jupyter nbconvert` line below\n",
    "1. Execute it.\n",
    "1. Comment out the convert again\n",
    "1. Save the notebook again in that form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook nn.ipynb to script\n",
      "[NbConvertApp] Writing 27145 bytes to nn.py\n"
     ]
    }
   ],
   "source": [
    "###!jupyter nbconvert --to script nn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
