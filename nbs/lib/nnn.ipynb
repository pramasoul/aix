{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets multiple\n",
    "Version 0.1, in `nnn`\n",
    "\n",
    "# Note:\n",
    "This is the source for `nnn.py`, the foundation library of these neural net experiments. It contains a simple library, and a quantity, never sufficient, of test code, guarded by `if __name__ = '__main__'`. It makes various noises as it runs, but should not fail any asserts. See the bottom of the file for the procedure to produce the importable library file `nnn.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should do [Working efficiently with jupyter lab](https://florianwilhelm.info/2018/11/working_efficiently_with_jupyter_lab/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import MutableSequence\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "from types import FunctionType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of components which:\n",
    "1. accept an ordered set of reals (we'll use `numpy.array`, and  call them vectors) at the input port and produce another at the output port - this is forward propagation. ${\\displaystyle f\\colon \\mathbf {R} ^{n}\\to \\mathbf {R} ^{m}}$\n",
    "1. accept an ordered set of reals at the output port, representing the gradient of the loss function at the output, and produce the gradient of the loss function at the input port - this is back propagation, aka backprop. ${\\displaystyle b\\colon \\mathbf {R} ^{m}\\to \\mathbf {R} ^{n}}$\n",
    "1. from the gradient of the loss function at the output, calculate the partial of the loss function w.r.t the internal parameters ${\\displaystyle \\frac{\\partial E}{\\partial w} }$\n",
    "1. accept a scalar $\\eta$ to control the adjustment of internal parameters. _Or is this effected by scaling the loss gradient before passing? YES_\n",
    "1. update internal parameters ${\\displaystyle w \\leftarrow w - \\eta \\frac{\\partial E}{\\partial w} }$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(MutableSequence):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Compute response to input\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backprop(self, output_delE):\n",
    "        \"\"\"Use output error gradient to adjust internal parameters, return gradient of error at input\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def state_vector(self):\n",
    "        \"\"\"Provide the layer's learnable state as a vector\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # __delitem__, __getitem__, __len__, __setitem__, insert\n",
    "    \n",
    "    def __delitem__(self, ix):\n",
    "        raise NotImplementedError\n",
    "     \n",
    "    def __getitem__(self, ix):\n",
    "        raise NotImplementedError\n",
    "     \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "     \n",
    "    def __setitem__(self, ix, value):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def insert(self, ix, value):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network built of a cascade of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    version_string = '0.6'\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.eta = 0.1 #FIXME\n",
    "        \n",
    "    def extend(self, net):\n",
    "        self.layers.append(net)\n",
    "        return self\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        v = input\n",
    "        for net in self.layers:\n",
    "            v = net(v)\n",
    "        return v\n",
    "    \n",
    "    def learn(self, facts, eta=None):\n",
    "        eta = eta or self.eta\n",
    "        for x, ideal in facts:\n",
    "            y = self(x)\n",
    "            e = y - ideal\n",
    "            egrad = e * eta / np.atleast_2d(e).shape[-2] #TESTME: normalization\n",
    "            for net in reversed(self.layers):\n",
    "                egrad = net.backprop(egrad)\n",
    "        #loss = float(e.dot(e.T))/2.0\n",
    "        loss = np.einsum('...ij,...ij', e, e) / (2.0 * np.atleast_2d(e).shape[-2])\n",
    "        self.eta = eta\n",
    "        return loss\n",
    "\n",
    "    def losses(self, facts):\n",
    "        return [np.einsum('...ij,...ij', e, e) / (2.0 * np.atleast_2d(e).shape[-2]) \\\n",
    "                for e in (self(x) - ideal for x, ideal in facts)]\n",
    "        \n",
    "    def state_vector(self):\n",
    "        \"\"\"Provide the network's learnable state as a vector\"\"\"\n",
    "        return np.concatenate([layer.state_vector() for layer in self.layers])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            lsvlen = len(layer.state_vector())\n",
    "            layer.set_state_from_vector(sv[i:i+lsvlen])\n",
    "            i += lsvlen\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(Layer):\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        return output_delE\n",
    "\n",
    "    def state_vector(self):\n",
    "        return np.array([])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine\n",
    "A layer that does an [affine transformation](https://mathworld.wolfram.com/AffineTransformation.html) aka affinity, which is the classic fully-connected layer with output offsets.\n",
    "\n",
    "$$ \\mathbf{M} \\mathbf{x} + \\mathbf{b} = \\mathbf{y} $$\n",
    "where\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{j=1}^{n} x_j \\mathbf{\\hat{x}}_j \\\\\n",
    "\\mathbf{b} = \\sum_{i=1}^{m} b_i \\mathbf{\\hat{y}}_i \\\\\n",
    "\\mathbf{y} = \\sum_{i=1}^{m} y_i \\mathbf{\\hat{y}}_i\n",
    "$$\n",
    "and $\\mathbf{M}$ can be written\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    m_{1,1} & \\dots & m_{1,n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    m_{m,1} & \\dots & m_{m,n}\n",
    "\\end{bmatrix} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error gradient back-propagation\n",
    "$$ \n",
    "\\begin{align}\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{x}}\n",
    "  &= \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{x}} \\\\\n",
    "  &= \\mathbf{M}^\\mathsf{T}\\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter adjustment\n",
    "$$\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{M}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\mathbf{x} \\\\\n",
    " \\frac{\\partial loss}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}} \\frac{\\partial\\mathbf{y}}{\\partial\\mathbf{b}}\n",
    " = \\frac{\\partial loss}{\\partial\\mathbf{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapting to `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `numpy` it is more convenient to use row vectors, particularly for calculating the transform on multiple inputs in one operation. We use the identity $ \\mathbf{M} \\mathbf{x} = (\\mathbf{x} \\mathbf{M}^\\mathsf{T})^\\mathsf{T}.$ To avoid cluttering names, we will use `M` in the code below to hold $\\mathbf{M}^\\mathsf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineLayer(Layer):\n",
    "    \"\"\"An affine transformation, which is the classic fully-connected layer with offsets.\n",
    "    \n",
    "    The layer has n inputs and m outputs, which numbers must be supplied\n",
    "    upon creation. The inputs and outputs are marshalled in numpy arrays, 1-D\n",
    "    in the case of a single calculation, and 2-D when calculating the outputs\n",
    "    of multiple inputs in one call.\n",
    "    If called with 1-D array having shape == (n,), e.g numpy.arange(n), it will\n",
    "    return a 1-D numpy array of shape (m,).\n",
    "    If called with a 2-D numpy array, input shall have shape (k,n) and will return\n",
    "    a 2-D numpy array of shape (k,m), suitable as input to a subsequent layer\n",
    "    that has input width m.\n",
    "    \"\"\"\n",
    "    def __init__(self, n, m, multiples=1):\n",
    "        self.M = np.empty((multiples, n, m))\n",
    "        self.b = np.empty((multiples, 1, m))\n",
    "        self.randomize()\n",
    "        \n",
    "    def randomize(self):\n",
    "        self.M[:] = np.random.randn(*self.M.shape)\n",
    "        self.b[:] = np.random.randn(*self.b.shape)\n",
    "        \n",
    "    def __call__(self, x):#input[a,b,c] is net a, batch member b, dimension c\n",
    "        self.input = x\n",
    "        self.output = x @ self.M + self.b\n",
    "        return self._squeezeit(self.output, ref=self.input)\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = output_delE @ self.M.transpose(0,2,1)\n",
    "        self.M -= np.einsum('...ki,...kj->...ji', np.atleast_2d(output_delE), np.atleast_2d(self.input))\n",
    "        self.b -= np.sum(np.atleast_2d(output_delE), 0)\n",
    "        #return input_delE.squeeze(tuple(range(input_delE.ndim - output_delE.ndim)))\n",
    "        return self._squeezeit(input_delE, ref=output_delE)\n",
    "\n",
    "    def state_vector(self):\n",
    "        return np.concatenate((self.M.ravel(), self.b.ravel()))\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        \"\"\"Set the layer's learnable state from a vector\"\"\"\n",
    "        l_M = len(self.M.ravel())\n",
    "        l_b = len(self.b.ravel())\n",
    "        self.M[:] = sv[:l_M].reshape(self.M.shape)\n",
    "        self.b[:] = sv[l_M : l_M + l_b].reshape(self.b.shape)\n",
    "        \n",
    "    def _squeezeit(self, v, ref):\n",
    "        if self.M.shape[0] == 1:\n",
    "            return v.squeeze(tuple(range(v.ndim - ref.ndim)))\n",
    "        return v\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        rv = self.__class__(*self.M.shape[-2:])\n",
    "        rv.M = self.M[ix].reshape((1,) + self.M[ix].shape)\n",
    "        rv.b = self.b[ix].reshape((1,) + self.b[ix].shape)\n",
    "        return rv\n",
    "    \n",
    "    def __setitem__(self, ix, a):\n",
    "        if not np.shares_memory(self.M[ix], a.M):\n",
    "            self.M[ix,:] = a.M\n",
    "        if not np.shares_memory(self.b[ix], a.b):\n",
    "            self.b[ix,:] = a.b\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.M.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "Maps a scalar function on the inputs, for e.g. activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class was_MapLayer(Layer):\n",
    "    \"\"\"Map a scalar function on the input taken element-wise\"\"\"\n",
    "    def __init__(self, fun, dfundx):\n",
    "        self.vfun = np.vectorize(fun)\n",
    "        self.vdfundx = np.vectorize(dfundx)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        return self.vfun(x)\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        input_delE = self.vdfundx(self.input) * output_delE\n",
    "        return input_delE\n",
    "\n",
    "    def state_vector(self):\n",
    "        return np.array([])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapLayer(Layer):\n",
    "    \"\"\"Map a scalar function on the input taken element-wise\"\"\"\n",
    "    def __init__(self, f, df):\n",
    "        if hasattr(f, '__call__') and hasattr(df, '__call__'):\n",
    "            self.f, self.df = np.vectorize(f), np.vectorize(df)\n",
    "            self.single = True\n",
    "        elif hasattr(f, '__getitem__') and hasattr(f[0], '__call__') and \\\n",
    "                hasattr(df, '__getitem__') and hasattr(df[0], '__call__'):\n",
    "            self.f = [np.vectorize(fun) for fun in f]\n",
    "            self.df = [np.vectorize(fun) for fun in df]\n",
    "            self.single = False\n",
    "        else:\n",
    "            raise ValueError(\"MapLayer requires a function or sequence of functions\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        if self.single:\n",
    "            return self.f(x)\n",
    "        return np.array([f(v) for f,v in zip(cycle(self.f),x)])\n",
    "    \n",
    "    def backprop(self, output_delE):\n",
    "        if self.single:\n",
    "            return self.df(self.input) * output_delE\n",
    "        return np.array([df(v) for df,v in zip(cycle(self.df), output_delE)])\n",
    "        \n",
    "    def state_vector(self):\n",
    "        return np.array([])\n",
    "    \n",
    "    def set_state_from_vector(self, sv):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "*Incomplete* \\\n",
    "Also `unittest` the `.py` version with a separate test script, see `test-nn_v3.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a few test arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_wide is:\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "two_wide is:\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "three_wide is:\n",
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    one_wide = np.atleast_2d(np.arange(1*4)).reshape(-1,1)\n",
    "    print(f\"one_wide is:\\n{one_wide}\")\n",
    "    two_wide = np.arange(2*4).reshape(-1,2)\n",
    "    print(f\"two_wide is:\\n{two_wide}\")\n",
    "    three_wide = np.arange(3*4).reshape(-1,3)\n",
    "    print(f\"three_wide is:\\n{three_wide}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tooling for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import sympy\n",
    "    \n",
    "    class VC():\n",
    "        def grad(f, x, eps=1e-6):\n",
    "            epsihat = np.eye(x.shape[-1]) * eps\n",
    "            yp = np.apply_along_axis(f, 1, x + epsihat)\n",
    "            ym = np.apply_along_axis(f, 1, x - epsihat)\n",
    "            return (yp - ym)/(2 * eps)\n",
    "        \n",
    "        def tensor_grad(f, x, eps=1e-6):\n",
    "            return np.apply_along_axis(lambda v: VC.grad(f, v, eps), 1, x)\n",
    "            \n",
    "    def closenuf(a, b, tol=0.001):\n",
    "        return np.allclose(a, b, rtol=tol)\n",
    "    \n",
    "    def arangep(n, starting_index=0):\n",
    "        sympy.sieve.extend_to_no(starting_index + n)\n",
    "        return np.array(sympy.sieve._list[starting_index:starting_index + n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  0.,  0.],\n",
       "       [ 0.,  8.,  0.],\n",
       "       [ 0.,  0., 10.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VC.grad(lambda x:x**2, three_wide[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [-1.,  0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VC.grad(lambda x: x @ np.array([[0,1], [-1, 0]]), two_wide[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  2.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  4.        ]],\n",
       "\n",
       "       [[ 6.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  8.        ,  0.        ],\n",
       "        [ 0.        ,  0.        , 10.        ]],\n",
       "\n",
       "       [[12.        ,  0.        ,  0.        ],\n",
       "        [ 0.        , 14.        ,  0.        ],\n",
       "        [ 0.        ,  0.        , 15.99999999]],\n",
       "\n",
       "       [[17.99999999,  0.        ,  0.        ],\n",
       "        [ 0.        , 19.99999999,  0.        ],\n",
       "        [ 0.        ,  0.        , 21.99999998]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VC.tensor_grad(lambda x:x**2, three_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input to a layer can be a single (row) vector, or a vertical stack of row vectors,\n",
    "a 2-d array that resembles a matrix. We need to test each layer class with both single and stacked input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    iL = IdentityLayer()\n",
    "    \n",
    "    # It's transparent from input to output\n",
    "    assert np.equal(iL(np.arange(5)), np.arange(5)).all()\n",
    "    assert (iL(three_wide) == three_wide).all()\n",
    "    \n",
    "    # It back-propagates the loss gradient without alteration\n",
    "    assert np.equal(iL.backprop(np.arange(7)), np.arange(7)).all()\n",
    "    assert (iL.backprop(three_wide) == three_wide).all()\n",
    "\n",
    "    # It works for stacked input\n",
    "    # (see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test single vector input behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1 2 2], y = [1 4 4], loss at y = 1.5\n",
      "∇𝑙𝑜𝑠𝑠(𝑦) = [-1.  1. -1.]\n",
      "∇𝑙𝑜𝑠𝑠(𝑥) = [-2.  4. -4.]\n",
      "backprop([-1.  1. -1.]) = [-2.  4. -4.]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    mL = MapLayer(lambda x:x**2, lambda d:2*d)\n",
    "    \n",
    "    # It applies the forward transformation\n",
    "    assert np.equal(mL(np.array([-2,1,3])), np.array([4,1,9])).all()\n",
    "    \n",
    "    # It back-propagages the loss gradient\n",
    "    x = np.array([1,2,2])\n",
    "    y = mL(x)\n",
    "    \n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = np.array([2,3,5])\n",
    "    loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x = {x}, y = {y}, loss at y = {loss_at_y}\")\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) = {grad_y}\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.grad(lambda x:loss(mL(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) = {grad_x}\")\n",
    "    \n",
    "    # The backprop method does the same\n",
    "    _ = mL(x) # Make sure the last x is in the right place\n",
    "    in_delE = mL.backprop(grad_y)\n",
    "    print(f\"backprop({grad_y}) = {in_delE}\")\n",
    "    assert closenuf(in_delE, grad_x)\n",
    "    \n",
    "    # The backprop operation did not change the behavior\n",
    "    assert np.equal(mL(x), y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test stacked-vectors input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "y =\n",
      "[[ 0  1]\n",
      " [ 4  9]\n",
      " [16 25]\n",
      " [36 49]], loss = 152.5\n",
      "\n",
      "∇𝑙𝑜𝑠𝑠(𝑦) = [[-34.00000003 -36.00000002]\n",
      " [-26.00000002 -19.99999998]\n",
      " [ -1.99999999  12.00000001]\n",
      " [ 37.9999999   59.99999985]]\n",
      "\n",
      "∇𝑙𝑜𝑠𝑠(𝑥) = [[   0.          -71.99999999]\n",
      " [-103.99999996 -120.00000001]\n",
      " [ -16.          120.00000002]\n",
      " [ 456.0000001   840.00000027]]\n",
      "\n",
      "backprop([[-34.00000003 -36.00000002]\n",
      " [-26.00000002 -19.99999998]\n",
      " [ -1.99999999  12.00000001]\n",
      " [ 37.9999999   59.99999985]]) =\n",
      "[[  -0.          -72.00000005]\n",
      " [-104.00000008 -119.99999987]\n",
      " [ -15.99999996  120.00000012]\n",
      " [ 455.99999885  839.99999788]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    mL = MapLayer(lambda x:x**2, lambda d:2*d)\n",
    "    \n",
    "    two_wide_sq = np.array([[ 0,  1],\n",
    "                            [ 4,  9],\n",
    "                            [16, 25],\n",
    "                            [36, 49]])\n",
    "    # It applies the forward transformation\n",
    "    assert np.equal(mL(two_wide), two_wide_sq).all()\n",
    "    \n",
    "    # It back-propagages the loss gradient\n",
    "    x = two_wide\n",
    "    y = mL(x)\n",
    "\n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = two_wide * 2 + 11\n",
    "    #print(y - ideal)\n",
    "    #loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss = lambda v: np.einsum('ij,ij', v-ideal, v-ideal) / (2 * v.shape[0])\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x =\\n{x}\\ny =\\n{y}, loss = {loss_at_y}\\n\")\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.tensor_grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) = {grad_y}\\n\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.tensor_grad(lambda x:loss(mL(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) = {grad_x}\\n\")\n",
    "    \n",
    "    # The backprop method does the same\n",
    "    _ = mL(x) # Make sure the last x is in the right place\n",
    "    in_delE = mL.backprop(grad_y)\n",
    "    print(f\"backprop({grad_y}) =\\n{in_delE}\")\n",
    "    assert closenuf(in_delE, grad_x)\n",
    "    \n",
    "    # The backprop operation did not change the behavior\n",
    "    assert np.equal(mL(x), y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test stacked-vectors input with multiple functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # A list of lambdas is applied cyclically to the argument sequence\n",
    "    mL = MapLayer([lambda x,i=i:x**i for i in range(2, 4)],\n",
    "                  [lambda x,i=i:i*x**(i-1) for i in range(2, 4)])\n",
    "    \n",
    "    x = np.ones(5) * 10\n",
    "    y = np.array([100, 1000, 100, 1000, 100])\n",
    "    dy = np.array([20, 300, 20, 300, 20])\n",
    "    assert np.equal(mL(x), y).all()\n",
    "    assert np.equal(mL.backprop(x), dy).all()\n",
    "\n",
    "    # When applied to a multi, it cycles through application correctly\n",
    "    x = np.arange(2*3*4).reshape(2,3,4)\n",
    "    y = np.array(\n",
    "        [[[    0,     1,     4,     9],\n",
    "          [   16,    25,    36,    49],\n",
    "          [   64,    81,   100,   121]],\n",
    "\n",
    "         [[ 1728,  2197,  2744,  3375],\n",
    "          [ 4096,  4913,  5832,  6859],\n",
    "          [ 8000,  9261, 10648, 12167]]])\n",
    "    assert np.equal(mL(x), y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test single vector input behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for single input-vector operations:\n",
    "* input and output widths\n",
    "* state vector setting and getting\n",
    "* forward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    \n",
    "    # The input and output widths are correct\n",
    "    assert a(np.arange(2)).shape == (3,) \n",
    "\n",
    "    # Its internal state can be set\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    # and read back\n",
    "    assert (a.state_vector() == np.arange(9)).all()\n",
    "    # NOTE: The two assertions below are commented out because they depend\n",
    "    # on white-box knowledge, and are duplicative of other tests\n",
    "    #assert np.equal(a.M, np.array([[0, 1, 2],\n",
    "    #                               [3, 4, 5]])).all()\n",
    "    #assert np.equal(a.b, np.array([6, 7, 8])).all()\n",
    "\n",
    "    # Its internal state observed using numerical gradient is correct\n",
    "    x = np.random.rand(2)\n",
    "    y = a(x)\n",
    "    dydx = VC.grad(a, x)\n",
    "    b = y - x.dot(dydx)\n",
    "    #print(dydx, b)\n",
    "    #print(dydx, np.arange(6).reshape(2,-1))\n",
    "    assert closenuf(dydx, np.arange(6).reshape(2, -1))\n",
    "    #print(b, np.arange(6, 9))\n",
    "    assert closenuf(b, np.arange(6, 9))\n",
    "    \n",
    "    # It performs a single-input forward calculation correctly\n",
    "    x = np.array([2, 1])\n",
    "    y = a(x)\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\\n\")\n",
    "    assert (y == np.array([9, 13, 17])).all()\n",
    "    \n",
    "    # It performs a different single-input forward calculation correctly\n",
    "    a.set_state_from_vector(np.array([ 2,  3,  5,  7, 11, 13, 17, 19, 23]))\n",
    "    x = np.array([[29, 31]])\n",
    "    y = a(x)\n",
    "    assert (y == np.array([[292, 447, 571]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for single input-vector operations:\n",
    "* back-propagation of the loss gradient\n",
    "* learning (change in forward function) from the back-prop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [2 1], y = [ 9. 13. 17.], loss = 27.0\n",
      "∇𝑙𝑜𝑠𝑠(𝑦) = [-2.          1.          7.00000001]\n",
      "∇𝑙𝑜𝑠𝑠(𝑥) = [15.00000002 33.00000001]\n",
      "backprop([-0.2  0.1  0.7]) = [1.5 3.3]\n",
      "Now a([2 1]) = [10.2 12.4 12.8], loss = 4.319999988809314\n",
      "state_vector is [0.4 0.8 0.6 3.2 3.9 4.3 6.2 6.9 7.3]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "\n",
    "    # Doing a single-input-vector calculation\n",
    "    x = np.array([2, 1])\n",
    "    y = a(x)\n",
    "    assert np.equal(y, np.array([9, 13, 17])).all()\n",
    "\n",
    "    # It back-propagages the loss gradient\n",
    "    ideal = np.array([11,12,10])\n",
    "    loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x = {x}, y = {y}, loss = {loss_at_y}\")\n",
    "    grad_y = VC.grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) = {grad_y}\")\n",
    "    grad_x = VC.grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) = {grad_x}\")\n",
    "    \n",
    "    # Back-propagate the loss gradient from layer output to input\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.1 # Backprop one-tenth of the loss gradient\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    \n",
    "    # The loss gradient back-propagated to the layer input is correct\n",
    "    assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")\n",
    "    # FIXME: Check the change is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test batch operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* input and output widths\n",
    "* forward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    \n",
    "    # The input and output widths for the forward calculation are correct\n",
    "    x = two_wide\n",
    "    y = a(two_wide)\n",
    "    assert y.shape[0] == x.shape[0]\n",
    "    assert y.shape[1] == 3\n",
    "    \n",
    "    # The input and output widths for the backprop calculation are correct\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    assert bp.shape[0] == three_wide.shape[0]\n",
    "    assert bp.shape[1] == x.shape[1]\n",
    "\n",
    "    # The forward calculation is correct (in at least two instances)\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    assert (a(x) == np.array([[ 9, 11, 13],\n",
    "                              [15, 21, 27],\n",
    "                              [21, 31, 41],\n",
    "                              [27, 41, 55]])).all()\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "    a.set_state_from_vector(np.array([ 2,  3,  5,  7, 11, 13, 17, 19, 23]))\n",
    "    y = a(x)\n",
    "    #print(f\"x is: {x}\\ny is: {y}\")\n",
    "    assert (y == np.array([[ 24,  30,  36],\n",
    "                           [ 42,  58,  72],\n",
    "                           [ 60,  86, 108],\n",
    "                           [ 78, 114, 144]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* back-propagation of the loss gradient\n",
    "* learning (change in forward function) from the back-prop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y - ideal =\n",
      "[[-15. -19. -23.]\n",
      " [-27. -37. -45.]\n",
      " [-39. -55. -67.]\n",
      " [-51. -73. -89.]]\n",
      "x =\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "ideal =\n",
      "[[ 24  30  36]\n",
      " [ 42  58  72]\n",
      " [ 60  86 108]\n",
      " [ 78 114 144]]\n",
      "y =\n",
      "[[ 9. 11. 13.]\n",
      " [15. 21. 27.]\n",
      " [21. 31. 41.]\n",
      " [27. 41. 55.]], loss = 3765.5\n",
      "\n",
      "∇𝑙𝑜𝑠𝑠(𝑦) =\n",
      "[[ -55.99999895  -81.33333358 -102.66666595]\n",
      " [ -47.99999988  -67.99999983  -83.99999933]\n",
      " [ -40.00000035  -54.66666653  -65.33333317]\n",
      " [ -31.99999992  -41.33333346  -46.66666655]]\n",
      "∇𝑙𝑜𝑠𝑠(𝑥) =\n",
      "[[ -286.6666664  -1006.6666664 ]\n",
      " [ -235.9999994   -836.00000062]\n",
      " [ -185.33333332  -665.33333393]\n",
      " [ -134.66666633  -494.66666701]]\n",
      "backprop([[-0.55999999 -0.81333334 -1.02666666]\n",
      " [-0.48       -0.68       -0.83999999]\n",
      " [-0.4        -0.54666667 -0.65333333]\n",
      " [-0.32       -0.41333333 -0.46666667]]) = [[ -2.86666665 -10.06666661]\n",
      " [ -2.35999998  -8.35999996]\n",
      " [ -1.85333333  -6.65333333]\n",
      " [ -1.34666667  -4.94666666]]\n",
      "Now a([[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]) = [[ 16.99999999  21.93333333  26.06666661]\n",
      " [ 44.44        60.94666666  74.41333313]\n",
      " [ 71.88000001  99.95999999 122.75999966]\n",
      " [ 99.32000002 138.97333332 171.10666618]], loss = 325.0071954815595\n",
      "state_vector is [ 4.48000001  7.02666667  9.09333331  9.24       12.48       15.07999996\n",
      "  7.75999999  9.45333333 10.98666665]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(np.arange(9))\n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    y = a(x)\n",
    "\n",
    "    # It back-propagages the loss gradient\n",
    "\n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = x @ arangep(2*3).reshape(2,3) + arangep(3,6) # A known, different parameter setting\n",
    "    print(f\"y - ideal =\\n{y - ideal}\")\n",
    "    #loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss = lambda v: np.einsum('ij,ij', v-ideal, v-ideal) / (2 * v.shape[0])\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x =\\n{x}\\nideal =\\n{ideal}\\ny =\\n{y}, loss = {loss_at_y}\\n\")\n",
    "\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.tensor_grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) =\\n{grad_y}\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.tensor_grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) =\\n{grad_x}\")\n",
    "            \n",
    "    # Back-propagate the loss gradient from layer output to input\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.01 # Backprop one percent of the loss gradient\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    \n",
    "    # The loss gradient back-propagated to the layer input is correct\n",
    "    #assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")\n",
    "    # FIXME: Check the change is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test batch operations when the affine layer has only one input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* input and output widths\n",
    "* forward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(1,3)\n",
    "    a.set_state_from_vector(np.arange(6))\n",
    "    \n",
    "    # The input and output widths for the forward calculation are correct\n",
    "    x = one_wide\n",
    "    y = a(one_wide)\n",
    "    assert y.shape[0] == x.shape[0]\n",
    "    assert y.shape[1] == 3\n",
    "    \n",
    "    # The input and output widths for the backprop calculation are correct\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    assert bp.shape[0] == three_wide.shape[0]\n",
    "    assert bp.shape[1] == x.shape[1]\n",
    "\n",
    "    # The forward calculation is correct (in at least two instances)\n",
    "    a.set_state_from_vector(np.arange(6))\n",
    "    x = np.array([[0],\n",
    "                  [1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "    assert (y == np.array([[ 3.,  4.,  5.],\n",
    "                           [ 3.,  5.,  7.],\n",
    "                           [ 3.,  6.,  9.],\n",
    "                           [ 3.,  7., 11.]])).all()\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "    a.set_state_from_vector(np.array([ 2,  3,  5,  7, 11, 13]))\n",
    "    y = a(x)\n",
    "    #print(f\"x is: {x}\\ny is: {y}\")\n",
    "    assert (a(x) == np.array([[ 7, 11, 13],\n",
    "                              [ 9, 14, 18],\n",
    "                              [11, 17, 23],\n",
    "                              [13, 20, 28]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* back-propagation of the loss gradient\n",
    "* learning (change in forward function) from the back-prop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y - ideal =\n",
      "[[-14. -15. -18.]\n",
      " [-16. -17. -21.]\n",
      " [-18. -19. -24.]\n",
      " [-20. -21. -27.]]\n",
      "x =\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "ideal =\n",
      "[[17 19 23]\n",
      " [19 22 28]\n",
      " [21 25 33]\n",
      " [23 28 38]]\n",
      "y =\n",
      "[[ 3.  4.  5.]\n",
      " [ 3.  5.  7.]\n",
      " [ 3.  6.  9.]\n",
      " [ 3.  7. 11.]], loss = 570.25\n",
      "\n",
      "∇𝑙𝑜𝑠𝑠(𝑦) =\n",
      "[[-22.66666667 -26.00000005 -34.00000003]\n",
      " [-22.66666667 -24.66666677 -31.33333342]\n",
      " [-22.66666667 -23.33333344 -28.66666659]\n",
      " [-22.66666667 -22.         -25.99999993]]\n",
      "∇𝑙𝑜𝑠𝑠(𝑥) =\n",
      "[[-94.0000001 ]\n",
      " [-87.3333334 ]\n",
      " [-80.66666669]\n",
      " [-74.0000001 ]]\n",
      "backprop([[-0.22666667 -0.26       -0.34      ]\n",
      " [-0.22666667 -0.24666667 -0.31333333]\n",
      " [-0.22666667 -0.23333333 -0.28666667]\n",
      " [-0.22666667 -0.22       -0.26      ]]) = [[-0.94      ]\n",
      " [-0.87333334]\n",
      " [-0.80666667]\n",
      " [-0.74      ]]\n",
      "Now a([[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]) = [[ 3.90666667  4.96        6.2       ]\n",
      " [ 5.26666667  7.33333334  9.86666666]\n",
      " [ 6.62666667  9.70666668 13.53333333]\n",
      " [ 7.98666667 12.08000001 17.19999999]], loss = 389.28084440511896\n",
      "state_vector is [1.36       2.37333334 3.66666666 3.90666667 4.96       6.2       ]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(1,3)\n",
    "    a.set_state_from_vector(np.arange(6))\n",
    "    x = np.array([[0],\n",
    "                  [1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "    y = a(x)\n",
    "    #print(f\"x =\\n{x}\\ny =\\n{y}\")\n",
    "    \n",
    "    # It back-propagages the loss gradient\n",
    "\n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = x @ arangep(1*3).reshape(1,3) + arangep(3,6) # A known, different parameter setting\n",
    "    print(f\"y - ideal =\\n{y - ideal}\")\n",
    "    #loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss = lambda v: np.einsum('ij,ij', v-ideal, v-ideal) / (2 * v.shape[0])\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x =\\n{x}\\nideal =\\n{ideal}\\ny =\\n{y}, loss = {loss_at_y}\\n\")\n",
    "\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.tensor_grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) =\\n{grad_y}\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.tensor_grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) =\\n{grad_x}\")\n",
    "            \n",
    "    # Back-propagate the loss gradient from layer output to input\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.01 # Backprop one percent of the loss gradient\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    \n",
    "    # The loss gradient back-propagated to the layer input is correct\n",
    "    #assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")\n",
    "    # FIXME: Check the change is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test multinet batch operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* input and output widths\n",
    "* state vector setting and getting\n",
    "* forward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3, multiples=4)\n",
    "    a.set_state_from_vector(np.arange(9*4))\n",
    "    \n",
    "    # The input and output widths for the forward calculation are correct\n",
    "    x = two_wide\n",
    "    y = a(two_wide)\n",
    "    assert y.shape[-2] == x.shape[-2]\n",
    "    assert y.shape[-1] == 3\n",
    "    \n",
    "    # The input and output widths for the backprop calculation are correct\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    assert bp.shape[-2] == three_wide.shape[-2]\n",
    "    assert bp.shape[-1] == x.shape[-1]\n",
    "\n",
    "    # The forward calculation is correct (in at least two instances)\n",
    "    a.set_state_from_vector(np.arange(9*4))\n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    assert (a(x) == np.array(\n",
    "      [[[ 27.,  29.,  31.],\n",
    "        [ 33.,  39.,  45.],\n",
    "        [ 39.,  49.,  59.],\n",
    "        [ 45.,  59.,  73.]],\n",
    "\n",
    "       [[ 36.,  38.,  40.],\n",
    "        [ 66.,  72.,  78.],\n",
    "        [ 96., 106., 116.],\n",
    "        [126., 140., 154.]],\n",
    "\n",
    "       [[ 45.,  47.,  49.],\n",
    "        [ 99., 105., 111.],\n",
    "        [153., 163., 173.],\n",
    "        [207., 221., 235.]],\n",
    "\n",
    "       [[ 54.,  56.,  58.],\n",
    "        [132., 138., 144.],\n",
    "        [210., 220., 230.],\n",
    "        [288., 302., 316.]]])).all()\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "    a.set_state_from_vector(arangep(9*4))\n",
    "    y = a(x)\n",
    "    #print(f\"x is: {x}\\ny is: {y}\")\n",
    "    assert (y == np.array(\n",
    "      [[[ 104.,  112.,  116.],\n",
    "        [ 122.,  140.,  152.],\n",
    "        [ 140.,  168.,  188.],\n",
    "        [ 158.,  196.,  224.]],\n",
    "\n",
    "       [[ 136.,  140.,  150.],\n",
    "        [ 228.,  240.,  270.],\n",
    "        [ 320.,  340.,  390.],\n",
    "        [ 412.,  440.,  510.]],\n",
    "\n",
    "       [[ 180.,  190.,  198.],\n",
    "        [ 368.,  394.,  414.],\n",
    "        [ 556.,  598.,  630.],\n",
    "        [ 744.,  802.,  846.]],\n",
    "\n",
    "       [[ 218.,  232.,  240.],\n",
    "        [ 510.,  540.,  564.],\n",
    "        [ 802.,  848.,  888.],\n",
    "        [1094., 1156., 1212.]]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* back-propagation of the loss gradient\n",
    "* learning (change in forward function) from the back-prop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y - ideal =\n",
      "[[[ -33.  -37.  -41.]\n",
      "  [ -45.  -55.  -63.]\n",
      "  [ -57.  -73.  -85.]\n",
      "  [ -69.  -91. -107.]]\n",
      "\n",
      " [[ -58.  -64.  -70.]\n",
      "  [-120. -130. -152.]\n",
      "  [-182. -196. -234.]\n",
      "  [-244. -262. -316.]]]\n",
      "x =\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]]\n",
      "ideal =\n",
      "[[[ 48  54  60]\n",
      "  [ 66  82  96]\n",
      "  [ 84 110 132]\n",
      "  [102 138 168]]\n",
      "\n",
      " [[ 82  90  98]\n",
      "  [174 190 218]\n",
      "  [266 290 338]\n",
      "  [358 390 458]]]\n",
      "y =\n",
      "[[[ 15.  17.  19.]\n",
      "  [ 21.  27.  33.]\n",
      "  [ 27.  37.  47.]\n",
      "  [ 33.  47.  61.]]\n",
      "\n",
      " [[ 24.  26.  28.]\n",
      "  [ 54.  60.  66.]\n",
      "  [ 84.  94. 104.]\n",
      "  [114. 128. 142.]]], loss = [ 6681.5 52637. ]\n",
      "\n",
      "∇𝑙𝑜𝑠𝑠(𝑦) =\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]]]]\n",
      "∇𝑙𝑜𝑠𝑠(𝑥) =\n",
      "[[[0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (2,4,2,3)->(2,4,newaxis,newaxis) (2,3,2)->(2,newaxis,newaxis) and requested shape (2,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-30a9cdc56b2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Make sure the last x is in the right place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mout_delE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_y\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;31m# Backprop one percent of the loss gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0min_delE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_delE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"backprop({out_delE}) = {in_delE}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f7cd850dbfc0>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, output_delE)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_delE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0minput_delE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_delE\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'...ki,...kj->...ji'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_delE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_delE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (2,4,2,3)->(2,4,newaxis,newaxis) (2,3,2)->(2,newaxis,newaxis) and requested shape (2,2)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(2,3, multiples=2)\n",
    "    a.set_state_from_vector(np.arange(9*2))\n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    y = a(x)\n",
    "\n",
    "    # It back-propagages the loss gradient\n",
    "\n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = x @ arangep(2*2*3).reshape(2,2,3) + arangep(2*3, 2*2*3).reshape(2,1,3) # A known, different parameter setting\n",
    "    print(f\"y - ideal =\\n{y - ideal}\")\n",
    "    #loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss = lambda v: np.einsum('...ij,...ij', y-ideal, y-ideal) / (2 * np.atleast_2d(y-ideal).shape[-2])\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x =\\n{x}\\nideal =\\n{ideal}\\ny =\\n{y}, loss = {loss_at_y}\\n\")\n",
    "\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.tensor_grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) =\\n{grad_y}\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.tensor_grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) =\\n{grad_x}\")\n",
    "            \n",
    "    # Back-propagate the loss gradient from layer output to input\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.01 # Backprop one percent of the loss gradient\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    \n",
    "    # The loss gradient back-propagated to the layer input is correct\n",
    "    #assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")\n",
    "    # FIXME: Check the change is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-26-83d7fd585499>\u001b[0m(15)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     13 \u001b[0;31m    \u001b[0;31m# for loss function, use L2-distance from some ideal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m    \u001b[0;31m# (divided by 2, for convenient gradient = error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 15 \u001b[0;31m    \u001b[0mideal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0marangep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0marangep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# A known, different parameter setting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y - ideal =\\n{y - ideal}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m    \u001b[0;31m#loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  arangep(2*2*3, 2*(2*3 + 3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 67,  71,  73,  79,  83,  89,  97, 101, 103, 107, 109, 113])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  2*(2*3 + 3) - 2*2*3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  2*(2*3 + 3), 2*2*3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 12)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  range(7,2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(7, 2)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  list(range(7,2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Error in argument: '(range(7,2))'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test multinet batch operations when the affine layer has only one input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* input and output widths\n",
    "* forward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(1,3)\n",
    "    a.set_state_from_vector(np.arange(6))\n",
    "    \n",
    "    # The input and output widths for the forward calculation are correct\n",
    "    x = one_wide\n",
    "    y = a(one_wide)\n",
    "    assert y.shape[0] == x.shape[0]\n",
    "    assert y.shape[1] == 3\n",
    "    \n",
    "    # The input and output widths for the backprop calculation are correct\n",
    "    bp = a.backprop(three_wide * 0.001)\n",
    "    assert bp.shape[0] == three_wide.shape[0]\n",
    "    assert bp.shape[1] == x.shape[1]\n",
    "\n",
    "    # The forward calculation is correct (in at least two instances)\n",
    "    a.set_state_from_vector(np.arange(6))\n",
    "    x = np.array([[0],\n",
    "                  [1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "    assert (y == np.array([[ 3.,  4.,  5.],\n",
    "                           [ 3.,  5.,  7.],\n",
    "                           [ 3.,  6.,  9.],\n",
    "                           [ 3.,  7., 11.]])).all()\n",
    "    #print(f\"a.M is:\\n{a.M}\\na.b is {a.b}\\nx is: {x}\\ny is: {y}\")\n",
    "    a.set_state_from_vector(np.array([ 2,  3,  5,  7, 11, 13]))\n",
    "    y = a(x)\n",
    "    #print(f\"x is: {x}\\ny is: {y}\")\n",
    "    assert (a(x) == np.array([[ 7, 11, 13],\n",
    "                              [ 9, 14, 18],\n",
    "                              [11, 17, 23],\n",
    "                              [13, 20, 28]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test, for batch operations:\n",
    "* back-propagation of the loss gradient\n",
    "* learning (change in forward function) from the back-prop operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Affine\n",
    "    a = AffineLayer(1,3)\n",
    "    a.set_state_from_vector(np.arange(6))\n",
    "    x = np.array([[0],\n",
    "                  [1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "    y = a(x)\n",
    "    #print(f\"x =\\n{x}\\ny =\\n{y}\")\n",
    "    \n",
    "    # It back-propagages the loss gradient\n",
    "\n",
    "    # for loss function, use L2-distance from some ideal\n",
    "    # (divided by 2, for convenient gradient = error)\n",
    "    ideal = x @ arangep(1*3).reshape(1,3) + arangep(3,6) # A known, different parameter setting\n",
    "    print(f\"y - ideal =\\n{y - ideal}\")\n",
    "    #loss = lambda v: (v - ideal).dot(v - ideal) / 2.0\n",
    "    loss = lambda v: np.einsum('ij,ij', v-ideal, v-ideal) / (2 * v.shape[0])\n",
    "    loss_at_y = loss(y)\n",
    "    print(f\"x =\\n{x}\\nideal =\\n{ideal}\\ny =\\n{y}, loss = {loss_at_y}\\n\")\n",
    "\n",
    "    \n",
    "    # find numerical gradient of loss function at y, the layer output\n",
    "    grad_y = VC.tensor_grad(loss, y)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑦) =\\n{grad_y}\")\n",
    "    \n",
    "    # find the numerical gradient of the loss w.r.t. the input of the layer\n",
    "    grad_x = VC.tensor_grad(lambda x:loss(a(x)), x)\n",
    "    print(f\"∇𝑙𝑜𝑠𝑠(𝑥) =\\n{grad_x}\")\n",
    "            \n",
    "    # Back-propagate the loss gradient from layer output to input\n",
    "    _ = a(x) # Make sure the last x is in the right place\n",
    "    out_delE = grad_y * 0.01 # Backprop one percent of the loss gradient\n",
    "    in_delE = a.backprop(out_delE)\n",
    "    print(f\"backprop({out_delE}) = {in_delE}\")\n",
    "    \n",
    "    # The loss gradient back-propagated to the layer input is correct\n",
    "    #assert closenuf(in_delE / 0.1, grad_x)\n",
    "    \n",
    "    # And how did the learning affect the layer?\n",
    "    print(f\"Now a({x}) = {a(x)}, loss = {loss(a(x))}\")\n",
    "    print(f\"state_vector is {a.state_vector()}\")\n",
    "    # FIXME: Check the change is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest, the empty network, does identity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    net = Network()\n",
    "    assert all(x == net(x) for x in [0, 42, 'cows in trouble'])\n",
    "    assert all((x == net(x)).all() for x in [np.arange(7), np.arange(3*4*5).reshape(3,4,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stack of maps composes the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    net = Network()\n",
    "    net.extend(MapLayer(lambda x: x**3, lambda d: 3*d**2))\n",
    "    assert all(net(x) == x**3 for x in [0, 42, -3.14])\n",
    "    net.extend(MapLayer(lambda x: 7-x, lambda d: -1))\n",
    "    assert all(net(x) == 7-x**3 for x in [0, 42, -3.14])\n",
    "    \n",
    "    # It operates on each element of an input vector separately\n",
    "    assert (net(np.arange(4)) == 7 - np.arange(4) ** 3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A composition of affine transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_[to do someday]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test simple batch learning of a single affine layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from pprint import pprint\n",
    "    \n",
    "    net = Network()\n",
    "    a = AffineLayer(2,3)\n",
    "    a.set_state_from_vector(np.arange(9)) # A well-known initial state\n",
    "    net.extend(a)\n",
    "    print(f\"\\nNet has state {net.state_vector()}\")\n",
    "\n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "\n",
    "    # The net wraps the layer\n",
    "    y = a(x)\n",
    "    assert (net(x) == y).all()\n",
    "    \n",
    "    # Make the training batch.\n",
    "    # We use a separate affine layer, initialized differently, to determine the ideal\n",
    "    t_a = AffineLayer(2,3)\n",
    "    t_a.set_state_from_vector(arangep(9)) # A known different initial state (of primes)\n",
    "    ideal = t_a(x)\n",
    "    \n",
    "    fact = (x, ideal)\n",
    "    print(f\"fact is:\")\n",
    "    pprint(fact, indent=1)\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    net.eta = 0.01\n",
    "    for i in range(10):\n",
    "        print(f\"net.learn([fact]) = {net.learn([fact])}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    # A simple fact yielder. Since it delivers multiple facts in succession,\n",
    "    # it is a \"facts\", aka \"batch cluster\"\n",
    "    def fact_ory(fact, n):\n",
    "        for i in range(n):\n",
    "            yield fact\n",
    "    \n",
    "    def facts_printer(facts):\n",
    "        # after Network.learn\n",
    "        for fact in facts:\n",
    "            print(f\"fact: \")\n",
    "            pprint(fact)\n",
    "            x, ideal = fact\n",
    "            print(f\"from which we get:\\n\\tx={x}\\n\\tideal={ideal}\\n\")\n",
    "    \n",
    "    #facts_printer(fact_ory(fact, 5))\n",
    "    \n",
    "    #print(f\"list(fact_ory(facts[0], 3)) =\\n{list(fact_ory(facts[0], 3))}\\n\")\n",
    "    print(f\"net.learn(fact_ory(fact,10)) = {net.learn(fact_ory(fact,10))}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    for i in range(1000):\n",
    "        loss = net.learn(fact_ory(fact,10))\n",
    "        if loss < 1e-7:\n",
    "            break\n",
    "    print(f\"did {(i+1)*10} more learnings of fact. Now loss is {loss}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    print(f\"net.state_vector() = {net.state_vector()}\")\n",
    "    \n",
    "    # The network has learned the target transform\n",
    "    assert closenuf(net(x), fact[1])\n",
    "    \n",
    "if False: # This section of the test is misconceived. Skip it\n",
    "    # Save prior results and learn again, with different batch clustering\n",
    "    prev_run_loss = loss\n",
    "    prev_y = net(x)\n",
    "    net.set_state_from_vector(np.arange(9)) # A well-known initial state\n",
    "    print(f\"\\nReset net to state {net.state_vector()}\")\n",
    "\n",
    "    # Try multiple batches in each call to Network.learn\n",
    "    def multibatch_fact_ory(fact, n):\n",
    "        for i in range(n//2):\n",
    "            yield fact, fact\n",
    "    facts_printer(multibatch_fact_ory(fact, 5))\n",
    "\n",
    "    for i in range(1000):\n",
    "        loss = net.learn(multibatch_fact_ory(fact,10))\n",
    "        if loss < 1e-7:\n",
    "            break\n",
    "    print(f\"did {(i+1)*10} learnings of fact. Now loss is {loss}\")\n",
    "    print(f\"net(x) =\\n{net(x)}\")\n",
    "    \n",
    "    # The results should match exactly\n",
    "    assert loss == prev_run_loss\n",
    "    assert (net(x) == prev_y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Network.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Make a network. Leave it with the default identity behavior.\n",
    "    net = Network()\n",
    "    \n",
    "    x = np.array([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5],\n",
    "                  [6, 7]])\n",
    "    ideal = net(x)\n",
    "    facts = [(x, ideal), (x, ideal-np.array([1,-1])), (x, 2*x)]\n",
    "    assert (net.losses(facts) == [0, 1, 17.5])\n",
    "\n",
    "    # Add some layers\n",
    "    net.extend(AffineLayer(2,3)).extend(MapLayer(np.sin, np.cos)).extend(AffineLayer(3,2))\n",
    "    # Place it in a known state for test repeatability\n",
    "    net.set_state_from_vector(np.arange(len(net.state_vector())))\n",
    "    ideal = net(x)\n",
    "    facts = [(x, ideal), (x, ideal-np.array([1,-1]))]\n",
    "    #print(net.losses(facts))\n",
    "    assert (net.losses(facts) == [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce an importable `nnn.py`:\n",
    "1. Save this notebook\n",
    "1. Uncomment the `jupyter nbconvert` line below\n",
    "1. Execute it.\n",
    "1. Comment out the convert again\n",
    "1. Save the notebook again in that form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###!jupyter nbconvert --to script nnn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
